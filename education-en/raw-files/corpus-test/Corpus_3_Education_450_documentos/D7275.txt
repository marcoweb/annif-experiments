#ITI#Evaluation of teacher professional learning workshops on the use of technology - a systematic review#FTI#
#IRE#Teacher professional learning workshops have been frequently used to prepare in-service and pre-service teachers for effective use of technology in education. Evaluation of these workshops is crucial to identify the effectiveness of these programmes in terms of improving teaching skills, increasing knowledge, changing attitudes, and developing capabilities which support the achievement of student learning outcomes. Multiple approaches for evaluating professional development programmes have been developed, though each with different emphases and theoretical positioning. In this systematic review, 41 research-oriented teacher professional development workshops on technology use were critically analysed to understand how such workshops are evaluated. This study examines content evaluation, instrument types, and common professional development frameworks in teacher professional development workshops and reports on their usage and alignment. Based on the findings, the majority of papers in the systematic review did not deploy comprehensive professional development evaluation models to assess teacher professional development workshops. The majority of studies did not report on the use of established instruments for the purposes of data collection. It was further observed that the majority of these studies did not attempt to evaluate different dimensions of teacher change or different dimensions related to evaluation#FRE#
#IPC#PRISMA; professional development; professional development evaluation framework; systematic review; Teacher change; teacher professional development workshop#FPC#
#IRF#Allen G.E., Et al., A systematic review of the evi- dence base for active supervision in pre-k–12 settings, Behavioral disorders, 45, 3, pp. 167-182, (2020); 
Bates R., A critical analysis of evaluation practice: the kirkpatrick model and the principle of beneficence, Evaluation and program planning, 27, 3, pp. 341-347, (2004); 
Bhattacherjee A., Understanding information systems continuance: an expectation-confirmation model, MIS quarterly, 25, 3, pp. 351-370, (2001); 
Biggs J., Kember D., Leung D.Y., The revised two-factor study process questionnaire: r-spq-2f, British journal of educational psychology, 71, 1, pp. 133-149, (2001); 
Brooke J., System usability scale (sus): a quick-and-dirty method of system evaluation user information, (1986); 
Bruce C., Information literacy as a catalyst for educational change: a background paper, International Information Literacy Conferences and Meetings, pp. 1-17, (2003); 
Canbazoglu Bilici S., Fen ve teknoloji ¨o˘gretmenlerine teknolojik pedago- jik Alan bilgisi kazandırma ama¸clı e˘gitim uygulamaları ii, (2013); 
Carroll E.A., Et al., Creativity factor evaluation: towards a standardized survey metric for creativity support, Proceedings of the seventh ACM conference on Creativity and cognition, pp. 127-136, (2009); 
Ciampa K., Implementing a digital reading and writing workshop model for content literacy instruction in an urban elementary (k–8) school, The reading teacher, 70, 3, pp. 295-306, (2016); 
Clarke D., Hollingsworth H., Elaborating a model of teacher profes- sional growth, Teaching and teacher education, 18, 8, pp. 947-967, (2002); 
Cook D.A., Twelve tips for evaluating educational programs, Medical teacher, 32, 4, pp. 296-301, (2010); 
Crompton H., Burke D., Gregory K.H., The use of mobile learning in pk-12 education: a systematic review, Computers & education, 110, pp. 51-63, (2017); 
Datta L.-E., Evaluation theory, models, and applications, by Daniel l. stufflebeam and Anthony j. shinkfield. San Francisco: Jossey-bass, 2007. 768 pp, American journal of evaluation, 28, 4, pp. 573-576, (2007); 
Desimone L., Et al., How do district management and implementation strategies relate to the quality of the professional development that districts provide to teachers?, Teachers college record, 104, 7, pp. 1265-1312, (2002); 
Desimone L.M., Improving impact studies of teachers’ professional development: toward better conceptualizations and measures, Educational researcher, 38, 3, pp. 181-199, (2009); 
Ertmer P.A., Et al., Teacher beliefs and technology integration practices: a critical relationship, Computers & education, 59, 2, pp. 423-435, (2012); 
Garet M.S., Et al., What makes professional development effective? Results from a national sam- ple of teachers, American educational research journal, 38, 4, pp. 915-945, (2001); 
Graham R., Et al., Measuring the tpack confidence of inservice science teachers, TechTrends, 53, 5, pp. 70-79, (2009); 
Guskey T.R., Evaluating professional development, (2000); 
Guskey T.R., Does it make a difference? Evaluating professional development’, Educational leadership, 59, 6, (2002); 
Guskey T.R., Professional development and teacher change, Teachers and teaching, 8, 3, pp. 381-391, (2002); 
Hakan K., Seval F., CIPP evaluation model scale: development, reli- ability and validity, Procedia-social and behavioral sciences, 15, pp. 592-599, (2011); 
Holton III E.F., The flawed four-level evaluation model, Human resource development quarterly, 7, 1, pp. 5-21, (1996); 
Holton III E.F., Holton’s evaluation model: new evidence and con- struct elaborations, Advances in developing human resources, 7, 1, pp. 37-54, (2005); 
Hu C., Fyfe V., Impact of a new curriculum on pre-service teachers’ technical, pedagogical and content knowledge (tpack), Proceedings ascilite sydney, pp. 184-189, (2010); 
Kaminski J., Diffusion of innovation theory, Canadian journal of nurs- ing informatics, 6, 2, pp. 1-6, (2011); 
Kennedy-Clark S., Pre-service teachers’ perspectives on using scenario- based virtual worlds in science education, Computers & education, 57, 4, pp. 2224-2235, (2011); 
Kirkpatrick D., Kirkpatrick J., Evaluating training programs: the four levels, (2006); 
Koehler M., Mishra P., What is technological pedagogical content knowledge (tpack)?, Contemporary issues in technology and teacher education, 9, 1, pp. 60-70, (2009); 
Kostiainen E., Et al., Meaningful learning in teacher education, Teaching and teacher education, 71, April, pp. 66-77, (2018); 
Lai J.W., Bower M., How is the use of technology in education evaluated? A systematic review, Computers & education, 133, pp. 27-42, (2019); 
Moher D., Et al., Pre- ferred reporting items for systematic reviews and meta-analyses: the prisma statement, PLoS med, 6, 7, (2009); 
Mulder G., The concept and measurement of mental effort, Energet- ics and human information processing’, pp. 175-198, (1986); 
Patton M.Q., Utilization-focused evaluation, (2008); 
Pekrun R., Et al., Mea- suring emotions in students’ learning and performance: the achievement emo-tions questionnaire (aeq), Contemporary educational psychology, 36, 1, pp. 36-48, (2011); 
Pintrich P.R., Et al., A manual for the use of the motivated strategies for learning questionnaire (mslq), (1991); 
Poplin C.J., Models of professional development. (seeds of innovation), THE journal (technological horizons in education), 30, 11, (2003); 
Sampson V., Clark D., The development and validation of the nature of science as argument questionnaire (nsaaq), Annual Conference of the National Association for Research in Science Teaching, (2006); 
Schmidt D.A., Et al., Technological pedagogical content knowledge (tpack) the devel- opment and validation of an assessment instrument for preservice teachers, Journal of research on technology in education, 42, 2, pp. 123-149, (2009); 
Sherer M., Et al., The self-efficacy scale: construction and validation, Psychological reports, 51, 2, pp. 663-671, (1982); 
Szajna B., Empirical evaluation of the revised technology acceptance model, Management science, 42, 1, pp. 85-92, (1996); 
Taylor R., Interpretation of the correlation coefficient: a basic review, Journal of diagnostic medical sonography, 6, 1, pp. 35-39, (1990); 
Venkatesh V., Thong J.Y., Xu X., Unified theory of acceptance and use of technology: a synthesis and the road ahead, Journal of the association for information systems, 17, 5, pp. 328-376, (2016); 
Watson D., Clark L.A., The panas-x: manual for the positive and negative affect schedule-expanded form, (1999); 
Winne P.H., Perry N.E., Measuring self-regulated learning, Hand- book of self-regulation, pp. 531-566, (2000); 
Wolery M., Lane K.L., Writing tasks: literature reviews, research proposals, and final reports, Single case research methodology, pp. 50-84, (2014)#FRF#
