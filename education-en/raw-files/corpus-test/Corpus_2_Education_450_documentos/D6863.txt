#ITI#An error-analysis study from an EFL writing context: Human and Automated Essay Scoring Approaches#FTI#
#IRE#Evaluating written texts is believed to be a time-consuming process that can lack consistency and objectivity. Automated essay scoring (AES) can provide solutions to some of the limitations of human scoring. This research aimed to evaluate the performance of one AES system, Grammarly, in comparison to human raters. Both approaches’ performances were analyzed quantitatively using Corder’s (1974) error analysis approach to categorize the writing errors in a corpus of 197 essays written by English as a foreign language (EFL) learners. Pearson correlation coefficient and paired sample t-tests were conducted to analyze and compare errors detected by both approaches. According to the study’s results, a moderate correlation between human raters and AES in terms of the total scores and the number of errors detected. Results also indicated that the total number of errors detected by AES is significantly higher than human raters and that the latter tend to give students higher scores. The findings encourage a more open attitude towards AES systems to support EFL writing teachers in assessing students’ work#FRE#
#IPC#Automated essay scoring (AES); Correlation; EFL; Feedback; Human raters; Writing#FPC#
#IRF#Al-Ahdal A., Using computer software as a tool of error analysis: Giving EFL teachers and learners a much-needed impetus, International Journal of Innovation, Creativity, and Change, 12, 2, pp. 418-437, (2020); 
Alrashidi O., Phan H., Education context and English teaching and learning in the Kingdom of Saudi Arabia: An overview, English Language Teaching, 8, 5, pp. 33-44, (2015); 
Alshakhi A., Revisiting the writing assessment process at a Saudi English language institute: Problems and solutions, English Language Teaching, 12, 1, pp. 176-185, (2019); 
Attali Y., Validity and reliability of automated essay scoring, Handbook of Automated Essay Evaluation: Current Applications and New Directions, pp. 181-198, (2013); 
Attali Y., A comparison of newly-trained and experienced raters on a standardized writing assessment, Language Testing, 33, 1, pp. 99-115, (2016); 
Attali Y., Lewis W., Steier M., Scoring with the computer: Alternative procedures for improving the reliability of holistic essay scoring, Language Testing, 30, 1, pp. 125-141, (2013); 
Barkaoui K., Do ESL essay raters’ evaluation criteria change with experience? A mixed-methods, cross-sectional study, TESOL Quarterly, 44, 1, pp. 31-57, (2010); 
Barkaoui K., Variability in ESL essay rating processes: The role of the rating scale and rater experience, Language Assessment Quarterly, 7, 1, pp. 54-74, (2010); 
Bennett R.E., Bejar I.I., Validity and automated scoring: It’s not only the scoring, Educational Measurement: Issues and Practice, 17, pp. 9-17, (1998); 
Progress and new directions in technology for automated essay evaluation, .), the Oxford Handbook of Applied Linguistics (, pp. 529-539, (2010); 
Chastian K., Characteristics of graded and ungraded compositions, Modern Language Journal, 11, pp. 367-383, (1990); 
Chukharev-Hudilainen E., Saricaoglu A., Causal discourse analyzer: Improving automated feedback on academic ESL writing, Computer Assisted Language Learning, 29, 3, pp. 494-516, (2016); 
Cohen J., A coefficient of agreement for nominal scales, Educational and Psychological Measurement, 20, 1, pp. 37-46, (1960); 
Corder S.P., pp. 161-170, (1967); 
Corder S.P., Error Analysis and Interlanguage, (1981); 
Cotos E., Automated writing analysis for writing pedagogy, Writing & Pedagogy, 7, 2-3, pp. 197-231, (2015); 
Dembsey J.M., Closing the Grammarly® gaps: A study of claims and feedback from an online grammar program, The Writing Center Journal, 36, 1, pp. 63-96, (2017); 
Dikli S., Bleyle S., Automated essay scoring feedback for second language writers: How does it compare to instructor feedback?, Assessing Writing, 22, pp. 1-17, (2014); 
Douglas D., Understanding Language Testing, (2010); 
Gamper J., Knapp J., A review of intelligent CALL systems, Computer Assisted Language Learning, 15, 4, pp. 329-342, (2002); 
Ghufron M.A., Rosyida F., The role of Grammarly in assessing English as a foreign language (EFL) writing, Lingua Cultura, 12, 4, pp. 395-403, (2018); 
Goh T.T., Sun H., Yang B., Microfeatures influencing writing quality: The case of Chinese students’ SAT essays, Computer Assisted Language Learning, 33, 4, pp. 455-481, (2020); 
Higgins J.J., Computer-assisted language learning, Language Teaching, 16, 2, pp. 102-114, (1983); 
Huang S.J., Error Analysis and Teaching Composition (Unpublished master’s Thesis), (2001); 
Huang S.J., Automated versus human scoring: A case study in an EFL context, Electronic Journal of Foreign Language Teaching, 11, pp. 149-164, (2014); 
Jayavalan K., Razali A.B., Effectiveness of online grammar checker to improve secondary students’ English narrative essay writing, International Research Journal of Education and Sciences, 2, 1, pp. 1-6, (2018); 
Ke Z., Automated essay scoring: A survey of the state of the art, International Joint Conference on Artificial Intelligence 28Th Annual Meeting, (2019); 
Kenning M.J., Kenning M.M., Introduction to computer assisted language teaching, Oxford University Press, (1983); 
Li J., Link S., Hegelheimer V., Rethinking the role of automated writing evaluation (AWE) feedback in ESL writing instruction, Journal of Second Language Writing, 27, pp. 1-18, (2015); 
Li Z., Link S., Ma H., Yang H., Hegelheimer V., The role of automated writing evaluation holistic scores in the ESL classroom, System, 44, pp. 66-78, (2014); 
Lu M., Deng Q., Yang M., EFL writing assessment: Peer assessment vs. automated essay scoring, International Symposium on Emerging Technologies for Education, pp. 21-29, (2019); 
Morse J.M., Qualitative nursing research: A contemporary dialogue, Strategies for Sampling., pp. 127-145, (1991); 
Nova M., Utilizing Grammarly in evaluating academic writing: A narrative research on EFL students’ experience, Premise: Journal of English Education, 7, 1, pp. 80-97, (2018); 
O'Neill R., Russell A.M.T., Stop! Grammar time: University students’ perceptions of the automated feedback program Grammarly, Australasian Journal of Educational Technology, 35, 1, pp. 42-56, (2019); 
Park J., An AI-based English grammar checker vs. Human raters in evaluating EFL learners’ writing, Multimedia-Assisted Language Learning, 22, 1, pp. 112-131, (2019); 
Perdana I., Farida M., Online grammar checkers and their use for EFL writing, Journal of English Teaching, Applied Linguistics, and Literatures, 2, 2, pp. 67-76, (2019); 
Prinsloo D., Bothma T., A copulative decision tree as a writing tool for Sepedi, South African Journal of African Languages, 40, 1, pp. 85-97, (2020); 
Polit D.F., Hungler B.P., (1993); 
Ranalli J., Automated written corrective feedback: How well can students make use of it?, Computer Assisted Language Learning, 31, 7, pp. 653-674, (2018); 
Rao Z., Li X., Native and non-native teachers’ perceptions of error gravity: The effects of cultural and educational factors, The Asia-Pacific Education Researcher, 26, 1-2, pp. 51-59, (2017); 
Reilly E.D., Stafford R.E., Williams K.M., Corliss S.B., Evaluating the validity and applicability of automated essay scoring in two massive open online courses, International Review of Research in Open and Distributed Learning, 15, 5, pp. 83-98, (2014); 
Seker M., Intervention in teachers’ differential scoring judgments in assessing L2 writing through communities of assessment practice, Studies in Educational Evaluation, 59, pp. 209-217, (2018); 
Sharma C., Bishnoi A., Sachan A.K., Verma A., Automated essay evaluation using natural language processing, International Research Journal of Engineering and Technology, 5, 6, pp. 2055-2058, (2019); 
Shermis M., Burstein J., Automated Essay Scoring: A Cross-Disciplinary Perspective, (2003); 
Shermis M.D., State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration, Assessing Writing, 20, pp. 53-76, (2014); 
Automated essay scoring: Writing assessment and instruction, International Encyclopedia of Education, pp. 75-80, (2010); 
Sparks J.R., Song Y., Brantley W., Liu O.L., Assessing written communication in higher education: Review and recommendations for next-generation assessment (, 2, (2014); 
Vojak C., Kline S., Cope B., McCarthey S., Kalantzis M., New spaces and old places: An analysis of writing assessment software, Computers and Composition, 28, 2, pp. 97-111, (2011); 
Wali F.A., Huijser H., Write to improve: Exploring the impact of an automated feedback tool on Bahraini learners of English, Learning & Teaching in Higher Education: Gulf Perspectives, 15, 1, (2018); 
Wang J., Brown M.S., Automated essay scoring versus human scoring: A comparative study, Journal of Technology, Learning, and Assessment, 6, 2, (2007); 
Wilson J., Roscoe R., Automated writing evaluation and feedback: Multiple metrics of efficacy, Journal of Educational Computing Research, 58, 1, pp. 87-125, (2019); 
Wiseman C.S., Rater effects: Ego engagement in rater decision-making, Assessing Writing, 17, 3, pp. 150-173, (2012); 
Wind S.A., Engelhard G., How invariant and accurate are domain ratings in writing assessment?, Assessing Writing, 18, 4, pp. 278-299, (2013); 
Wu H., Garza E.V., Types and attributes of English writing errors in the EFL Context – A study of error analysis, Journal of Language Teaching & Research, 5, 6, pp. 125-141, (2014)#FRF#
