#ITI#Detecting Careless Responding in Survey Data Using Stochastic Gradient Boosting#FTI#
#IRE# Careless responding is a bias in survey responses that disregards the actual item content, constituting a threat to the factor structure, reliability, and validity of psychological measurements. Different approaches have been proposed to detect aberrant responses such as probing questions that directly assess test-taking behavior (e.g., bogus items), auxiliary or paradata (e.g., response times), or data-driven statistical techniques (e.g., Mahalanobis distance). In the present study, gradient boosted trees, a state-of-the-art machine learning technique, are introduced to identify careless respondents. The performance of the approach was compared with established techniques previously described in the literature (e.g., statistical outlier methods, consistency analyses, and response pattern functions) using simulated data and empirical data from a web-based study, in which diligent versus careless response behavior was experimentally induced. In the simulation study, gradient boosting machines outperformed traditional detection mechanisms in flagging aberrant responses. However, this advantage did not transfer to the empirical study. In terms of precision, the results of both traditional and the novel detection mechanisms were unsatisfactory, although the latter incorporated response times as additional information. The comparison between the results of the simulation and the online study showed that responses in real-world settings seem to be much more erratic than can be expected from the simulation studies. We critically discuss the generalizability of currently available detection methods and provide an outlook on future research on the detection of aberrant response patterns in survey research.#FRE#
#IPC# careless responding; data cleaning; gradient boosted trees; outlier detection; response times#FPC#
#IRF# Arias V.B., Garrido L.E., Jenaro C., Martinez-Molina A., Arias B., A little garbage in, lots of garbage out: Assessing the impact of careless responding in personality survey data, Behavior Research Methods, (2020); 
Ashton M., Lee K., The HEXACO-60: A short measure of the major dimensions of personality, Journal of Personality Assessment, 91, 4, pp. 340-345, (2009); 
Berk R.A., Statistical learning from a regression perspective, (2017); 
Berry D.T.R., Wetter M.W., Baer R.A., Larsen L., Clark C., Monroe K., MMPI-2 random responding indices: Validation using a self-report methodology, Psychological Assessment, 4, 3, pp. 340-345, (1992); 
Bowling N.A., Gibson A.M., Houpt J.W., Brower C.K., Will the questions ever end? Person-level increases in careless responding during questionnaire completion, Organizational Research Methods, (2020); 
Bowling N.A., Huang J.L., Your attention please! Toward a better understanding of research participant carelessness, Applied Psychology, 67, 2, pp. 227-230, (2018); 
Breiman L., Random forests, Machine Learning, 45, pp. 5-32, (2001); 
Buchanan E.M., Scofield J.E., Methods to detect low quality data and its implication for psychological research, Behavior Research Methods, 50, 6, pp. 2586-2596, (2018); 
Cearns M., Hahn T., Baune B.T., Recommendations and future directions for supervised machine learning in psychiatry, Translational Psychiatry, 9, (2019); 
Chalmers R.P., mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Couper M.P., Technology trends in survey data collection, Social Science Computer Review, 23, 4, pp. 486-501, (2005); 
Crede M., Random responding as a threat to the validity of effect size estimates in correlational research, Educational and Psychological Measurement, 70, 4, pp. 596-612, (2010); 
Curran P.G., Methods for the detection of carelessly invalid responses in survey data, Journal of Experimental Social Psychology, 66, pp. 4-19, (2016); 
Curran P.G., Hauser K.A., I’m paid biweekly, just not by leprechauns: Evaluating valid-but-incorrect response rates to attention check items, Journal of Research in Personality, 82, (2019); 
Denison A., Wiernik B., Careless response processes are heterogeneous: Comment on Goldammer et al, PsyArXiv Preprints, (2020); 
Drasgow F., Levine M.V., Williams E.A., Appropriateness measurement with polychotomous item response models and standardized indices, British Journal of Mathematical and Statistical Psychology, 38, 1, pp. 67-86, (1985); 
Dunn A.M., Heggestad E.D., Shanock L.R., Theilgard N., Intra-individual response variability as an indicator of insufficient effort responding: Comparison to other indicators and relationships with individual differences, Journal of Business and Psychology, 33, 1, pp. 105-121, (2018); 
Emons W.H.M., Nonparametric person-fit analysis of polytomous item scores, Applied Psychological Measurement, 32, 3, pp. 224-247, (2008); 
Fan J., Han F., Liu H., Challenges of Big Data analysis, National Science Review, 1, 2, pp. 293-314, (2014); 
Ferrari P.A., Barbiero A., Simulating ordinal data, Multivariate Behavioral Research, 47, 4, pp. 566-589, (2012); 
Figurska M., Stanczyk M., Kulesza K., Humans cannot consciously generate random numbers sequences: Polemic study, Medical Hypotheses, 70, 1, pp. 182-185, (2008); 
Friedman J.H., Greedy function approximation: A gradient boosting machine, Annals of Statistics, 29, 5, pp. 1189-1232, (2001); 
Friedman J.H., Stochastic gradient boosting, Computational Statistics and Data Analysis, 38, 4, pp. 367-378, (2002); 
Garcia V., Sanchez J.S., Mollineda R.A., On the effectiveness of preprocessing methods when dealing with different levels of class imbalance, Knowledge-Based Systems, 25, 1, pp. 13-21, (2012); 
Geiger M., Olderbak S., Sauter R., Wilhelm O., The “g” in faking: Doublethink the validity of personality self-report measures for applicant selection, Frontiers in Psychology, 9, pp. 1-15, (2018); 
Gibson A.M., Bowling N.A., The effects of questionnaire length and behavioral consequences on careless responding, European Journal of Psychological Assessment, 36, 2, pp. 410-420, (2020); 
Glas C.A.W., Dagohoy A.V.T., A person fit test for IRT models for polytomous items, Psychometrika, 72, 2, pp. 159-180, (2007); 
Gnambs T., Kaspar K., Disclosure of sensitive behaviors across self-administered survey modes: A meta-analysis, Behavior Research Methods, 47, pp. 1237-1259, (2015); 
Gnambs T., Schroeders U., Cognitive abilities explain wording effects in the Rosenberg Self-Esteem Scale, Assessment, 27, 2, pp. 404-418, (2020); 
Goritz A.S., Borchert K., Hirth M., Using attention testing to select crowdsourced workers and research participants, Social Science Computer Review, (2019); 
Greenwell B., Boehmke B., Cunnigham J., gbm: Generalized boosted regression models, (2019); 
Gummer T., Rossmann J., Silber H., Using instructed response items as attention checks in web surveys: Properties and implementation, Sociological Methods & Research, (2018); 
Hong M., Steedle J.T., Cheng Y., Methods of detecting insufficient effort responding: Comparisons and practical recommendations, Educational and Psychological Measurement, 80, 2, pp. 312-345, (2020); 
Huang J.L., Curran P.G., Keeney J., Poposki E.M., DeShon R.P., Detecting and deterring insufficient effort responding to surveys, Journal of Business and Psychology, 27, 1, pp. 99-114, (2012); 
Huang J.L., Liu M., Bowling N.A., Insufficient effort responding: Examining an insidious confound in survey data, Journal of Applied Psychology, 100, 3, pp. 828-845, (2015); 
Jacobucci R., Grimm K.J., Machine learning and psychological research: The unexplored effect of measurement, Perspectives on Psychological Science, 15, 3, pp. 809-816, (2020); 
James G., Witten D., Hastie T., Tibshirani R., An introduction to statistical learning, (2017); 
Johnson J.A., Ascertaining the validity of individual protocols from web-based personality inventories, Journal of Research in Personality, 39, 1, pp. 103-129, (2005); 
Karabatsos G., Comparing the aberrant response detection performance of thirty-six person-fit statistics, Applied Measurement in Education, 16, 4, pp. 277-298, (2003); 
Kennedy R., Clifford S., Burleigh T., Waggoner P.D., Jewell R., Winter N.J.G., The shape of and solutions to the MTurk quality crisis, Political Science Research and Methods, 8, 4, pp. 614-629, (2020); 
Kieslich P.J., Henninger F., Mousetrap: An integrated, open-source mouse-tracking package, Behavior Research Methods, 49, 5, pp. 1652-1667, (2017); 
Kosinski M., Stillwell D., Graepel T., Private traits and attributes are predictable from digital records of human behavior, Proceedings of the National Academy of Sciences of the United States of America, 110, 15, pp. 5802-5805, (2013); 
Kroehne U., Goldhammer F., How to conceptualize, represent, and analyze log data from technology-based assessments? A generic framework and an application to questionnaire items, Behaviormetrika, 45, pp. 527-563, (2018); 
Kuhn M., Building predictive models in r using the caret package, Journal of Statistical Software, 28, 5, pp. 1-26, (2008); 
Kuhn M., Caret, (2020); 
Kurtz J.E., Parrish C.L., Semantic response consistency and protocol validity in structured personality assessment: The case of the NEO-PI-R, Journal of Personality Assessment, 76, 2, pp. 315-332, (2001); 
Leiner D.J., Too fast, too straight, too weird: Non-reactive indicators for meaningless data in internet surveys, Survey Research Methods, 13, 3, pp. 229-248, (2019); 
Leiner D.J., SoSci survey, (2020); 
Litman L., Robinson J., Conducting online research on Amazon Mechanical Turk and beyond, (2020); 
Mahalanobis P.C., On the generalised distance in statistics, Proceedings of the National Institute of Sciences of India, 2, 1, pp. 49-55, (1936); 
Maniaci M.R., Rogge R.D., Caring about carelessness: Participant inattention and its effects on research, Journal of Research in Personality, 48, pp. 61-83, (2014); 
Marjanovic Z., Holden R., Struthers W., Cribbie R., Greenglass E., The inter-item standard deviation (ISD): An index that discriminates between conscientious and random responders, Personality and Individual Differences, 84, pp. 79-83, (2015); 
McGrath R.E., Mitchell M., Kim B.H., Hough L., Evidence for response bias as a source of error variance in applied assessment, Psychological Bulletin, 136, 3, pp. 450-470, (2010); 
Meade A.W., Allred C.M., Pappalardo G., Stoughton J.W., Careless response and attrition as sources of bias in online survey assessments of personality traits and performance, Computers in Human Behavior, 76, pp. 417-430, (2017); 
Meade A.W., Craig S.B., Identifying careless responses in survey data, Psychological Methods, 17, 3, pp. 437-455, (2012); 
Meijer R.R., Molenaar I.W., Sijtsma K., Influence of test and person characteristics on nonparametric appropriateness measurement, Applied Psychological Measurement, 18, 2, pp. 111-120, (1994); 
Meijer R.R., Person-fit research: An introduction, Applied Measurement in Education, 9, 1, pp. 3-8, (1996); 
Mittenberg W., Patton C., Canyock E.M., Condit D.C., Base rates of malingering and symptom exeggeration, Journal of Clinical and Experimental Neuropsychology, 24, 8, pp. 1094-1102, (2002); 
Nichols D.S., Greene R.L., Schmolck P., Criteria for assessing inconsistent patterns of item endorsement on the MMPI: Rationale, development, and empirical trials, Journal of Clinical Psychology, 45, 2, pp. 239-250, (1989); 
Niessen A.S.M., Meijer R.R., Tendeiro J.N., Detecting careless respondents in web-based questionnaires: Which method to use?, Journal of Research in Personality, 63, pp. 1-11, (2016); 
Olson K., Parkhurst B., Collecting paradata for measurement error evaluations, Improving surveys with paradata, pp. 43-72, (2013); 
Oppenheimer D.M., Meyvis T., Davidenko N., Instructional manipulation checks: Detecting satisficing to increase statistical power, Journal of Experimental Social Psychology, 45, 4, pp. 867-872, (2009); 
Orthey R., Vrij A., Meijer E., Leal S., Blank H., Eliciting response bias within forced choice tests to detect random responders, Scientific Reports, 9, (2019); 
Seeboth A., Mottus R., Successful explanations start with accurate descriptions: Questionnaire items as personality markers for more accurate predictions: Items as personality markers, European Journal of Personality, 32, 3, pp. 186-201, (2018); 
Shmueli G., To explain or to predict?, Statistical Science, 25, 3, pp. 289-310, (2010); 
Soderberg C.K., Using OSF to share data: A step-by-step guide, Advances in Methods and Practices in Psychological Science, 1, 1, pp. 115-120, (2018); 
Sotaridona L.S., Meijer R.R., Statistical properties of the K-index for detecting answer copying, Journal of Educational Measurement, 39, 2, pp. 115-132, (2002); 
Stachl C., Pargent F., Hilbert S., Harari G.M., Schoedel R., Vaid S., Gosling S.D., Buhner M., Personality research and assessment in the era of machine learning, European Journal of Personality, (2020); 
Steger D., Schroeders U., Gnambs T., A meta-analysis of test scores in proctored and unproctored ability assessments, European Journal of Psychological Assessment, 36, 1, pp. 174-184, (2020); 
Steger D., Schroeders U., Wilhelm O., Caught in the act: Predicting cheating in unproctored knowledge assessment, Assessment, (2020); 
Tendeiro J.N., Meijer R.R., Niessen A.S.M., PerFit: An R package for person-fit analysis in IRT, Journal of Statistical Software, 74, 5, pp. 1-27, (2016); 
Tharwat A., Classification assessment methods, Applied Computing and Informatics, (2018); 
Vabalas A., Gowen E., Poliakoff E., Casson A.J., Machine learning algorithm validation with a limited sample size, PLoS One, 14, 11, (2019); 
Van Hulse J., Khoshgoftaar T.M., Napolitano A., Experimental perspectives on learning from imbalanced data, ICML ’07: Proceedings of the 24th International Conference on Machine Learning, pp. 935-942, (2007); 
Wang C., Xu G., A mixture hierarchical model for response times and response accuracy, British Journal of Mathematical and Statistical Psychology, 68, 3, pp. 456-477, (2015); 
Wang C., Xu G., Shang Z., Kuncel N., Detecting aberrant behavior and item preknowledge: A comparison of mixture modeling method and residual method, Journal of Educational and Behavioral Statistics, 43, 4, pp. 469-501, (2018); 
Weiner S.P., Dalessio A.T., Oversurveying: Causes, consequences, and cures, Getting action from organizational surveys: New concepts, technologies, and applications, pp. 294-311, (2006); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, 2, pp. 163-183, (2005); 
Woods C.M., Careless responding to reverse-worded items: Implications for confirmatory factor analysis, Journal of Psychopathology and Behavioral Assessment, 28, 3, pp. 186-191, (2006); 
Yarkoni T., Westfall J., Choosing prediction over explanation in psychology: Lessons from machine learning, Perspectives on Psychological Science, 12, 6, pp. 1100-1122, (2017); 
Yentes R., Wilhelm F., Careless, (2018); 
Zopluoglu C., Detecting examinees with item preknowledge in large-scale testing using Extreme Gradient Boosting (XGBoost), Educational and Psychological Measurement, 79, 5, pp. 931-961, (2019)#FRF#
