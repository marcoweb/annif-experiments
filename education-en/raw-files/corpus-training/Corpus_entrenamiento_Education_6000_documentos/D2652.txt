#ITI#Beyond item analysis: Connecting student behaviour and performance using e-assessment logs#FTI#
#IRE# Traditional item analyses such as classical test theory (CTT) use exam-taker responses to assessment items to approximate their difficulty and discrimination. The increased adoption by educational institutions of electronic assessment platforms (EAPs) provides new avenues for assessment analytics by capturing detailed logs of an exam-taker's journey through their exam. This paper explores how logs created by EAPs can be employed alongside exam-taker responses and CTT to gain deeper insights into exam items. In particular, we propose an approach for deriving features from exam logs for approximating item difficulty and discrimination based on exam-taker behaviour during an exam. Items for which difficulty and discrimination differ significantly between CTT analysis and our approach are flagged through outlier detection for independent academic review. We demonstrate our approach by analysing de-identified exam logs and responses to assessment items of 463 medical students enrolled in a first-year biomedical sciences course. The analysis shows that the number of times an exam-taker visits an item before selecting a final response is a strong indicator of an item's difficulty and discrimination. Scrutiny by the course instructor of the seven items identified as outliers suggests our log-based analysis can provide insights beyond what is captured by traditional item analyses. Practitioner notes What is already known about this topic Traditional item analysis is based on exam-taker responses to the items using mathematical and statistical models from classical test theory (CTT). The difficulty and discrimination indices thus calculated can be used to determine the effectiveness of each item and consequently the reliability of the entire exam. What this paper adds Data extracted from exam logs can be used to identify exam-taker behaviours which complement classical test theory in approximating the difficulty and discrimination of an item and identifying items that may require instructor review. Implications for practice and/or policy Identifying the behaviours of successful exam-takers may allow us to develop effective exam-taking strategies and personal recommendations for students. Analysing exam logs may also provide an additional tool for identifying struggling students and items in need of revision#FRE#
#IPC# assessment analytics; classical test theory; computer-based assessment; e-assessment; exam log; exam-taking behaviour; item analysis; learning analytics#FPC#
#IRF# Abdi S., Khosravi H., Sadiq S., Modelling learners in crowdsourcing educational systems, Artificial intelligence in education, pp. 3-9, (2020); 
Abdi S., Khosravi H., Sadiq S., Modelling learners in adaptive educational systems: A multivariate glicko-based approach, Lak21: 11th international learning analytics and knowledge conference, pp. 497-503, (2021); 
Abdi S., Khosravi H., Sadiq S., Darvishi A., Open learner models for multi-activity educational systems, International conference on artificial intelligence in education, pp. 11-17, (2021); 
Abdi S., Khosravi H., Sadiq S., Gasevic D., A multivariate elo-based learner model for adaptive educational systems, EDM: Proceedings of the 12th International Conference on Educational Data Mining, pp. 228-233, (2019); 
Aiken L.R., Relationships between the item difficulty and discrimination indexes, Educational and Psychological Measurement, 39, 4, pp. 821-824, (1979); 
Barana A., Conte A., Fissore C., Marchisio M., Rabellino S., Learning analytics to improve formative assessment strategies, Journal of e-Learning and Knowledge Society, 15, pp. 75-88, (2019); 
Bauer D., Kopp V., Fischer M.R., Answer changing in multiple choice assessment change that answer when in doubt–and spread the word!, BMC Medical Education, 7, 1, pp. 1-5, (2007); 
Bezirhan U., von Davier M., Grabovsky I., Modeling item revisit behavior: The hierarchical speed–accuracy–revisits model, Educational and Psychological Measurement, 81, 2, pp. 363-387, (2021); 
Bumbalkova E., Test-taking strategies in second language receptive skills tests: A literature review, International Journal of Instruction, 14, 2, pp. 647-664, (2021); 
Chiavaroli N., Familari M., When majority doesn't rule: The use of discrimination indices to improve the quality of mcqs, Bioscience Education, 17, 1, pp. 1-7, (2011); 
Cleophas C., Hoennige C., Meisel F., Meyer P., Who's cheating? Mining patterns of collusion from text and events in online exams, INFORMS Transactions on Education, (2021); 
Costagliola G., Fuccella V., Giordano M., Polese G., Monitoring online tests through data visualization, IEEE Transactions on Knowledge and Data Engineering, 21, 6, pp. 773-784, (2008); 
Couchman J.J., Miller N.E., Zmuda S.J., Feather K., Schwartzmeyer T., The instinct fallacy: The metacognition of answering and revising during college exams, Metacognition and Learning, 11, 2, pp. 171-185, (2016); 
Cousin G., An introduction to threshold concepts, 17, 1, (2006); 
De Champlain A.F., A primer on classical test theory and item response theory for assessments in medical education, Medical Education, 44, 1, pp. 109-117, (2010); 
Dennick R., Wilkinson S., Purcell N., Online eassessment: Amee guide no. 39, Medical Teacher, 31, 3, pp. 192-206, (2009); 
Dodonova Y.A., Dodonov Y.S., Processing speed and intelligence as predictors of school achievement: Mediation or unique contribution?, Intelligence, 40, 2, pp. 163-171, (2012); 
Ellis A.P.J., Ryan A.M., Race and cognitive-ability test performance: The mediating effects of test preparation, test-taking strategy use and self-efficacy, Journal of Applied Social Psychology, 33, 12, pp. 2607-2629, (2003); 
Ellis C., Broadening the scope and increasing the usefulness of learning analytics: The case for assessment analytics: Colloquium, British Journal of Educational Technology, 44, 4, pp. 662-664, (2013); 
Engelhardt L., Goldhammer F., Validating test score interpretations using time information, Frontiers in Psychology, 10, (2019); 
Gasevic D., Greiff S., Shaffer D.W., Towards strengthening links between learning analytics and assessment: Challenges and potentials of a promising new bond, Computers in Human Behavior, 134, (2022); 
Goldhammer F., Naumann J., Stelter A., Toth K., Rolke H., Klieme E., The time on task effect in reading and problem solving is moderated by task difficulty and skill: Insights from a computer-based large-scale assessment, Journal of Educational Psychology, 106, 3, pp. 608-626, (2014); 
Greiff S., Niepel C., Scherer R., Martin R., Understanding students' performance in a computer-based assessment of complex problem solving: An analysis of behavioral data from computer-generated log files, Computers in Human Behavior, 61, pp. 36-46, (2016); 
Hingorjo M.R., Jaleel F., Analysis of one-best mcqs: The difficulty index, discrimination index and distractor efficiency, Journal of the Pakistan Medical Association, 62, 2, (2012); 
Hong E., Sas M., Sas J.C., Test-taking strategies of high and low mathematics achievers, The Journal of Educational Research, 99, 3, pp. 144-155, (2006); 
Ifenthaler D., Greiff S., Leveraging learning analytics for assessment and feedback, Online learning analytics, pp. 1-18, (2021); 
Ifenthaler D., Greiff S., Gibson D., Making use of data for assessments: Harnessing analytics and data science, Second handbook of information technology in primary and secondary education, pp. 1-16, (2018); 
Jordan S., E-assessment: Past, present and future, New Directions, 9, 1, pp. 87-106, (2013); 
Jung Kim Y.-M., Investigation of neel's new item analysis technique (john h. neel), (2001); 
Kane M., Mislevy R., Validating score interpretations based on response processes, Validation of score meaning for the next generation of assessments, pp. 11-24, (2017); 
Karelia B.N., Pillai A., Vegada B.N., The levels of difficulty and discrimination indices and relationship between them in four-response type multiple choice questions of pharmacology summative tests of year ii mbbs students, IeJSME, 7, 2, pp. 41-46, (2013); 
Khosravi H., Shum S.B., Chen G., Conati C., Tsai Y.S., Kay J., Knight S., Martinez-Maldonado R., Sadiq S., Gasevic D., Explainable artificial intelligence in education, Computers and Education: Artificial Intelligence, 3, (2022); 
Klein Entink R.H., Fox J.-P., van der Linden W.J., A multivariate multilevel approach to the modeling of accuracy and speed of test takers, Psychometrika, 74, 1, pp. 21-48, (2008); 
Kong X.J., Wise S.L., Bhola D.S., Setting the response time threshold parameter to differentiate solution behavior from rapid-guessing behavior, Educational and Psychological Measurement, 67, 4, pp. 606-619, (2007); 
Kupiainen S., Vainikainen M.-P., Marjanen J., Hautamaki J., The role of time on task in computer-based low-stakes assessment of cross-curricular skills, Journal of Educational Psychology, 106, 3, pp. 627-638, (2014); 
Lang C., Wise A., Siemens G., Gasevic D., Handbook of Learning Analytics, (2017); 
Leary L.F., Dorans N.J., Implications for altering the context in which test items appear: A historical perspective on an immediate concern, Review of Educational Research, 55, 3, pp. 387-413, (1985); 
Lee Y., Estimating student ability and problem difficulty using item response theory (irt) and trueskill, (2019); 
Livingston S.A., Item analysis, Handbook of test development, pp. 421-441, (2006); 
Llamas-Nistal M., Fernandez-Iglesias M.J., Gonzalez-Tato J., Mikic-Fonte F.A., Blended e-assessment: Migrating classical exams to the digital world, Computers Education, 62, pp. 72-87, (2013); 
Padilla J.-L., Benitez I., Validity evidence based on response processes, Psicothema, 26, 1, pp. 136-144, (2014); 
Pagni S.E., Bak A.G., Eisen S.E., Murphy J.L., Finkelman M.D., Kugel G., The benefit of a switch: Answer-changing on multiple-choice exams by first-year dental students, Journal of Dental Education, 81, 1, pp. 110-115, (2017); 
Palmiero C., Cecconi L., Use of learning analytics in formative and summative evaluation, Journal of e-Learning and Knowledge Society, 3, pp. 89-99, (2019); 
Papamitsiou Z., Economides A.A., Exhibiting achievement behavior during computer-based testing: What temporal trace data and personality traits tell us?, Computers in Human Behavior, 75, pp. 423-438, (2017); 
Papamitsiou Z., Economides A., Students' perception of performance vs. actual performance during computer-based testing: A temporal approach, Inted2014 proceedings, pp. 401-411, (2014); 
Papamitsiou Z., Economides A.A., Temporal learning analytics visualizations for increasing awareness during assessment, International Journal of Educational Technology in Higher Education, 12, 3, pp. 129-147, (2015); 
Papamitsiou Z., Economides A.A., Pappas I.O., Giannakos M.N., Explaining learning performance using response-time, self-regulation and satisfaction from content: An fsQCA approach, Proceedings of the 8th International Conference on Learning Analytics and Knowledge, pp. 181-190, (2018); 
Papamitsiou Z.K., Economides A.A., Towards the alignment of computer-based assessment outcome with learning goals: The LAERS architecture, 2013 IEEE Conference on e-Learning, e-Management and e-Services, pp. 13-17, (2013); 
Papamitsiou Z.K., Terzis V., Economides A.A., Temporal learning analytics for computer based testing, Proceedings of the fourth international conference on learning analytics and knowledge, pp. 31-35, (2014); 
Rose C.P., McLaughlin E.A., Liu R., Koedinger K.R., Explanatory learner models: Why machine learning (alone) is not the answer, British Journal of Educational Technology, 50, 6, pp. 2943-2958, (2019); 
Sharma K., Papamitsiou Z., Olsen J.K., Giannakos M., Predicting learners' effortful behaviour in adaptive assessment using multimodal data, Proceedings of the Tenth International Conference on Learning Analytics & Knowledge, pp. 480-489, (2020); 
Sim S.-M., Rasiah R.I., Relationship between item difficulty and discrimination indices in true/false-type multiple choice questions of a para-clinical multidisciplinary paper, Annals-Academy of Medicine Singapore, 35, 2, (2006); 
Stenlund T., Eklof H., Lyren P.-E., Group differences in test-taking behaviour: An example from a high-stakes testing program, Assessment in Education: Principles, Policy & Practice, 24, 1, pp. 4-20, (2017); 
Stenlund T., Lyren P.-E., Eklof H., The successful test taker: Exploring test-taking behavior profiles through cluster analysis, European Journal of Psychology of Education, 33, 2, pp. 403-417, (2018); 
Thillmann H., Gossling J., Marschner J., Wirth J., Leutner D., Metacognitive knowledge about and metacognitive regulation of strategy use in self-regulated scientific discovery learning: New methods of assessment in computer-based learning environments, International handbook of metacognition and learning technologies, pp. 575-588, (2013); 
Toton S.L., Maynes D.D., Detecting examinees with pre-knowledge in experimental data using conditional scaling of response times, Frontiers in Education, 4, (2019); 
van der Linden W.J., Using response times for item selection in adaptive testing, Journal of Educational and Behavioral Statistics, 33, 1, pp. 5-20, (2008); 
van der Linden W.J., Guo F., Bayesian procedures for identifying aberrant response-time patterns in adaptive testing, Psychometrika, 73, 3, pp. 365-384, (2008); 
Wang J., Bao L., Analyzing force concept inventory with item response theory, American Journal of Physics, 78, 10, pp. 1064-1070, (2010); 
Wauters K., Desmet P., Van Noortgate W., Monitoring learners' proficiency: Weight adaptation in the elo rating system, Proceedings of the 4th international conference on educational data mining 2011, (2010); 
Wiggins B.C., Detecting and dealing with outliers in univariate and multivariate contexts, (2000); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, 3, pp. 237-252, (2015); 
Wise S.L., Gao L., A general approach to measuring test-taking effort on computer-based tests, Applied Measurement in Education, 30, 4, pp. 343-354, (2017)#FRF#
