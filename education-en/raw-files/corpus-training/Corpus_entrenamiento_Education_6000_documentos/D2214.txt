#ITI#Indices of Subscore Utility for Individuals and Subgroups Based on Multivariate Generalizability Theory#FTI#
#IRE# Conventional methods for evaluating the utility of subscores rely on traditional indices of reliability and on correlations among subscores. One limitation of correlational methods is that they do not explicitly consider variation in subtest means. An exception is an index of score profile reliability designated as G, which quantifies the ratio of true score profile variance to observed score profile variance. G has been shown to be more sensitive than correlational methods to group differences in score profile utility. However, it is a group average, representing the expected value over a population of examinees. Just as score reliability varies across individuals and subgroups, one can expect that the reliability of score profiles will vary across examinees. This article proposes two conditional indices of score profile utility grounded in multivariate generalizability theory. The first is based on the ratio of observed profile variance to the profile variance that can be attributed to random error. The second quantifies the proportion of observed variability in a score profile that can be attributed to true score profile variance. The article describes the indices, illustrates their use with two empirical examples, and evaluates their properties with simulated data. The results suggest that the proposed estimators of profile error variance are consistent with the known error in simulated score profiles and that they provide information beyond that provided by traditional measures of subscore utility. The simulation study suggests that artificially large values of the indices could occur for about 5% to 8% of examinees. The article concludes by suggesting possible applications of the indices and discusses avenues for further research.#FRE#
#IPC# generalizability theory; score profiles; score reports; subscores#FPC#
#IRF# Standards for educational and psychological testing, (2014); 
Brennan R.L., Raw-score conditional standard errors of measurement in generalizability theory, Applied Psychological Measurement, 22, pp. 307-333, (1998); 
Brennan R.L., Generalizability theory, (2001); 
Brennan R.L., Manual for mGENOVA, Version 2.1, (2001); 
Brennan R.L., Utility indices for decisions about subscores, (2011); 
Bridgeman B., Can a two-question test be reliable and valid for predicting academic outcomes?, Educational Measurement: Issues and Practice, 35, pp. 21-24, (2016); 
Bridgeman B., Lewis C., The relationship of essay and multiple-choice scores with college courses, Journal of Educational Measurement, 31, pp. 37-50, (1994); 
Conger A.J., Lipshitz R., Measures of reliability for profiles and test batteries, Psychometrika, 38, pp. 411-427, (1973); 
Cronbach L.J., Gleser G., Assessing similarity between profiles, Psychological Bulletin, 50, pp. 456-473, (1953); 
Detterman D.K., Daniel M.H., Correlations of mental tests with each other and with cognitive variables are highest for low-IQ groups, Intelligence, 13, pp. 349-359, (1989); 
Feinberg R.A., Jurich D.P., Guidelines for interpreting subscores, Educational Measurement: Issues and Practice, 36, pp. 5-13, (2017); 
Feinberg R.A., Wainer H., A simple equation to predict a subscoreâ€™s value, Educational Measurement: Issues and Practice, 33, pp. 55-56, (2014); 
Feldt L.S., Steffen M., Gupta N.C., A comparison of five methods for estimating the standard error of measurement at specific score levels, Applied Psychological Measurement, 9, pp. 351-361, (1985); 
Fleishman E.A., A comparative study of aptitude patterns in unskilled and skilled psychomotor performances, Journal of Applied Psychology, 41, pp. 263-272, (1957); 
Haberman S.J., When can subscores have value?, Journal of Educational and Behavioral Statistics, 33, pp. 204-229, (2008); 
Haberman S.J., Sinharay S., Reporting of subscores using multidimensional item response theory, Psychometrika, 75, (2010); 
Haladyna T.M., Kramer G.A., The validity of subscores for a credentialing examination, Evaluation & the Health Professions, 27, pp. 349-368, (2004); 
Holtzman K.Z., Swanson D.B., Ouyang W., Dillon G.F., Boulet J.R., International variation in performance by clinical discipline and task on the USMLE Step 2 clinical knowledge component, Academic Medicine, 89, pp. 1558-1562, (2014); 
Huff K., Goodman D.P., The demand for cognitive diagnostic assessment, Cognitive diagnostic assessment for education: Theory and applications, pp. 19-60, (2007); 
Jiang Z., Using the linear mixed-effect model framework to estimate generalizability variance components in R, Methodology, 14, pp. 133-142, (2018); 
Jiang Z., Raymond M.R., The use of multivariate generalizability theory to evaluate the quality of subscores, Applied Psychological Measurement, 42, pp. 595-612, (2018); 
Klauer K.C., Rettig K., An approximately standardized person test for assessing consistency with a latent trait model, British Journal of Mathematical and Statistical Psychology, 43, pp. 193-206, (1990); 
Livingston S.A., A note on subscores, Educational Measurement: Issues and Practice, 34, (2015); 
Lord F.M., Estimating test reliability, Educational and Psychological Measurement, 15, pp. 325-336, (1955); 
Plake B.S., Reynolds C.R., Gutkin T.B., A technique for the comparison of profile variability between independent groups, Journal of Clinical Psychology, 37, pp. 142-146, (1981); 
Puhan G., Sinharay S., Haberman S.J., Larkin K., The utility of augmented subscores in a licensure exam: An evaluation of methods using empirical data, Applied Measurement in Education, 23, pp. 266-285, (2010); 
Raju N.S., Price L.R., Oshima T.C., Nering M.L., Standardized conditional SEM: A case for conditional reliability, Applied Psychological Measurement, 31, pp. 169-180, (2007); 
Raymond M.R., Swygert K.A., Kahraman N., Psychometric equivalence of ratings for repeat examinees on a performance assessment for physician licensure, Journal of Educational Measurement, 49, pp. 339-361, (2012); 
Reckase M.D., Xu J.R., The evidence for a subscore structure in a test of English language competence for English language learners, Educational and Psychological Measurement, 75, pp. 805-825, (2015); 
Reardon S.F., Kalogrides D., Fahle E.M., Podolsky A., Zarate R.C., The relationship between test item format and gender achievement gaps on math and ELA tests in fourth and eighth grades, Educational Researcher, 47, pp. 284-294, (2018); 
Rodriguez M.C., Three options are optimal for multiple-choice test items: A meta-analysis of 80 years of research, Educational Measurement: Issues & Practice, 24, pp. 3-13, (2005); 
Sinharay S., How often do subscores have added value? Results from operational and simulated data, Journal of Educational Measurement, 47, pp. 150-174, (2010); 
Sinharay S., A note on assessing the added value of subscores, Educational Measurement: Issues and Practice, 32, pp. 38-42, (2013); 
Sinharay S., Haberman S.J., An empirical investigation of population invariance in the value of subscores, International Journal of Testing, 14, pp. 22-48, (2014); 
Sinharay S., Haberman S.J., Puhan G., Subscores based on classical test theory: To report or not to report, Educational Measurement: Issues and Practice, 26, pp. 21-28, (2007); 
Stone C.A., Ye F., Zhu X., Lane S., Providing subscale scores for diagnostic information: A case study for when the test is essentially unidimensional, Applied Measurement in Education, 23, pp. 63-86, (2010); 
Thissen D., Wainer H., Wang X.B., Are tests comprising both multiple-choice and free-response items necessarily less unidimensional than multiple-choice tests? An analysis of two tests, Journal of Educational Measurement, 31, pp. 113-123, (1994); 
Van der Maas H.L.J., Molenaar D., Maris G., Kievit R.A., Borsboom D., Cognitive psychology meets psychometric theory: On the relation between process models for decision making and latent variable models for individual differences, Psychological Review, 118, pp. 339-356, (2011)#FRF#
