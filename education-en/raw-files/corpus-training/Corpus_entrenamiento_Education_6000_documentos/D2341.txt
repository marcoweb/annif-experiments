#ITI#Multidimensional Forced-Choice CAT With Dominance Items: An Empirical Comparison With Optimal Static Testing Under Different Desirability Matching#FTI#
#IRE# Several forced-choice (FC) computerized adaptive tests (CATs) have emerged in the field of organizational psychology, all of them employing ideal-point items. However, despite most items developed historically follow dominance response models, research on FC CAT using dominance items is limited. Existing research is heavily dominated by simulations and lacking in empirical deployment. This empirical study trialed a FC CAT with dominance items described by the Thurstonian Item Response Theory model with research participants. This study investigated important practical issues such as the implications of adaptive item selection and social desirability balancing criteria on score distributions, measurement accuracy and participant perceptions. Moreover, nonadaptive but optimal tests of similar design were trialed alongside the CATs to provide a baseline for comparison, helping to quantify the return on investment when converting an otherwise-optimized static assessment into an adaptive one. Although the benefit of adaptive item selection in improving measurement precision was confirmed, results also indicated that at shorter test lengths CAT had no notable advantage compared with optimal static tests. Taking a holistic view incorporating both psychometric and operational considerations, implications for the design and deployment of FC assessments in research and practice are discussed.#FRE#
#IPC# computerized adaptive testing; forced choice; multidimensional item response theory; personality; Thurstonian IRT model#FPC#
#IRF# Andrich D., Hyperbolic cosine latent trait models for unfolding direct responses and pairwise preferences, Applied Psychological Measurement, 19, 3, pp. 269-290, (1995); 
Ashton M.C., Lee K., The HEXACO-60: A short measure of the major dimensions of personality, Journal of Personality Assessment, 91, pp. 340-345, (2009); 
Ashton M.C., Lee K., Perugini M., Szarota P., de Vries R.E., Di Blas L., Boies K., De Raad B., A six-factor structure of personality-descriptive adjectives: Solutions from psycholexical studies in seven languages, Journal of Personality and Social Psychology, 86, 2, pp. 356-366, (2004); 
Ben-Shachar M., Ludecke D., Makowski D., effectsize: Estimation of effect size indices and standardized parameters, Journal of Open Source Software, 5, 56, (2020); 
Birkeland S.A., Manson T.M., Kisamore J.L., Brannick M.T., Smith M.A., A meta-analytic investigation of job applicant faking on personality measures, International Journal of Selection and Assessment, 14, 4, pp. 317-335, (2006); 
Boyce A.S., Conway J.S., Caputo P.M., ADEPT-15 technical documentation: Development and validation of Aon Hewitt’s personality model and adaptive employee test (ADEPT-15), (2014); 
Brown A., Personality assessment, forced-choice, International encyclopedia of the social & behavioral sciences, pp. 840-848, (2015); 
Brown A., Item response models for forced-choice questionnaires: A common framework, Psychometrika, 81, 1, pp. 135-160, (2016); 
Brown A., Bartram D., Doing less but getting more: Improving forced-choice measures with IRT, (2009); 
Brown A., Bartram D., OPQ32r technical manual, (2009); 
Brown A., Inceoglu I., Lin Y., Preventing rater biases in 360-degree feedback by forcing choice, Organizational Research Methods, 20, 1, pp. 121-148, (2017); 
Brown A., Maydeu-Olivares A., Issues that should not be overlooked in the dominance versus ideal point controversy, Industrial and Organizational Psychology, 3, 4, pp. 489-493, (2010); 
Brown A., Maydeu-Olivares A., Item response modeling of forced-choice questionnaires, Educational and Psychological Measurement, 71, 3, pp. 460-502, (2011); 
Brown A., Maydeu-Olivares A., How IRT can solve problems of ipsative data in forced-choice questionnaires, Psychological Methods, 18, 1, pp. 36-52, (2013); 
Brown A., Maydeu-Olivares A., Ordinal factor analysis of graded-preference questionnaire data, Structural Equation Modeling, 25, 4, pp. 516-529, (2017); 
Cao M., Drasgow F., Does forcing reduce faking? A meta-analytic review of forced-choice personality measures in high-stakes situations, Journal of Applied Psychology, 104, 11, pp. 1347-1368, (2019); 
Chang H., Ying Z., A global information approach to computerized adaptive testing, Applied Psychological Measurement, 20, 3, pp. 213-229, (1996); 
Chen C., Wang W., Chiu M.M., Ro S., Item selection and exposure control methods for computerized adaptive testing with multidimensional ranking items, Journal of Educational Measurement, 57, 2, pp. 343-369, (2019); 
Chernyshenko O.S., Stark S., Prewett M.S., Gray A.A., Stilson F.R., Tuttle M.D., Normative scoring of multidimensional pairwise preference personality scales using IRT: Empirical comparisons with other formats, Human Performance, 22, 2, pp. 105-127, (2009); 
Cheung M.W.L., Chan W., Reducing uniform response bias with ipsative measurement in multiple-group confirmatory factor analysis, Structural Equation Modeling, 9, 1, pp. 55-77, (2002); 
Christiansen N.D., Burns G.N., Montgomery G.E., Reconsidering forced-choice item formats for applicant personality assessment, Human Performance, 18, 3, pp. 267-307, (2005); 
Clemans W.V., An analytical and empirical examination of some properties of ipsative measures, (1966); 
Coombs C.H., A theory of data, (1964); 
Cornwell J.M., Dunlap W.P., On the questionable soundness of factoring ipsative data: A response to Saville & Wilson (1991), Journal of Occupational and Organizational Psychology, 67, 2, pp. 89-100, (1994); 
Cover T.M., Thomas J.A., Elements of information theory, (2006); 
Davey T., Nering M.L., Controlling item exposure and maintaining item security, Computer-based testing: Building the foundation for future assessments, pp. 165-191, (2002); 
de Mendiburu F., agricolae: Statistical Procedures for Agricultural Research, (2020); 
Drasgow F., Stark S., Chernyshenko O.S., Nye C.D., Hulin C.L., White L.A., Development of the tailored adaptive personality assessment system (TAPAS) to support army selection and classification decisions (tech. rep. no. 1311), (2012); 
Forero C.G., Maydeu-Olivares A., Estimation of IRT graded response models: Limited versus full information methods, Psychological Methods, 14, 3, pp. 275-299, (2009); 
Fox J., Weisberg S., An {R} Companion to applied regression, (2019); 
Goldberg L.R., Johnson J.A., Eber H.W., Hogan R., Ashton M.C., Cloninger C.R., Gough H.G., The international personality item pool and the future of public-domain personality measures, Journal of Research in Personality, 40, 1, pp. 84-96, (2006); 
Hicks L.E., Some properties of ipsative, normative, and forced-choice normative measures, Psychological Bulletin, 74, 3, pp. 167-184, (1970); 
Hirsh J.B., Peterson J.B., Predicting creativity and academic success with a “Fake-proof” measure of the Big Five, Journal of Research in Personality, 42, 5, pp. 1323-1333, (2008); 
Houston J.S., Borman W.C., Farmer W.L., Bearden R.M., Development of the navy computer adaptive personality scales (NCAPS), (2006); 
Jackson D.N., Wroblewski V.R., Ashton M.C., The impact of faking on employment tests: Does forced choice offer a solution?, Human Performance, 13, 4, pp. 371-388, (2000); 
Johnson C.E., Wood R., Blinkhorn S.F., Spuriouser and spuriouser: The use of ipsative personality tests, Journal of Occupational Psychology, 61, 2, pp. 153-162, (1988); 
Joo S., Lee P., Stark S., Adaptive testing with the GGUM-RANK multidimensional forced choice model: Comparison of pair, triplet, and tetrad scoring, Behavior Research Methods, 52, pp. 761-772, (2019); 
Krug R.E., A selection set preference index, Journal of Applied Psychology, 42, 3, pp. 168-170, (1958); 
Kullback S., Information theory and statistics, (1959); 
Kuncel N.R., Tellegen A., A conceptual and empirical reexamination of the measurement of the social desirability of items: Implications for detecting desirable response style and scale development, Personnel Psychology, 62, 2, pp. 201-228, (2009); 
Lee K., Ashton M.C., The HEXACO personality factors in the indigenous personality lexicons of English and 11 other languages, Journal of Personality, 76, 5, pp. 1001-1054, (2008); 
Lee K., Ashton M.C., The HEXACO personality inventory—Revised: Scale descriptions, (2009); 
Lee P., Joo S., Lee S., Examining stability of personality profile solutions between likert-type and multidimensional forced choice measure, Personality and Individual Differences, 142, pp. 13-20, (2019); 
Lehmann E.L., Casella G., Theory of point estimation, (1998); 
Lin Y., Asking the right questions: Increasing fairness and accuracy of personality assessments with computerised adaptive testing, (2020); 
Lin Y., Brown A., Influence of context on item parameters in forced-choice personality assessments, Educational and Psychological Measurement, 77, 3, pp. 389-414, (2017); 
Lord F.M., Maximum likelihood and Bayesian parameter estimation in item response theory, Journal of Educational Measurement, 23, pp. 157-162, (1986); 
Martin B.A., Bowen C.C., Hunt S.T., How effective are people at faking on personality questionnaires?, Personality and Individual Differences, 32, 2, pp. 247-256, (2002); 
Maydeu-Olivares A., Hernandez A., McDonald R.P., A multidimensional ideal point item response theory model for binary data, Multivariate Behavioral Research, 41, 4, pp. 445-471, (2006); 
Meade A.W., Psychometric problems and issues involved with creating and using ipsative measures for selection, Journal of Occupational and Organizational Psychology, 77, 4, pp. 531-551, (2004); 
Merk J., Schlotz W., Falter T., The Motivational Value Systems Questionnaire (MVSQ): Psychometric analysis using a forced choice Thurstonian IRT model, Frontiers in Psychology, 8, (2017); 
Mislevy R.J., Bayes modal estimation in item response models, Psychometrika, 51, pp. 177-195, (1986); 
Morillo D., Abad F.J., Kreitchmann R.S., Leenen I., Hontangas P., Ponsoda V., The journey from likert to forced-choice Questionnaires: Evidence of the invariance of item parameters, Journal of Work and Organizational Psychology, 35, 2, pp. 75-83, (2019); 
Mulder J., van der Linden W.J., Multidimensional adaptive testing with optimal design criteria for item selection, Psychometrika, 74, 2, pp. 273-296, (2009); 
Mulder J., van der Linden W.J., Multidimensional adaptive testing with Kullback-Leibler information item selection, Elements of adaptive testing, pp. 77-101, (2010); 
O'Neill T.A., Lewis R.J., Law S.J., Larson N., Hancock S., Radan J., Lee N., Carswell J.J., Forced-choice pre-employment personality assessment: Construct validity and resistance to faking, Personality and Individual Differences, 115, pp. 120-127, (2017); 
Pavlov G., Maydeu-Olivares A., Fairchild A.J., Effects of applicant faking on forced-choice and likert scores, Organizational Research Methods, 22, 3, pp. 710-739, (2019); 
Reckase M.D., Multidimensional item response theory, (2009); 
Salgado J.F., Moderator effects of job complexity on the validity of forced-choice personality inventories for predicting job performance, Journal of Work and Organizational Psychology, 33, 3, pp. 229-238, (2017); 
Salgado J.F., Anderson N., Tauriz G., The validity of ipsative and quasi-ipsative forced-choice personality inventories for different occupational groups: A comprehensive meta-analysis, Journal of Occupational and Organizational Psychology, 88, 4, pp. 797-834, (2015); 
Salgado J.F., Tauriz G., The five-factor model, forced-choice personality inventories and performance: A comprehensive meta-analysis of academic and occupational validity studies, European Journal of Work and Organizational Psychology, 23, 1, pp. 3-30, (2014); 
Segall D.O., Multidimensional adaptive testing, Psychometrika, 61, 2, pp. 331-354, (1996); 
Seo D.G., Weiss D.J., Best design for multidimensional computerized adaptive testing with the bifactor model, Educational and Psychological Measurement, 75, 6, pp. 954-978, (2015); 
Global personality inventory—Adaptive: Technical manual, (2009); 
Silvey S.D., Optimal design: An introduction to the theory for parameter estimation, (1980); 
Singmann H., Bolker B., Westfall J., Aust F., Ben-Shachar M.S., afex: Analysis of factorial experiments, (2021); 
Stark S., A new IRT approach to test construction and scoring designed to reduce the effects of faking in personality assessment, (2002); 
Stark S.E., A new IRT approach to test construction and scoring designed to reduce the effects of faking in personality assessment: The generalized graded unfolding model for multi -unidimensional paired comparison responses (Order No. 3044232), (2002); 
Stark S., Chernyshenko O.S., Computerized adaptive testing with the Zinnes and Griggs pairwise preference ideal point model, International Journal of Testing, 11, 3, pp. 231-247, (2011); 
Stark S., Chernyshenko O.S., Drasgow F., An IRT approach to constructing and scoring pairwise preference items involving stimuli on different dimensions: The multi-unidimensional pairwise-preference model, Applied Psychological Measurement, 29, pp. 184-203, (2005); 
Stark S., Chernyshenko O.S., Drasgow F., White L.A., Adaptive testing with multidimensional pairwise preference items: Improving the efficiency of personality and other noncognitive assessments, Organizational Research Methods, 15, 3, pp. 463-487, (2012); 
Usami S., Sakamoto A., Naito J., Abe Y., Developing pairwise preference-based personality test and experimental investigation of its resistance to faking effect by item response model, International Journal of Testing, 16, 4, pp. 288-309, (2016); 
Veldkamp B.P., van der Linden W.J., Multidimensional adaptive testing with constraints on test content, Psychometrika, 67, 4, pp. 575-588, (2002); 
Wang C., Chang H., Item selection in MCAT—The new application of Kullback–Leibler information, (2010); 
Wang C., Chang H., Item selection in multidimensional computerized adaptive testing: Gaining information from different angles, Psychometrika, 76, 3, pp. 363-384, (2011); 
Wang W.-C., Qiu X.-L., Chen C.-W., Ro S., Jin K.-Y., Item response theory models for Ipsative tests with multidimensional pairwise comparison items, Applied Psychological Measurement, 41, 8, pp. 600-613, (2017); 
Watrin L., Geiger M., Spengler M., Wilhelm O., Forced- choice versus Likert responses on an occupational Big Five questionnaire, Journal of Individual Differences, 40, pp. 134-148, (2019); 
Weissman A., Mutual information item selection in adaptive classification testing, Educational and Psychological Measurement, 67, 1, pp. 41-58, (2007); 
Zinnes J.L., Griggs R.A., Probabilistic, multidimensional unfolding analysis, Psychometrika, 39, 3, pp. 327-350, (1974)#FRF#
