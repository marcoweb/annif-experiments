#ITI#Assessing high-school students' modeling performance on Newtonian mechanics#FTI#
#IRE# Assessing scientific modeling competence (SMC) is challenging because of the multi-dimensionality of the construct and the potential variability of student performance across tasks. To deal with the challenges, this study applied Kane's validity framework to assess high-school students' SMC in Newtonian mechanics and examined how students' performance depended on tasks. We first specified students' SMC in three dimensions: Conceptualization, Deploying Scientific Knowledge, and Representation, incorporating 11 components (e.g., conceptualization of objects). We assessed 305 high-school students' modeling performance on two Newtonian mechanics tasks, taking task dependency into account. We applied the Many-facet Rasch measurement (MFRM) to examine students' ability and discriminate students' performance variability across tasks. MFRM analyses revealed that among the three dimensions of SMC, Representation is most challenging for students to achieve compared to Conceptualization and Deploying Scientific Knowledge, especially among higher-performing students. Five out of the 11 SMC components were task-dependent, suggesting significant score variance due to the variability of tasks. After excluding task variance, we found that students were grouped into four levels (i.e., primitive non-model, intuitive model, qualitative model, and quantitative model), and the majority of students were distributed at the medium two levels. We conclude the study by introducing three putative theories that may account for task dependency: depth of cognition, knowledge transfer, and construct irrelevant interest. The study suggests that task dependency does not necessarily compromise the interpretation of scores, as long as assessors appropriately consider potential theories in their interpretive arguments when designing tasks and interpreting scores#FRE#
#IPC# competence; construct irrelevant interest; depth of cognition; force and interaction; knowledge transfer; many-facet Rasch measurement; Newtonian mechanics; scientific modeling; task dependency; validity#FPC#
#IRF# Alonzo A.C., Steedle J.T., Developing and accessing a force and motion learning progression, Science Education, 93, 3, pp. 389-421, (2009); 
Avenia-Tapper B., Llosa L., Construct relevant or irrelevant? The role of linguistic complexity in the assessment of English language learners' science knowledge, Educational Assessment, 20, 2, pp. 95-111, (2015); 
Aviani I., Erceg N., Mesic V., Drawing and using free body diagrams: Why it may be better not to decompose forces, Physical Review Special Topics - Physics Education Research, 11, 2, pp. 1-14, (2015); 
Bamberger Y.M., Davis E.A., Middle-school science students' scientific modeling performances across content areas and within a learning progression, International Journal of Science Education, 35, 2, pp. 213-238, (2013); 
Bao L., Hogg K., Zollman D., Model analysis of fine structures of student models: An example with Newton's third law, American Journal of Physics, 70, 7, pp. 766-778, (2002); 
Bathke A.C., Schabenberger O., Tobias R.D., Madden L.V., Greenhouse–Geisser adjustment and the ANOVA-type statistic: Cousins or twins?, The American Statistician, 63, 3, pp. 239-246, (2009); 
Baxter G.P., Glaser R., Investigating the cognitive complexity of science assessments, Educational Measurement: Issues and Practice, 15, 1, pp. 37-45, (1998); 
Benjamin D., Wright's annotated key math diagnostic profile. B.D. Wright … Rasch measurement, Transactions, 25, 4, (2012); 
Bond T., Fox C.M., Applying the Rasch model: Fundamental measurement in the human sciences, (2015); 
Boone W.J., Staver J.R., Yale M.S., Rasch analysis in the human sciences, (2014); 
Boone W.J., Townsend J.S., Staver J.R., Utilizing multifaceted Rasch measurement through FACETS to evaluate science education data sets composed of judges, respondents, and rating scale items: An exemplar utilizing the elementary science teaching analysis matrix instrument, Science Education, 100, 2, pp. 221-238, (2016); 
Bowden J., Dall'Alba G., Martin E., Laurillard D., Marton F., Masters G., Ramsden P., Stephanou A., Walsh E., Displacement, velocity and frames of reference: Phenomenon graphic studies of students' understanding and some implications for teaching and assessment, American Journal of Physics, 60, pp. 262-269, (1992); 
Briggs D.C., Wilson M., An introduction to multidimensional measurement using Rasch models, Journal of Applied Measurement, 4, 1, pp. 87-100, (2003); 
Capps D.K., Shemwell J.T., Moving beyond the model as a copy problem: Investigating the utility of teaching about structure-preserving transformations in the model-referent relationship, International Journal of Science Education, 42, 12, pp. 2008-2031, (2020); 
Caspar M., Kepler, (2012); 
Cheng M.F., Brown D.E., The role of scientific modeling criteria in advancing students' explanatory ideas of magnetism, Journal of Research in Science Teaching, 52, 8, pp. 1053-1081, (2015); 
Cheng M.F., Lin J.L., Investigating the relationship between students' views of scientific models and their development of models, International Journal of Science Education, 37, 15, pp. 2453-2475, (2015); 
Chi M.T., Feltovich P.J., Glaser R., Categorization and representation of physics problems by experts and novices, Cognitive Science, 5, 2, pp. 121-152, (1981); 
Chi S., Liu X., Wang Z., Comparing student science performance between hands-on and traditional item types: A many-facet Rasch analysis, Studies in Educational Evaluation, 70, (2021); 
Chi S., Wang Z., Liu X., Investigating disciplinary context effect on student scientific inquiry competence, International Journal of Science Education, 41, 18, pp. 2736-2764, (2019); 
Curriculum and standards in physics, (2017); 
Chiu M.H., Lin J.W., Modeling competence in science education, Disciplinary and Interdisciplinary Science Education Research, 1, 1, pp. 1-11, (2019); 
Clement J., Model-based learning as a key research area for science education, International Journal of Science Education, 22, 9, pp. 1041-1053, (2000); 
Clement J., Creative model construction in scientists and students: The role of imagery, analogy and mental simulation, (2008); 
Clement J., Reasoning patterns in Galileo's analysis of machines and in expert protocols: Roles for analogy, imagery, and mental simulation, Topoi, 39, 4, pp. 973-985, (2020); 
Cronbach L.J., Test validation, Educational measurement, pp. 443-507, (1971); 
Cronbach L.J., Construct validation after thirty years, Intelligence: Measurement, theory, and public policy, pp. 147-171, (1989); 
Cronbach L.J., Meehl P.E., Construct validity in psychological tests, Psychological Bulletin, 52, 4, pp. 1-28, (1955); 
Duncan Seraphin K., Harrison G.M., Philippoff J., Brandon P.R., Nguyen T.T.T., Lawton B.E., Vallin L.M., Teaching aquatic science as inquiry through professional development: Teacher characteristics and student outcomes, Journal of Research in Science Teaching, 54, 9, pp. 1219-1245, (2017); 
Eckes T., Introduction to many-facet Rasch measurement: Analyzing and evaluating rater-mediated assessments, (2015); 
Fensham P.J., Real world contexts in PISA science: Implications for context-based science education, Journal of Research in Science Teaching, 46, 8, pp. 884-896, (2009); 
Fortus D., Restructuring school physics around real-world problems: A cognitive justification, (2005); 
Fortus D., Shwartz Y., Rosenfeld S., High school students' meta-modeling knowledge, Research in Science Education, 46, 6, pp. 787-810, (2016); 
Galilei G., On motion, and on mechanics: Comprising de motu (ca. 1590) (trans: Drabkin I, Drake S), 5, (1960); 
Giere R.N., Bickle J., Maudlin R.F., Understanding scientific reasoning, (2006); 
Gilbert J.K., Multiple representations in chemical education, 4, (2009); 
Gilbert J.K., Justi R., Modelling-based teaching in science education, 9, (2016); 
Grosslight L., Unger C., Jay E., Smith C.L., Understanding models and their use in science - conceptions of middle and high-school students and experts, Journal of Research in Science Teaching, 28, 9, pp. 799-822, (1991); 
Haladyna T.M., Downing S.M., Construct-irrelevant variance in high-stakes testing, Educational Measurement: Issues and Practice, 23, 1, pp. 17-27, (2004); 
Halloun I.A., Schematic modeling for meaningful learning of physics, Journal of Research in Science Teaching, 33, 9, pp. 1019-1041, (1996); 
Harris C.J., Krajcik J.S., Pellegrino J.W., DeBarger A.H., Designing knowledge-in-use assessments to promote deeper learning, Educational Measurement: Issues and Practice, 38, 2, pp. 53-67, (2019); 
Hesse M.B., Forces and fields. The concept of action at a distance in the history of physics, (1962); 
Hestenes D., Modeling games in the Newtonian world, American Journal of Physics, 60, 8, pp. 732-748, (1992); 
Hestenes D., Modeling theory for math and science education, Modeling students' mathematical modeling competencies, pp. 13-41, (2010); 
Hestenes D., Modeling theory for math and science education, Modeling students' mathematical modeling competencies, pp. 13-41, (2013); 
Hestenes D., Wells M., Swackhamer G., Force concept inventory, The Physics Teacher, 30, pp. 141-158, (1992); 
Johnson-Laird P.N., How we reason, (2006); 
Justi R.S., Gilbert J.K., Modelling, teachers' views on the nature of modelling, and implications for the education of modellers, International Journal of Science Education, 24, 4, pp. 369-387, (2002); 
Kane M., Validating score interpretations and uses, Language Testing, 29, 1, pp. 3-17, (2012); 
Kane M., Validating the interpretations and uses of test scores, Journal of Educational Measurement, 50, 1, pp. 1-73, (2013); 
Kane M.T., Current concerns in validity theory, Journal of Educational Measurement, 38, 4, pp. 319-342, (2001); 
Ke L., Schwarz C.V., Supporting students' meaningful engagement in scientific modeling through epistemological messages: A case study of contrasting teaching approaches, Journal of Research in Science Teaching, 58, 3, pp. 335-365, (2021); 
Kelly A.M., Teaching Newton's laws with the iPod touch in conceptual physics, The Physics Teacher, 49, 4, pp. 202-205, (2011); 
Kleer J., Brown J.S., Assumptions and ambiguities in mechanistic mental models, Mental models, pp. 155-190, (1983); 
Krajcik J., Merritt J., Engaging students in scientific practices: What does constructing and revising models look like in the science classroom?, The Science Teacher, 79, 3, (2012); 
Krell M., Belzen A.U.Z., Kruger D., Students' levels of understanding models and modeling in biology: Global or aspect-dependent?, Research in Science Education, 44, 1, pp. 109-132, (2014); 
Lazenby K., Stricker A., Brandriet A., Rupp C.A., Mauger-Sonnek K., Becker N.M., Mapping undergraduate chemistry students' epistemic ideas about models and modeling, Journal of Research in Science Teaching, 57, 5, pp. 794-824, (2020); 
Lehrer R., Schauble L., Cultivating model-based reasoning in science education, The Cambridge handbook of: The learning sciences, pp. 371-387, (2006); 
Lehrer R., Schauble L., Seeding evolutionary thinking by engaging children in modeling its foundations, Science Education, 96, 4, pp. 701-724, (2012); 
Lehrer R., Schauble L., The development of scientific thinking, The development of scientific thinking. In handbook of child psychology and developmental science, (2015); 
Lesh R., Doerr H.M., Foundations of a models and modeling perspective on mathematics teaching, learning, and problem solving, Beyond constructivism: Models and modeling perspectives on mathematics problem solving, learning, and teaching, pp. 3-33, (2003); 
Linacre J.M., Many faceted Rasch measurement, (1989); 
Linacre J.M., Facets (version 3.67.0) [Rasch measurement computer software], (2014); 
Liu X., Using and developing measurement instruments in science education: A Rasch modeling approach, (2020); 
Lopes J.B., Costa N., The evaluation of modeling competencies: Difficulties and potentials for the learning of the sciences, International Journal of Science Education, 29, 7, pp. 811-851, (2007); 
Marshall J.A., Carrejo D.J., Students' mathematical modeling of motion, Journal of Research in Science Teaching, 45, 2, pp. 153-173, (2008); 
Mauchly J.W., Significance test for sphericity of a normal n-variate distribution, The Annals of Mathematical Statistics, 11, 2, pp. 204-209, (1940); 
McDermott L.C., A perspective on teacher preparation in physics and other sciences: The need for special science courses for teachers, American Journal of Physics, 58, pp. 734-742, (1990); 
Mintzes J.J., Wandersee J.H., Novak J.D., Assessing science understanding, Assessing science understanding: A human constructivist view, (2005); 
Mulder Y.G., Bollen L., de Jong T., Lazonder A.W., Scaffolding learning by modelling: The effects of partially worked-out models, Journal of Research in Science Teaching, 53, 3, pp. 502-523, (2016); 
Namdar B., Shen J., Modeling-oriented assessment in K-12 science education: A synthesis of research from 1980 to 2013 and new directions, International Journal of Science Education, 37, 7, pp. 993-1023, (2015); 
A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
Nehm R.H., Ha M., Item feature effects in evolution assessment, Journal of Research in Science Teaching, 48, 3, pp. 237-256, (2011); 
Neumann K., Viering T., Boone W.J., Fischer H.E., Towards a learning progression of energy, Journal of Research in Science Teaching, 50, 2, pp. 162-188, (2013); 
Next generation science standards: For states, by states, (2013); 
Nicolaou C.T., Constantinou C.P., Assessment of the modeling competence: A systematic review and synthesis of empirical research, Educational Research Review, 13, pp. 52-73, (2014); 
(2020); 
Orban G., Dupont P., Vogels R., De Bruyn B., Bormans G., Mortelmans L., Task dependency of visual processing in the human visual system, Behavioural Brain Research, 76, 1-2, pp. 215-223, (1996); 
Osborne J.F., Not “hands on” but “minds on”: A response to Furtak and Penuel, Science Education, 103, 5, pp. 1280-1283, (2019); 
Otten L.J., Rugg M.D., Task dependency of the neural correlates of episodic encoding as measured by fMRI, Cerebral Cortex, 11, 12, pp. 1150-1160, (2001); 
Papaevripidou M., Constantinou C.P., Zacharia Z.C., Modeling complex marine ecosystems: An investigation of two teaching approaches with fifth graders, Journal of Computer Assisted Learning, 23, 2, pp. 145-157, (2007); 
Pellegrino J.W., Hilton M.L., Education for life and work: Developing transferable knowledge and skills in the 21st- century, (2012); 
Pellegrino J.W., Wilson M.R., Koenig J.A., Beatty A.S., Developing assessments for the next generation science standards, (2014); 
Russ R.S., Odden T.O.B., Intertwining evidence- and model-based reasoning in physics sensemaking: An example from electrostatics, Physical Review Physics Education Research, 13, 2, (2017); 
Sbeglia G.C., Nehm R.H., Do you see what I-SEA? A Rasch analysis of the psychometric properties of the Inventory of Student Evolution Acceptance, Science Education, 103, 2, pp. 287-316, (2019); 
Schwarz C.V., Passmore C., Reiser B.J., Helping students make sense of the world using next generation science and engineering practices, (2017); 
Schwarz C.V., Reiser B.J., Davis E.A., Kenyon L., Acher A., Fortus D., Shwartz Y., Hug B., Krajcik J., Developing a learning progression for scientific modeling: Making scientific modeling accessible and meaningful for learners, Journal of Research in Science Teaching, 46, 6, pp. 632-654, (2009); 
Shavelson R.J., On an approach to testing and modeling competence, Educational Psychologist, 48, 2, pp. 73-86, (2013); 
Shavelson R.J., Baxter G.P., Gao X., Sampling variability of performance assessments, Journal of Educational Measurement, 30, 3, pp. 215-232, (1993); 
Shemwell J.T., Capps D.K., Learning abstraction as a modeling competence, Towards a competence-based view on models and modeling in science education, pp. 291-307, (2019); 
Sherin B., Common sense clarified: Intuitive knowledge and its role in physics expertise, Journal of Research in Science Teaching, 43, pp. 535-555, (2006); 
Sins P.H., Savelsbergh E.R., van Joolingen W.R., van Hout-Wolters B.H., The relation between students' epistemological understanding of computer models and their cognitive processing on a modelling task, International Journal of Science Education, 31, 9, pp. 1205-1229, (2009); 
Solano-Flores G., Li M., Generalizability theory and the fair and valid assessment of linguistic minorities, Educational Research and Evaluation, 19, 2-3, pp. 245-263, (2013); 
Stewart J., Techniques for assessing and representing information in cognitive structure, Science Education, 64, 2, pp. 223-235, (1980); 
Stewart J., Griffin H., Stewart G., Context sensitivity in the force concept inventory, Physical Review Special Topics-Physics Education Research, 3, 1, (2007); 
Stratford S.J., Krajcik J., Soloway E., Secondary students' dynamic modeling processes: Analyzing, reasoning about, synthesizing, and testing models of stream ecosystems, Journal of Science Education and Technology, 7, 3, pp. 215-234, (1998); 
Treagust D.F., Chittleborough G., Mamiala T.L., Students' understanding of the role of scientific models in learning science, International Journal of Science Education, 24, 4, pp. 357-368, (2002); 
Upmeier zu Belzen A., Driel J.V., Kruger D., Introducing a framework for modeling competence, Towards a competence-based view on models and modeling in science education, pp. 3-19, (2019); 
Wright B.D., Stone M., Best test design: Rasch measurement, (1979); 
Wu H.K., Puntambekar S., Pedagogical affordances of multiple external representations in scientific processes, Journal of Science Education and Technology, 21, 6, pp. 754-767, (2012); 
Wu M.L., Adams R.J., Wilson M.R., Haldane S., ACER ConQuest 2.0: General item response modelling software [computer program manual], (2007); 
You H.S., Marshall J.A., Delgado C., Assessing students' disciplinary and interdisciplinary understanding of global carbon cycling, Journal of Research in Science Teaching, 55, 3, pp. 377-398, (2018); 
Zhai X., Haudek K., Shi L., Nehm R.H., Urban-Lurain M., From substitution to redefinition: A framework of machine learning-based science assessment, Journal of Research in Science Teaching, 57, 9, pp. 1430-1459, (2020); 
Zhai X., Haudek K.C., Stuhlsatz M.A., Wilson C., Evaluation of construct-irrelevant variance yielded by machine and human scoring of a science teacher PCK constructed response assessment, Studies in Educational Evaluation, 67, pp. 1-12, (2020); 
Zhai X., Haudek K.C., Wilson C., Stuhlsatz M.A., A framework of construct-irrelevant variance for contextualized constructed-response assessment, Frontiers in Education, 6, pp. 1-13, (2021); 
Zhai X., Krajcik J., Pellegrino J., On the validity of machine learning-based next generation science assessments: A validity inferential network, Journal of Science Education and Technology, 30, 2, pp. 298-312, (2021); 
Zhai X., Li M., Validating a partial-credit scoring approach for multiple-choice science items: An application of fundamental ideas in science, International Journal of Science Education, 43, 10, pp. 1640-1666, (2021); 
Zhai X., Pellegrino J.W., Large-scale assessment in science education, Handbook of research in science education, 3; 
Zhang B., Liu X., Krajcik J.S., Expert models and modeling processes associated with a computer-modeling tool, Science Education, 90, 4, pp. 579-604, (2006); 
Zwickl B.M., Hu D., Finkelstein N., Lewandowski H.J., Model-based reasoning in the physics laboratory: Framework and initial results, Physical Review Special Topics - Physics Education Research, 11, 2, pp. 1-12, (2015)#FRF#
