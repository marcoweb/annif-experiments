#ITI#Toward a System of Evidence for All: Current Practices and Future Opportunities in 37 Randomized Trials#FTI#
#IRE# As a result of the evidence-based decision-making movement, the number of randomized trials evaluating educational programs and curricula has increased dramatically over the past 20 years. Policy makers and practitioners are encouraged to use the results of these trials to inform their decision making in schools and school districts. At the same time, however, little is known about the schools taking part in these randomized trials, both regarding how and why they were recruited and how they compare to populations in need of research. In this article, we report on a study of 37 cluster randomized trials funded by the Institute of Education Sciences between 2011 and 2015. Principal investigators of these grants were interviewed regarding the recruitment process and practices. Additionally, data on the schools included in 34 of these studies were analyzed to determine the general demographics of schools included in funded research, as well as how these samples compare to important policy relevant populations. We show that the types of schools included in research differ in a variety of ways from these populations. Large schools from large school districts in urban areas were overrepresented, whereas schools from small school districts in rural areas and towns are underrepresented. The article concludes with a discussion of how recruitment practices might be improved in order to meet the goals of the evidence-based decision-making movement#FRE#
#IPC# decision making; descriptive analysis; educational policy; evaluation; mixed methods; program evaluation; research methodology; research utilization; survey research#FPC#
#IRF# Chhin C.S., Taylor K.A., Wei W.S., Supporting a culture of replication: An examination of education and special education research grants funded by the Institute of Education Sciences, Educational Researcher, 47, 9, pp. 594-605, (2018); 
Cook T.D., Randomized experiments in educational policy research: A critical examination of the reasons the educational evaluation community has offered for not doing them, Educational Evaluation and Policy Analysis, 24, 3, pp. 175-199, (2002); 
Curtin R., Presser S., Singer E., Changes in telephone survey nonresponse over the past quarter century, Public Opinion Quarterly, 69, 1, pp. 87-98, (2005); 
Dong N., Maynard R., PowerUp!: A tool for calculating minimum detectable effect sizes and minimum required sample sizes for experimental and quasi-experimental design studies, Journal of Research on Educational Effectiveness, 6, 1, pp. 24-67, (2013); 
Request for applications: Cognition and student learning research grants (NCER-05-07), (2004); 
Request for applications: Education research grants (CFDA Number: 84.305A), (2016); 
Lortie-Forgues H., Inglis M., Rigorous large-scale educational RCTs are often uninformative: Should we be concerned?, Educational Researcher, 48, 3, pp. 158-166, (2019); 
Raudenbush S.W., Martinez A., Spybrook J., Strategies for improving precision in group-randomized experiments, Educational Evaluation and Policy Analysis, 29, 1, pp. 5-29, (2007); 
Saldana J., The coding manual for qualitative researchers, (2016); 
Shadish W., Cook T.D., Campbell D.T., Experimental and quasi-experimental designs for generalized causal inference, (2002); 
Singer E., The use of incentives to reduce nonresponse in household surveys, Survey Nonresponse, 51, pp. 163-177, (2002); 
Spybrook J., Are power analyses reported with adequate detail? Evidence from the first wave of group randomized trials funded by the Institute of Education Sciences, Journal of Research on Educational Effectiveness, 1, 3, pp. 215-235, (2008); 
Spybrook J., Kelcey B., Dong N., Power for detecting treatment by moderator effects in two-and three-level cluster randomized trials, Journal of Educational and Behavioral Statistics, 41, 6, pp. 605-627, (2016); 
Spybrook J., Lininger M., Cullen A., From planning to implementation: An examination of changes in the research design, sample size, and statistical power of group randomized trials launched by the Institute of Education Sciences, Journal of Research on Educational Effectiveness, 6, 4, pp. 396-420, (2013); 
Spybrook J., Raudenbush S.W., An examination of the precision and technical accuracy of the first wave of group-randomized trials funded by the Institute of Education Sciences, Educational Evaluation and Policy Analysis, 31, 3, pp. 298-318, (2009); 
Spybrook J., Shi R., Kelcey B., Progress in the past decade: An examination of the precision of cluster randomized trials funded by the US Institute of Education Sciences, International Journal of Research & Method in Education, 39, 3, pp. 255-267, (2016); 
Spybrook J., Zhang Q., Kelcey B., Dong N., Learning from cluster randomized trials in education: An assessment of the capacity of studies to determine what works, for whom, and under what conditions, Educational Evaluation and Policy Analysis, (2020); 
Stuart E.A., Bell S.H., Ebnesajjad C., Olsen R.B., Orr L.L., Characteristics of school districts that participate in rigorous national educational evaluations, Journal of Research on Educational Effectiveness, 10, 1, pp. 168-206, (2017); 
Stuart E.A., Cole S.R., Bradshaw C.P., Leaf P.J., The use of propensity scores to assess the generalizability of results from randomized trials, Journal of the Royal Statistical Society: Series A (Statistics in Society), 174, 2, pp. 369-386, (2011); 
Tipton E., Improving generalizations from experiments using propensity score subclassification: Assumptions, properties, and contexts, Journal of Educational and Behavioral Statistics, 38, 3, pp. 239-266, (2013); 
Tipton E., How generalizable is your experiment? An index for comparing experimental samples and populations, Journal of Educational and Behavioral Statistics, 39, 6, pp. 478-501, (2014); 
Tipton E., Fellers L., Caverly S., Vaden-Kiernan M., Borman G., Sullivan K., Ruiz D., Castilla V., Site selection in experiments: An assessment of site recruitment and generalizability in two scale-up studies, Journal of Research on Educational Effectiveness, 9, pp. 209-228, (2016); 
Tipton E., Hallberg K., Hedges L.V., Chan W., Implications of small samples for generalization: Adjustments and rules of thumb, Evaluation Review, 41, 5, pp. 472-505, (2017); 
Tipton E., Miller K., The generalizer, (2015); 
Tipton E., Olsen R.B., A review of statistical methods for generalizing from evaluations of educational interventions, Educational Researcher, (2018); 
Non-regulatory guidance: Using evidence to strengthen education investments, (2016); 
Common Core of Data, (2016); 
Wagner J.R., Adaptive survey design to reduce nonresponse bias, (2008); 
Wickham H., ggplot2: Elegant graphics for data analysis, (2016)#FRF#
