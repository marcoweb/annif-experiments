#ITI#A Polytomous Scoring Approach to Handle Not-Reached Items in Low-Stakes Assessments#FTI#
#IRE# In low-stakes assessments, some students may not reach the end of the test and leave some items unanswered due to various reasons (e.g., lack of test-taking motivation, poor time management, and test speededness). Not-reached items are often treated as incorrect or not-administered in the scoring process. However, when the proportion of not-reached items is high, these traditional approaches may yield biased scores and thereby threatening the validity of test results. In this study, we propose a polytomous scoring approach for handling not-reached items and compare its performance with those of the traditional scoring approaches. Real data from a low-stakes math assessment administered to second and third graders were used. The assessment consisted of 40 short-answer items focusing on addition and subtraction. The students were instructed to answer as many items as possible within 5 minutes. Using the traditional scoring approaches, students’ responses for not-reached items were treated as either not-administered or incorrect in the scoring process. With the proposed scoring approach, students’ nonmissing responses were scored polytomously based on how accurately and rapidly they responded to the items to reduce the impact of not-reached items on ability estimation. The traditional and polytomous scoring approaches were compared based on several evaluation criteria, such as model fit indices, test information function, and bias. The results indicated that the polytomous scoring approaches outperformed the traditional approaches. The complete case simulation corroborated our empirical findings that the scoring approach in which nonmissing items were scored polytomously and not-reached items were considered not-administered performed the best. Implications of the polytomous scoring approach for low-stakes assessments were discussed.#FRE#
#IPC# disengaged responding; low-stakes assessment; not-reached items; polytomous scoring; response time#FPC#
#IRF# Birnbaum A., Some latent ability models and their use in inferring an examinee’s ability, Statistical theories of mental test scores, pp. 397-479, (1968); 
Chalmers R.P., mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Custer M., Sharairi S., Swift D., Annual meeting of the National Council of Measurement in Education, Vancouver, British Columbia, Canada, A comparison of scoring options for omitted and not-reached items through the recovery of IRT parameters when utilizing the Rasch model and joint maximum likelihood estimation, (2012); 
Debeer D., Janssen R., De Boeck P., Modeling skipped and not-reached items using IRTrees, Journal of Educational Measurement, 54, 3, pp. 333-363, (2017); 
Glas C.A.W., Pimentel J.L., Modeling nonignorable missing data in speeded tests, Educational and Psychological Measurement, 68, 6, pp. 907-922, (2008); 
Goldhammer F., Kroehne U., Controlling individuals’ time spent on task in speeded performance measures: Experimental time limits, posterior time limits, and response time modeling, Applied Psychological Measurement, 38, 4, pp. 255-267, (2014); 
Hu L., Bentler P.M., Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6, 1, pp. 1-55, (1999); 
Kuhfeld M., Soland J., Using assessment metadata to quantify the impact of test disengagement on estimates of educational effectiveness, Journal of Research on Educational Effectiveness, 13, 1, pp. 147-175, (2020); 
Lindner M.A., Ludtke O., Nagy G., The onset of rapid-guessing behavior over the course of testing time: A matter of motivation and cognitive resources, Frontiers in Psychology, 10, (2019); 
List M.K., Koller O., Nagy G., A semiparametric approach for modeling not-reached items, Educational and Psychological Measurement, 79, 1, pp. 170-199, (2019); 
Ludlow L.H., O'Leary M., Scoring omitted and not-reached items: Practical data analysis implications, Educational and Psychological Measurement, 59, 4, pp. 615-630, (1999); 
Macaskill G., Adams R.J., Wu M.L., Scaling methodology and procedures for the mathematics and science competence, advanced mathematics and physics scale, Third international mathematics and science study: Implementation and analysis, pp. 91-120, (1998); 
Martin M.O., Mullis I.V.S., Kennedy A.M., PIRLS 2006 technical report, (2007); 
Masters G.N., A Rasch model for partial credit scoring, Psychometrika, 47, 2, pp. 149-174, (1982); 
Muraki E., A generalized partial credit model: Application of an EM algorithm, Applied Psychological Measurement, 16, 2, pp. 159-176, (1992); 
Okumura T., Empirical differences in omission tendency and reading ability in PISA: An application of tree-based item response models, Educational and Psychological Measurement, 74, pp. 611-626, (2014); 
Pohl S., Grafe L., Rose N., Dealing with omitted and not-reached items in competence tests: Evaluating approaches accounting for missing responses in item response theory models, Educational and Psychological Measurement, 74, 3, pp. 423-452, (2014); 
Pohl S., Ulitzsch E., von Davier M., Using response times to model not-reached items due to time limits, Psychometrika, 84, 3, pp. 892-920, (2019); 
R: A language and environment for statistical computing, (2019); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of noneffortful responses on aggregated scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, 1, pp. 74-104, (2017); 
Rose N., Item nonresponses in educational and psychological measurement, (2013); 
Rose N., von Davier M., Nagengast B., Modeling omitted and not-reached items in IRT models, Psychometrika, 82, 3, pp. 795-819, (2017); 
Rose N., von Davier M., Xu X., Modeling nonignorable missing data with item response theory (IRT) (ETS Research Rep. No. RR–10-11), Educational Testing Service, (2010); 
Samejima F., Estimation of latent ability using a response pattern of graded scores, Psychometrika Monograph Supplement, 34, 4, (1969); 
Soland J., Kuhfeld M., Do students rapidly guess repeatedly over time? A longitudinal analysis of student test disengagement, background, and attitudes, Educational Assessment, 24, 4, pp. 327-342, (2019); 
Tijmstra J., Bolsinova M., On the importance of the speed-ability trade-off when dealing with not reached items, Frontiers in Psychology, 9, (2018); 
Ulitzsch E., von Davier M., Pohl S., Using response times for joint modeling of response and omission behavior, Multivariate Behavioral Research, 55, 3, pp. 425-453, (2020); 
van der Linden W.J., A hierarchical framework for modeling speed and accuracy on test items, Psychometrika, 72, 3, pp. 287-308, (2007); 
Weeks J.P., von Davier M., Yamamoto K., Using response time data to inform the coding of omitted responses, Psychological Test and Assessment Modeling, 58, 4, pp. 671-701, (2016); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, 3, pp. 237-252, (2015); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement, 36, 4, pp. 52-61, (2017); 
Wise S.L., An information-based approach to identifying rapid-guessing thresholds, Applied Measurement in Education, 32, 4, pp. 325-336, (2019); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, 2, pp. 163-183, (2005); 
Wise S.L., Ma L., Setting response time thresholds for a CAT item pool: The normative threshold method, (2012); 
Xiao J., Bulut O., Evaluating the performances of missing data handling methods in ability estimation from sparse data, Educational and Psychological Measurement, 80, 5, pp. 932-954, (2020); 
Yi Q., Widiatmo H., Hanson B.A., Ban J., Harris D.J., Impact of scoring options for not reached items in CAT, (2001)#FRF#
