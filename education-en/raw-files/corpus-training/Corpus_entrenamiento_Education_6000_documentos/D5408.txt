#ITI#Interdisciplinary Science Assessment of Carbon Cycling: Construct Validity Evidence Based on Internal Structure#FTI#
#IRE#Growing interest in interdisciplinary (ID) understanding has led to the recent development of four ID assessments, none of which have previously been comprehensively validated. Sources of evidence for the validity of tests include construct validity, such as the internal structure of the test. ID tests may (and should) test both disciplinary (D) and ID understanding, and the internal structure can be examined to determine whether the theoretical relationship among D and ID knowledge is supported by empirical evidence. In this paper, we present an analysis of internal structure of ISACC, an ID assessment of carbon cycling, selected because the developers included both disciplinary and ID items and posited a relationship among these two types of knowledge. Responses from 454 high school and college students were analyzed using confirmatory factor analyses (CFA) with the Mplus software for internal structure focusing on dimensionality, as well as functioning by gender and race/ethnicity, and reliability. CFA confirmed that the underlying structure of the ISACC best matches a two-factor path model, supporting the developers’ theoretical hypothesis of the impact of disciplinary understanding on interdisciplinary understanding. No gender effect was found in the factor structure of the ISACC, indicating that females and males have similar performance. Race/ethnicity performance was similar to other science assessments, revealing possible ethnicity bias on the part of the instrument. The reliability coefficients for the two-factor path model were found to be sufficient. This study highlights the importance of the internal structure of test instruments as a source of validity evidence and models a procedure to assess internal structure#FRE#
#IPC#Construct validity; Fairness; Interdisciplinary science assessment; Interdisciplinary understanding; Internal structure; Reliability#FPC#
#IRF#Abraham J., Multidisciplinary explorations: Bridging the gap between engineering and biology, Journal of College Science Teaching, 33, 5, pp. 27-31, (2004); 
Standards for educational and psychological testing, (2014); 
ACARA STEM connections project report, Retrieved From, (2016); 
Boix Mansilla V., Duraisingh E.D., Targeted assessment of students' interdisciplinary work: An empirically grounded framework proposed, The Journal of Higher Education, 78, 2, pp. 215-237, (2007); 
Boone W.J., Staver J.R., Yale M.S., Rasch analysis in the human sciences, (2014); 
Boone W.J., Rasch analysis for instrument development: Why, when, and how?., 15, 4, (2016); 
Chen F.F., West S.G., Sousa K.H., A comparison of bifactor and second-order models of quality of life, Multivariate Behavioral Research, 41, 2, pp. 189-225, (2006); 
Chi M.T.H., Ceci S.J., Content knowledge: Its role, representation, and restructuring in memory development, Advances in child development and behavior, 20, pp. 91-142, (1987); 
Clark L.A., Watson D., Constructing validity: Basic issues in objective scale development, Psychological Assessment, 7, 3, pp. 309-319, (1995); 
Cronbach L.J., Coefficient alpha and the internal structure of tests, Psychometrika, 16, 3, pp. 297-334, (1951); 
Dimitrov D.M., Testing for factorial invariance in the context of construct validation, Measurement and Evaluation in Counseling and Development, 43, 2, pp. 121-149, (2010); 
DiStefano C., Liu J., Jiang N., Shi D., Examination of the weighted root mean square residual: Evidence for trustworthiness?, Structural Equation Modeling, 25, 3, pp. 453-466, (2018); 
Doerschuk P., Bahrim C., Daniel J., Kruger J., Mann J., Martin C., Closing the gaps and filling the STEM pipeline: A multidisciplinary approach, Journal of Science Education and Technology, 25, 4, pp. 682-695, (2016); 
Scotland E., Cfe Briefing Interdisciplinary Learning, (2012); 
Erduran S., Guilfoyle L., Park W., Chan J., Fancourt N., Argumentation and interdisciplinarity: Reflections from the Oxford argumentation in religion and science project, Disciplinary and Interdisciplinary Science Education Research, 1, 8, pp. 1-10, (2019); 
Garner J.K., Kaplan A., Hathcock S., Bergey B., Concept mapping as a mechanism for assessing science teachers’ cross-disciplinary field-based learning, Journal of Science Teacher Education, 31, 1, pp. 8-33, (2020); 
Gipps C.V., Murphy P., A fair test? Assessment, achievement and equity, (1994); 
Hirschfeld G., von Brachel R., Improving Multiple-Group confirmatory factor analysis in R–A tutorial in measurement invariance with continuous and ordinal indicators. Practical Assessment, Research, and Evaluation, 19, 1, (2014); 
Holbrook J.B., What is interdisciplinary communication? Reflections on the very idea of disciplinary integration, Synthese, 190, pp. 1865-1879, (2013); 
Hu L.T., Bentler P.M., Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6, 1, pp. 1-55, (1999); 
Hurd P.D., Why we must transform science education, Educational Leadership, 49, 2, pp. 33-35, (1991); 
Kane M.T., Validation, Educational measurement, pp. 17-63, (2006); 
Khisty L.L., Making inequality: Issues of language and meanings in mathematics teaching with Hispanic students, New directions for equity in mathematics education, pp. 279-297, (1995); 
Kline R., Principles and practice of structural equation modeling, (2015); 
Kochelmans J., Why interdisciplinarity? Interdisciplinarity and higher education, (1979); 
Lattuca L.R., Voigt L.J., Fath K.Q., Does interdisciplinarity promote learning? Theoretical support and researchable questions, The Review of Higher Education, 28, 1, pp. 23-48, (2004); 
LoGerfo L., Nichols A., Reardon S.F., Achievement gains in elementary and high school, (2006); 
Lord F.M., Novick M.R., Statistical theory of mental test scores, (1968); 
Marasco E.A., Behjat L., Developing a cross-disciplinary curriculum for the integration of engineering and design in elementary education, Proc. of The, (2013); 
Marshall J., Banner J., You H., Assessing the effectiveness of sustainability learning, Journal of College Science Teaching, 47, 3, pp. 57-67, (2018); 
Messick S., Validity, Educational measurement, (1989); 
The 2015 revised science curriculum. Report no. 2015–74, (2015); 
Moss P.A., Shifting conceptions of validity in educational measurement: Implications for performance assessment, Review of Educational Research, 62, 3, pp. 229-258, (1992); 
Musu-Gillette L., de Brey C., McFarland J., Hussar W., Sonnenberg W., Wilkinson-Flicker S., Status and Trends in the Education of Racial and Ethnic Groups 2017 (NCES 2017-051). Washington, DC; U.S. Department of Education, National Center for education statistics, (2017); 
Muthen L.K., Muthen B.O., Mplus User’s Guide, (1998); 
Pellegrino J., Chudowsky N., Glaser R., Knowing what students know: The science and design of educational assessment, Board on testing and assessment, Center for Education, division of behavioral and social science and education, (2001); 
A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas. Committee on a conceptual framework for new K-12 science education standards., (2012); 
Neurath O., Unified science as encyclopedic integration, Logical empiricism at its peak: Schlick, Carnap, and Neurath, pp. 309-335, (1996); 
Next generation science standards: For states, by states, (2013); 
Nowacek R.S., A discourse-based theory of interdisciplinary connections, The Journal of General Education, 54, 3, pp. 171-195, (2005); 
Raykov T., Bias of coefficient a for fixed congeneric measures with correlated errors, Applied Psychological Measurement, 25, 1, pp. 69-76, (2001); 
Raykov T., Evaluation of scale reliability for unidimensional measures using latent variable modeling, Measurement and Evaluation in Counseling and Development, 42, 3, pp. 223-232, (2009); 
Reiska P., Soika K., Canas A.J., Using concept mapping to measure changes in interdisciplinary learning during high school, Knowledge Management & E-Learning: An International Journal (KM&EL), 10, 1, pp. 1-24, (2018); 
Rennie L., Wallace J., Venville G., Integrating science, technology, engineering, and mathematics: Issues, reflections, and ways forward. In L. Rennie, G. Venville, & J. Wallace (Eds.), Exploring curriculum integration: Why integrate? (pp. 1–11), Routledge, (2012); 
Schaal S., Bogner F.X., Girwidz R., Concept mapping assessment of media assisted learning in interdisciplinary science education, Research in Science Education, 40, 3, pp. 339-352, (2010); 
Schumacker R.E., Lomax R.G., A Beginners Guide to Structural Equation Modeling, (2010); 
Sedlacek W.E., Issues in advancing diversity through assessment, Journal of Counseling & Development, 72, 5, pp. 549-553, (1994); 
Shen J., Liu O.L., Sung S., Designing interdisciplinary assessments in sciences for college students: An example on osmosis, International Journal of Science Education, 36, 11, pp. 1773-1793, (2014); 
Sireci S.G., On validity theory and test validation, Educational Researcher, 36, 8, pp. 477-481, (2007); 
Smith E.V., Evidence for the reliability of measures and validity of measure interpretation: A Rasch measurement perspective, Journal of Applied Measurement, 2, 3, pp. 281-311, (2001); 
Stock P., Burton R.J., Defining terms for integrated (multi-inter-transdisciplinary) sustainability research, Sustainability, 3, 8, pp. 1090-1113, (2011); 
Tate W.F., Race retrenchment and reform of school mathematics, Phi Delta Kappan, 75, 6, pp. 477-480, (1994); 
Willingham J.C., Pair J.D., Parrish J.C., A framework for cross-disciplinary engineering projects, Science Scope, 39, 7, (2016); 
Wilson M., Constructing Measures: An Item Response Modeling Approach, (2005); 
Wolfe C.R., Haynes C., Assessing interdisciplinary writing, Peer Review, 6, 1, pp. 126-169, (2003); 
Wright B.D., Stone M.H., Best Test Design: Rasch Measurement, (1979); 
You H.S., Why teach science with an interdisciplinary approach: History, trends, and conceptual frameworks, Journal of Education and Learning, 6, 4, pp. 66-77, (2017); 
You H.S., Marshall J.A., Delgado C., Assessing students' disciplinary and interdisciplinary understanding of global carbon cycling, Journal of Research in Science Teaching, 55, 3, pp. 377-398, (2018); 
You H.S., Marshall J.A., Delgado C., Toward Interdisciplinary Learning: Development and Validation of an Assessment for Interdisciplinary Understanding of Global Carbon Cycling, Research in Science Education, pp. 1-25, (2019); 
Yu C.Y., Evaluating Cutoff Criteria of Model Fit Indices for Latent Variable Models with Binary and Continuous Outcomes (Unpublished Doctoral Dissertation), (2002); 
Zimmerman D.W., Test reliability and the Kuder-Richardson formulas: Derivation from probability theory, Educational and Psychological Measurement, 32, 4, pp. 939-954, (1972)#FRF#
