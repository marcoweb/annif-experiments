#ITI#Using Machine Learning to Score Multi-Dimensional Assessments of Chemistry and Physics#FTI#
#IRE# In response to the call for promoting three-dimensional science learning (NRC, 2012), researchers argue for developing assessment items that go beyond rote memorization tasks to ones that require deeper understanding and the use of reasoning that can improve science literacy. Such assessment items are usually performance-based constructed responses and need technology involvement to ease the burden of scoring placed on teachers. This study responds to this call by examining the use and accuracy of a machine learning text analysis protocol as an alternative to human scoring of constructed response items. The items we employed represent multiple dimensions of science learning as articulated in the 2012 NRC report. Using a sample of over 26,000 constructed responses taken by 6700 students in chemistry and physics, we trained human raters and compiled a robust training set to develop machine algorithmic models and cross-validate the machine scores. Results show that human raters yielded good (Cohen’s k =.40–.75) to excellent (Cohen’s k >.75) interrater reliability on the assessment items with varied numbers of dimensions. A comparison reveals that the machine scoring algorithms achieved comparable scoring accuracy to human raters on these same items. Results also show that responses with formal vocabulary (e.g., velocity) were likely to yield lower machine-human agreements, which may be associated with the fact that fewer students employed formal phrases compared with the informal alternatives#FRE#
#IPC# Automatic scoring; Machine learning; Three-dimensional science learning#FPC#
#IRF# September 4, 2020, Retrieved From, (2020); 
Balfour S.P., Assessing writing in MOOCs: Automated Essay Scoring and Calibrated Peer Review<sup>TM</sup>, Research & Practice in Assessment, 8, pp. 40-48, (2013); 
Cheuk T., Osborne J., Cunningham K., Haudek K., Santiago M., Urban-Lurain M., Merril J., Towards an Equitable Design Framework of Developing Argumentation in Science tasks and Rubrics for Machine Learning, Presented at the Annual Meeting of the National Association for Research in Science Teaching (NARST), (2019); 
Statistical methods for rates and proportions (2nd ed.). New York: John Wiley, ISBN 978–0–471–26370–8, (1981); 
Garbage in, garbage out? Do machine learning application papers in social computing report where human-labeled training data comes from?, Proceedings of The, 2020, pp. 325-336, (2020); 
Ha M., Nehm R.H., The impact of misspelled words on automated computer scoring: a case study of scientific explanations, Journal of Science Education and Technology, 25, 3, pp. 358-374, (2016); 
Harris C.J., Krajcik J.S., Pellegrino J.W., Debarger A.H., Designing knowledge-in-use assessments to promote deeper learning, Educational Measurement: Issues and Practice, 38, 2, pp. 53-67, (2019); 
Haudek K., Santiago M., Wilson C., Stuhlsat Z M., Donovan B., Bracey Z., Gardner A., Osborne J., Cheuk T., Using Automated Analysis to Assess Middle School Students’ Competence with Scientific Argumentation, Presented at the Annual Meeting of the National Council on Measurement in Education (NCME), (2019); 
Large J., Lines J., Bagnall A., A probabilistic classifier ensemble weighting scheme based on cross-validated accuracy estimates, Data mining and knowledge discovery, 33, 6, pp. 1674-1709, (2019); 
Lee H.S., McNamara D., Bracey Z.B., Liu O.L., Gerard L., Sherin B., Wilson C., Pallant A., Linn M., Haudek K., Osborne J., Computerized Text Analysis: Assessment and Research Potentials for Promoting Learning, (2019); 
Lee H.S., Pallant A., Pryputniewicz S., Lord T., Mulholland M., Liu O.L., Automated text scoring and real-time adjustable feedback: Supporting revision of scientific arguments involving uncertainty, Science Education, 103, 3, pp. 590-622, (2019); 
Liu O.L., Brew C., Blackmore J., Gerard L., Automated scoring of constructed response science items: Prospects and obstacles, Educational Measurement-Issues and Practices, 33, 2, pp. 19-28, (2014); 
Lottridge S., Wood S., Shaw D., The effectiveness of machine score-ability ratings in predicting automated scoring performance, Applied Measurement in Education, 31, 3, pp. 215-232, (2018); 
Mao L., Liu O.L., Roohr K., Belur V., Mulholland M., Lee H.-S., Pallant A., Validation of automated scoring for a formative assessment that employs scientific argumentation, Educational Assessment, 23, 2, pp. 121-138, (2018); 
Mayfield E., Rose C., An interactive tool for supporting error analysis for text mining, In Proceedings of the NAACL HLT 2010 Demonstration Session, pp. 25-28, (2010); 
Mayfield E., Rose C.P., Open source machine learning for text, (2013); 
Science and Engineering for Grades 6–12: Investigation and Design at the Center, (2019); 
A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas, (2012); 
Developing Assessments for the Next Generation Science Standards, (2014); 
Nehm R.H., Haertig H., Human vs. computer diagnosis of students’ natural selection knowledge: testing the efficacy of text analytic software, Journal of Science Education and Technology, 21, 1, pp. 56-73, (2012); 
Next generation science standards: For states, by states, (2013); 
Pellegrino J.W., Proficiency in science: Assessment challenges and opportunities, Science, 340, 6130, pp. 320-323, (2013); 
Zhai X., Haudek K., Shi L., Nehm R., Urban-Lurain M., From substitution to redefinition: A framework of machine learning-based science assessment, Journal of Research in Science Teaching, 57, 9, pp. 1430-1459, (2020); 
Zhai X., Haudek K., Stuhlsatz M., Wilson C., Evaluation of construct-irrelevant variance yielded by machine and human scoring of a science teacher PCK constructed response assessment, Studies in Educational Evaluation, 67, pp. 1-12, (2020); 
Zhai X., Yin Y., Pellegrino J., Haudek K., Shi L., Applying machine learning in science assessment: A systematic review, Studies in Science Education, 56, 1, pp. 111-151, (2020); 
Zhu M., Lee H.-S., Wang T., Liu O.L., Belur V., Pallant A., Investigating the impact of automated feedback on students’ scientific argumentation, International Journal of Science Education, 39, 12, pp. 1648-1668, (2017)#FRF#
