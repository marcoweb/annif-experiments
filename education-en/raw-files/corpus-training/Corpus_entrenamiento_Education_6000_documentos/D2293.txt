#ITI#Developing Situated Measures of Science Instruction Through an Innovative Electronic Portfolio App for Mobile Devices: Reliability, Validity, and Feasibility#FTI#
#IRE# Adoption of new instructional standards in science demands high-quality information about classroom practice. Teacher portfolios can be used to assess instructional practice and support teacher self-reflection anchored in authentic evidence from classrooms. This study investigated a new type of electronic portfolio tool that allows efficient capture of classroom artifacts in multimedia formats using mobile devices. We assess the psychometric properties of measures of quality instruction in middle school science classrooms derived from the contents of portfolios collected using this novel tool—with instruction operationalized through dimensions aligned to the Next Generation Science Standards. Results reflect low rater error and adequate reliability for several dimensions, a dominant underlying factor, and significant relations to some relevant concurrent indicators. Although no relation was found to student standardized test scores or course grades, portfolio ratings did relate to student self-efficacy perceptions and enjoyment of science. We examine factors influencing measurement error, and consider the broader implications of the results for assessing the validity of portfolio score interpretations, and the feasibility and potential value of this type of tool for summative and formative uses, in the context of large-scale instructional improvement efforts.#FRE#
#IPC# generalizability theory; measures of instruction; performance assessment; portfolio assessment; validity#FPC#
#IRF# Allen C.D., Penuel W.R., Studying teachers’ sensemaking to investigate teachers’ responses to professional development focused on new standards, Journal of Teacher Education, 66, 2, pp. 136-149, (2015); 
Ball D.L., Forzani F.M., The work of teaching and the challenge for teacher education, Journal of Teacher Education, 60, 5, pp. 497-511, (2009); 
Borko H., Stecher B.M., Martinez J.F., Kuffner K.L., Barnes D., Arnold S.C., Using classroom artifacts to measure instructional practices in middle school science: A two-state field test, (2006); 
Cohen J., Goldhaber D., Building a more complete understanding of teacher evaluation using classroom observations, Educational Researcher, 45, 6, pp. 378-387, (2016); 
Correnti R., Martinez J.F., Conceptual and methodological issues in the study of teaching: Mission statement for improving instruction at scale, Educational Assessment, 17, pp. 2-3, (2012); 
Darling-Hammond L., Hyler M.E., Gardner M., Effective teacher professional development, (2017); 
Desimone L., LeFloch K., Are we asking the right questions? Using cognitive interviews to improve surveys in education research, Educational Evaluation and Policy Analysis, 26, 1, pp. 1-22, (2004); 
Gitomer D.H., Measurement issues and assessment for teaching quality, (2009); 
Grossman P., McDonald M., Back to the future: Directions for research in teaching and teacher education, American Educational Research Journal, 45, 1, pp. 184-205, (2008); 
Hushman C., Marley C., Guided instruction improves elementary student learning and self-efficacy in science, The Journal of Educational Research, 108, 5, pp. 371-381, (2015); 
Ing M., What about the “instruction” in instructional sensitivity? Validity issues in research on instructional sensitivity, Educational and Psychological Measurement, 78, 4, pp. 635-652, (2018); 
Ing M., Chinen S., Jackson K., Smith T., When should i use a measure to support instructional improvement at scale? The importance of considering both intended and actual use in validity arguments, Educational Measurement: Issues and Practice, 40, 1, pp. 92-100, (2020); 
Kane M., Validation, Educational measurement, pp. 17-64, (2006); 
Kane T.J., Staiger D.O., Gathering feedback for teaching: Combining high-quality observations with student surveys and achievement gains, (2012); 
Kennedy M.M., Approximations to indicators of student outcomes, Educational Evaluation and Policy Analysis, 21, pp. 345-363, (1999); 
Kersting N.B., Sherin B., Stigler J.W., Automated Scoring of Teachers’ OpenEnded Responses to Video Prompts: Bringing the Classroom Video Analysis (CVA) Assessment to Scale, Educational & Psychological Measurement, 74, 6, pp. 950-974, (2014); 
Kloser M., Edelman A., Floyd C., Martinez J.F., Stecher B., Srinivasan J., Interrogating practice or show and tell?: Using a digital portfolio to anchor a professional learning community of science teachers, pp. 210-241, (2021); 
Knapp M.S., Between systemic reforms and the mathematics and science classroom: The dynamics of innovation, implementation, and professional learning, Review of Educational Research, 67, 2, pp. 227-266, (1997); 
Krajcik J., Codere S., Dahsah C., Bayer R., Mun K., Planning instruction to meet the intent of the next generation science standards, Journal of Science Teacher Education, 25, 2, pp. 157-175, (2014); 
Kuhn D., Teaching and learning science as argument, Science Education, 94, 5, pp. 810-824, (2010); 
Latour B., Laboratory life: The construction of scientific facts, (1986); 
Lemke J.L., Talking science: Language, learning, and values, 1, (1990); 
Martinez J.F., Borko H., Stecher B., Luskin R., Kloser M., Measuring classroom assessment practice using instructional artifacts: A validation study of the QAS notebook, Educational Assessment, 17, pp. 107-131, (2012); 
Matsumura L.C., Garnier H.E., Slater S.C., Boston M.D., Toward measuring instructional interactions “at-scale., Educational Assessment, 13, 4, pp. 267-300, (2008); 
Mayer D., Measuring instructional practice: Can policy makers trust survey data?, Educational Evaluation and Policy Analysis, 21, 1, pp. 29-45, (1999); 
Medley D.M., Mitzel H.E., Measuring classroom behavior by systematic observation, Handbook of research on teaching, pp. 247-328, (1963); 
Meyer J.P., Cash A.H., Mashburn A., Occasions and the reliability of classroom observations: Alternative conceptualizations and methods of analysis, Educational Assessment, 16, 4, pp. 227-243, (2011); 
Muthen B.O., Muthen L.K., Mplus, (2012); 
Scoring guide: Understanding your scores, (2019); 
A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
Podolsky A., Kini T., Darling-Hammond L., Does teaching experience increase teacher effectiveness? A review of US research, Journal of Professional Capital and Community, 4, 4, pp. 286-308, (2019); 
Polikoff M.S., Gasparian H., Korn S., Gamboa M., Porter A.C., Smith T., Garet M.S., Flexibly using the surveys of enacted curriculum to study alignment, Educational Measurement: Issues and Practice, 39, pp. 38-47, (2020); 
Pollio M., Hochbein C.D., The association between standards-based grading and standardized test scores as an element of a high school reform model, Teachers College Record: The Voice of Scholarship in Education, 117, pp. 1-28, (2015); 
Raudenbush S.W., Bryk A.S., Hierarchical linear models: Applications and data analysis methods, (2002); 
Raudenbush S.W., Sadoff S., Statistical inference when classroom quality is measured with error, Journal of Research on Educational Effectiveness, 1, pp. 138-154, (2008); 
Rockoff J.E., The impact of individual teachers on student achievement: Evidence from panel data, American Economic Review, 94, 2, pp. 247-252, (2004); 
Rowan B., Correnti R., Studying reading instruction with teacher logs: Lessons from a study of instructional improvement, Educational Researcher, 38, pp. 120-131, (2009); 
Educative assessment & meaningful support: 2017 edTPA administrative report, (2018); 
Schunk D.H., Self-efficacy and achievement behaviors, Educational Psychology Review, 1, pp. 173-208, (1989); 
Schweig J.D., Quantifying error in survey measures of school and classroom environments, Applied Measurement in Education, 27, 2, pp. 133-157, (2014); 
Scruggs T.E., Brigham F.J., Mastropieri M.A., Common core science standards: Implications for students with learning disabilities, Learning Disabilities Research & Practice, 28, 1, pp. 49-57, (2013); 
Seidel T., Shavelson R.J., Teaching effectiveness research in the past decade: The role of theory and research design in disentangling meta-analysis results, Review of Educational Research, 77, 4, pp. 454-499, (2007); 
Shavelson R.J., Webb N.M., Generalizability theory: A primer, (1991); 
Shulman L., Teacher portfolios: A theoretical activity, With Portfolio in Hand, pp. 23-37, (1998); 
Smith P.S., (2010); 
Stecher B., Le V.N., Hamilton L., Ryan G., Robyn A., Lockwood J.R., Using structured classroom vignettes to measure instructional practices in mathematics, Educational Evaluation and Policy Analysis, 28, 2, pp. 101-130, (2006); 
Stefani L., Mason R., Pegler C., The educational potential of e-portfolios, (2007); 
Strudler N., Wetzel K., Electronic portfolios in teacher education: Forging a middle ground, Journal of Research on Technology in Education, 44, 2, pp. 161-173, (2011); 
Tangmunarunkit H., Hsieh C.K., Longstaff B., Nolen S., Jenkins J., Ketcham C., Estrin D., Ohmage: A general and extensible end-to-end participatory sensing platform, ACM Transactions on Intelligent Systems and Technology (TIST), 6, 3, (2015); 
e-QIS, (2018); 
William T., Measuring instruction in higher education: Summary of a convening, (2015); 
Windschitl M., Thompson J., Braaten M., Stroupe D., Proposing a core set of instructional practices and tools for teachers of science, Science Education, 96, 5, pp. 878-903, (2012); 
Zee M., Koomen H., Teacher self-efficacy and its effects on classroom processes, student academic adjustment, and teacher well-being: A synthesis of 40 years of research, Review of Educational Research, 86, 4, pp. 981-1015, (2016); 
Zeichner K., The turn once again toward practice-based teacher education, Journal of Teacher Education, 63, 5, pp. 376-382, (2012)#FRF#
