#ITI#Privacy risk quantification in education data using Markov model#FTI#
#IRE# With Big Data revolution, the education sector is being reshaped. The current data-driven education system provides many opportunities to utilize the enormous amount of collected data about students' activities and performance for personalized education, adapting teaching methods, and decision making. On the other hand, such benefits come at a cost to privacy. For example, the identification of a student's poor performance across multiple courses. While several works have been conducted on quantifying the re-identification risks of individuals in released datasets, they assume an adversary's prior knowledge about target individuals. Most of them do not utilize all the available information in the datasets. For example, event-level information that associates multiple records to the same individual and correlation between attributes. In this work, we propose a method using a Markov Model (MM) to quantify re-identification risks using all available information in the data under a more realistic threat model that assumes different levels of an adversary's knowledge about the target individual, ranging from any one of the attributes to all given attributes. Moreover, we propose a workflow for efficiently calculating MM risk which is highly scalable to large number of attributes. Experimental results from real education datasets show the efficacy of our model for re-identification risk. Practitioner notes What is already known about this topic? There has been a number of works/research conducted on privacy risk quantification in datasets and in the Web. Majority of them have strong assumption about adversary's prior knowledge of target individual(s). Most of them do not utilize all the available information in the datasets, eg, event-level or duplicate records and correlation between attributes. What this paper adds? This paper proposes a new re-identification risk quantification model using Markov models. Our model addresses the shortcomings of existing works, eg, strong assumption about adversary's knowledge, unexplainable model, and utilizing available information in the datasets. Specifically, our proposed model not only focuses on the uniqueness of data points in the datasets (as most of the other existing methods), but also takes into account uniformity and correlation characteristics of these data points. Re-identification risk quantification is computationally expensive and is not scalable to large datasets with increasing number of attributes. This paper introduces a workflow for data custodians to use to efficiently evaluate the worst-case re-identification risk in their datasets before releasing. It presents extensive experimental evaluation results of the proposed model for quantifying re-identification risks on several real education datasets. Implications for practice and/or policy? Empirical results on real education datasets validate the significance and efficacy of the proposed model for re-identification risk quantification compared to existing approaches. Our model can be used by the data custodians as a tool to evaluate the worst-case risk of a dataset. It empowers data custodians to make informed decisions on appropriate actions to mitigate these risks (eg, data perturbation) before sharing or releasing their datasets to third parties. A typical use case would be one where the data custodians is an online course/program provider, which collects data about students' engagement with their courses and would like to share it with third parties for them to run learning analytics that would provide value-added benefits back to the data custodian. We specifically study the privacy risk quantification for education data; however, our model is applicable to any tabular data release#FRE#
#IPC# correlation; Markov chain; re-identification risk; uniformity; uniqueness#FPC#
#IRF# Balsa E., Troncoso C., Diaz C., OB-PWS: Obfuscation-based private web search, Proceedings - IEEE symposium on security and privacy, pp. 491-505, (2012); 
Biega J.A., Gummadi K.P., Mele I., Milchevski D., Tryfonopoulos C., Weikum G., R-susceptibility: An ir-centric approach to assessing privacy risks for users in online communities, Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pp. 365-374, (2016); 
Biega J., Mele I., Weikum G., Probabilistic prediction of privacy risks in user search histories, Proceedings of the First International Workshop on Privacy and Security of Big Data, pp. 29-36, (2014); 
Bowman G.R., Pande V.S., Noe F., An introduction to Markov state models and their application to long timescale molecular simulation, 797, (2013); 
Metrics and frameworks for privacy risk assessments, (2021); 
Gervais A., Shokri R., Singla A., Capkun S., Lenders V., Quantifying web-search privacy, Proceedings of the ACM SIGSAC conference on computer and communications security 2014 (CCS '14), pp. 966-977, (2014); 
Hansell S., AOL removes search data on vast group of web users, New York Times, 8, (2006); 
Hanson R., Rennie E., Lee J., Grobler M., Digital trust, an Australian perspective: Corporate awareness and attitudes to consumer data, (2020); 
Kuzilek J., Hlosta M., Zdrahal Z., Open university learning analytics dataset, Scientific Data, 4, 1, (2017); 
Liu K., Terzi E., A framework for computing the privacy scores of users in online social networks, ACM Transactions on Knowledge Discovery from Data, 5, 1, (2010); 
Machanavajjhala A., Kifer D., Gehrke J., Venkitasubramaniam M., l-diversity: Privacy beyond k-anonymity, ACM Transactions on Knowledge Discovery from Data, 1, 1, (2007); 
Masood R., Vatsalan D., Ikram M., Kaafar M.A., Incognito: A method for obfuscating web data, Proceedings of the World Wide Web conference, pp. 267-276, (2018); 
Narayanan A., Shmatikov V., Robust de-anonymization of large sparse datasets, IEEE Symposium on Security and Privacy, pp. 111-125, (2008); 
Oppermann I., Nabaglo J., Henecka W., A measure of personal information in mobile data, 6G Wireless Summit (6G SUMMIT), pp. 1-6, (2020); 
Peddinti S.T., Saxena N., On the privacy of web search based on query obfuscation: A case study of trackmenot, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 6205, pp. 19-37, (2010); 
Sweeney L., Guaranteeing anonymity when sharing medical data, the Datafly system, Proceedings of the AMIA Annual Fall Symposium, pp. 51-55, (1997); 
Sweeney L., K-anonymity: A model for protecting privacy, International Journal of Uncertainty Fuzziness and Knowledge Based Systems, 10, 5, pp. 557-570, (2002); 
Templ M., Meindl B., Kowarik A., Chen S., Introduction to statistical disclosure control (SDC), (2013); 
Publication of MBS/PBS data, Commissioner initiated investigation report, (2018); 
Disclosure of Myki travel information, (2019); 
Wang Y., Baker R., Content or platform: Why do students complete MOOCS, MERLOT Journal of Online Learning and Teaching, 11, 1, pp. 17-30, (2015)#FRF#
