#ITI#Assessment validity and learning analytics as prerequisites for ensuring student-centred learning design#FTI#
#IRE# To ensure the validity of an assessment programme, it is essential to align it with the intended learning outcomes (LO). We present a model for ensuring assessment validity which supports this constructive alignment and uses learning analytics (LA). The model is based on LA that include a comparison between ideal LO weights (expressing the prioritization of LOs), actual assessment weights (maximum assessment points per LO), and student assessment results (actually obtained assessment points per LO), as well as clustering and trace data analysis. These analytics are part of a continuous improvement cycle, including strategic planning and learning design (LD) supported by LO prioritization, and monitoring and evaluation supported by LA. To illustrate and test the model, we conducted a study on the example of a graduate-level higher education course in applied mathematics, by analysing student assessment results and activity in a learning management system. The study showed that the analyses provided valuable insights with practical implications for the development of sound LD, tailored educational interventions, databases of assessment tasks, recommendation systems, and self-regulated learning. Future research should investigate the possibilities for automation of such LA, to enable full exploitation of their potential and use in everyday teaching and learning. Practitioner notesWhat is already known about this topicTo develop sound, student-centred learning design (LD), it is essential to ensure that assessment is constructively aligned with the intended learning outcomes (LO).This constructive alignment is crucial for ensuring the validity of an assessment program.Learning analytics (LA) can provide insights that help develop valid assessment programs.What this paper addsAs not all LOs are equally important, assessment programs should reflect the prioritization of LOs, which can be determined by using various multi-criteria decision-making (MCDM) methods.This article presents and illustrates, based on an empirical case, a model of continuous improvement of LD, which uses LA to compare how LOs are reflected in (actual) students' results, in an (actual) assessment program, and in the (ideal) prioritization of LOs based on MCDM.The study presents how clustering of students based on their assessment results can be used in LA to provide insights for educational interventions better targeted to students' needs.Implications for practice and/or policyThe proposed LA can provide important insights for the development (or improvement) of LD in line with the intended course LOs, but also study program LOs (if course and study program LOs are properly aligned).The LA can also contribute to the development of databases of assessment tasks aligned with course LOs, with ensured validity, supporting sharing and reusing, as well as to the development of tailored educational interventions (eg, based on clustering).The proposed LA can also contribute to the development of recommendation systems, with recommendations for the improvement of LD for teachers or learning suggestions for students, as well as students' meta-cognition and self-regulated learning#FRE#
#IPC# assessment; assessment utility; assessment validity; LD; learning analytics; learning outcomes; multi-criteria decision-making#FPC#
#IRF# Standards for educational & psychological testing, (2014); 
Airasian P.W., Cruikshank K.A., Mayer R.E., Pintrich P.R., Raths J., Wittrock M.C., A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives (complete edition), (2001); 
Biggs J., What the student does: Teaching for enhanced learning, Higher Education Research & Development, 18, 1, pp. 57-75, (1999); 
Bloom B.S., Englehart M.D., Furst E.J., Hill W.H., Krathwohl D.R., The taxonomy of educational objectives, handbook I: The cognitive domain, (1956); 
Communiqué of the conference of European ministers responsible for higher education, Leuven and Louvain-la-Neuve, 28–29 April 2009, (2009); 
Carter M., Egliston B., What are the risks of virtual reality data? Learning analytics, algorithmic bias and a fantasy of perfect data, New Media & Society, (2021); 
Chatti M.A., Muslim A., Guesmi M., Richtscheid F., Nasimi D., Shahin A., Damera R., How to design effective LA indicators? A human-centered design approach, European Conference on Technology Enhanced Learning, pp. 303-317, (2020); 
Chen B., Knight S., Wise A.F., Critical issues in designing and implementing temporal analytics, Journal of Learning Analytics, 5, 1, pp. 1-9, (2018); 
Coghlan S., Miller T., Paterson J., Good proctor or “big brother”? Ethics of online exam supervision technologies, Philosophy & Technology, 34, 4, pp. 1581-1606, (2021); 
Swamidass P.M., Encyclopedia of production and manufacturing management, (2000); 
Deming W.E., Out of the crisis, (1982); 
Di Mitri D., Schneider J., Klemke R., Specht M., Drachsler H., Read between the lines, pp. 51-60, (2019); 
Divjak B., Begicevic Redep N., Strategic decision-making cycle in higher education: Case study of e-learning, (2015); 
Divjak B., Grabar D., Svetec B., Vondra P., Balanced learning design planning: Concept and tool, Journal of Information and Organizational Sciences, 46, 2, (2022); 
Divjak B., Kadoic N., Zugec B., The use of decision-making methods to ensure assessment validity, 2021 IEEE Technology & Engineering Management Conference—Europe (TEMSCON-EUR), pp. 1-6, (2021); 
Divjak B., Pazur A.K., Zugec P., E-assessment in mathematics in higher education: A student perspective, International Journal of Mathematical Education in Science and Technology, 53, pp. 1-23, (2022); 
Entwistle N.J., Approaches to studying and levels of understanding: The influences of teaching and assessment, Higher education: Handbook of theory and research, 15, pp. 156-218, (2000); 
Fan Y., Matcha W., Uzir N.A.A., Wang Q., Gasevic D., Learning analytics to reveal links between learning design and self-regulated learning, International Journal of Artificial Intelligence in Education, 31, pp. 980-1021, (2021); 
Fincham O.E., Gasevic D., Jovanovic J.M., Pardo A., From study tactics to learning strategies: An analytical method for extracting interpretable representations, IEEE Transactions on Learning Technologies, 12, 1, pp. 59-72, (2018); 
Gabus A., Fontela E., World problems, an invitation to further thought within the framework of DEMATEL, (1972); 
Garrison D.R., E-learning in the 21st century: A framework for research and practice, (2011); 
Gasevic D., Greiff S., Shaffer D.W., Towards strengthening links between learning analytics and assessment: Challenges and potentials of a promising new bond, Computers in Human Behavior, 134, (2022); 
Gerritsen-van Leeuwenkamp K.J., Joosten-ten Brinke D., Kester L., Assessment quality in tertiary education: An integrative literature review, Studies in Educational Evaluation, 55, pp. 94-116, (2017); 
Gipps C., Fairness in assessment, Educational assessment in the 21st century, pp. 105-118, (2009); 
Gottipati S., Shankararaman V., Competency analytics tool: Analyzing curriculum using course competencies, Education and Information Technologies, 23, pp. 41-60, (2018); 
Greiff S., Scherer R., Kirschner P.A., Some critical reflections on the special issue: Current innovations in computer-based assessments, Computers in Human Behavior, 76, pp. 715-718, (2017); 
Hilliger I., Aguirre C., Miranda C., Celis S., Perez-Sanagustin M., Design of a curriculum analytics tool to support continuous improvement processes in higher education, Proceedings of the 10th International Conference on Learning Analytics and Knowledge (LAK ‘20), pp. 181-186, (2020); 
Jovanovic J., Gasevic D., Dawson S., Pardo A., Mirriahi N., Learning analytics to unveil learning strategies in a flipped classroom, The Internet and Higher Education, 33, pp. 74-85, (2017); 
Kadoic N., Characteristics of the analytic network process, a multi-criteria decision-making method, Croatian Operational Research Review, 9, 2, pp. 235-244, (2018); 
Kadoic N., Nova metoda za analizu složenih problema odlučivanja temeljena na analitičkom mrežnom procesu i analizi društvenih mreža, (2018); 
Killen R., Validity in outcomes-based assessment, Perspectives in Education, 21, pp. 1-14, (2003); 
Kitto K., Knight S., Practical ethics for building learning analytics, British Journal of Educational Technology, 50, pp. 2855-2870, (2019); 
Kohnke L., Foung D., Chen J., Using learner analytics to explore the potential contribution of multimodal formative assessment to academic success in higher education, SAGE Open, 12, 2, pp. 1-12, (2022); 
Kukea Shultz P., Englert K., Cultural validity as foundational to assessment development: An indigenous example, Frontiers in Education, 6, (2021); 
Lang C., Siemens G., Friend Wise A., Gasevic D., Merceron A., The handbook of learning analytics, (2022); 
Le Grange L., Beets P., (Re) conceptualizing validity in (outcomes-based) assessment, South African journal of education, 25, 2, pp. 115-119, (2005); 
Loyd S., Gholston N., Implementation of a plan-do-check-act pedagogy in industrial engineering education, International Journal of Engineering Education, 32, pp. 1260-1267, (2016); 
Messick S., The interplay of evidence and consequences in the validation of performance assessments, Educational Researcher, 23, pp. 13-23, (1995); 
Michnik J., Weighted influence non-linear gauge system (WINGS)—An analysis method for the systems of interrelated components, European Journal of Operational Research, 228, 3, pp. 536-544, (2013); 
Montano C.B., Hunt M.D., Boudreaux L., Improving the quality of student advising in higher education—A case study, Total Quality Management & Business Excellence, 16, 10, pp. 1103-1125, (2005); 
Nguyen Q., Huptych M., Rienties B., Linking students' timing of engagement to learning design and academic performance, (2018); 
Nguyen Q., Rienties B., Toetenel L., Ferguson F., Whitelock D., Examining the designs of computer-based assessment and its impact on student engagement, satisfaction, and pass rates, Computers in Human Behavior, 76, pp. 703-714, (2017); 
Norman D., The design of everyday things: Revised and expanded edition, (2013); 
O'Neil C., Weapons of math destruction: How big data increases inequality and threatens democracy, (2016); 
Pastore S., Andrade H.L., Teacher assessment literacy: A three-dimensional model, Teaching and Teacher Education, 84, pp. 128-138, (2019); 
Pellegrino J.W., DiBello L.V., Goldman S.R., A framework for conceptualizing and evaluating the validity of instructionally relevant assessments, Educational Psychologist, 51, 1, pp. 59-81, (2016); 
Rencher A., Methods of multivariate analysis, (2002); 
Saaty T.L., Fundamentals of the analytic network process, (1999); 
Saaty T.L., Fundamentals of the analytic network process—Dependence and feedback in decision-making with a single network, Journal of Systems Science and Systems Engineering, 13, pp. 129-157, (2004); 
Saaty T.L., Decision-making with the analytic hierarchy process, International Journal of Services Sciences, 1, 1, pp. 83-98, (2008); 
Saaty T.L., Sodenkamp M., The analytic hierarchy and analytic network measurement processes: The measurement of intangibles, Handbook of multicriteria analysis. Applied optimization, 103, (2010); 
Selwyn N., Re-imagining ‘learning analytics’ … a case for starting again?, The Internet and Higher Education, 46, (2020); 
Shah M., Pabel A., Richardson J.T.E., Introduction to the twenty-first century student experience: Issues, trends, disruptions and expectations, Assessing and enhancing student experience in higher education, pp. 1-27, (2021); 
Smith C.D., Worsfold K., Davies L., Fisher R., McPhail R., Assessment literacy and student learning: The case for explicitly developing students ‘assessment literacy.’), Assessment & Evaluation in Higher Education, 38, 1, pp. 44-60, (2013); 
Tempelaar D.T., Rienties B., Nguyen Q., Individual differences in the preference for worked examples: Lessons from an application of dispositional learning analytics, Applied Cognitive Psychology, 34, 4, pp. 890-905, (2020); 
Tsai Y.-S., Whitelock-Wainwright A., Gasevic D., More than figures on your laptop:(dis) trustful implementation of learning analytics, Journal of Learning Analytics, 8, 3, pp. 81-100, (2021); 
van der Vleuten C.P.M., The assessment of professional competence: Developments, research and practical implications, Advances in Health Sciences Education, 1, 1, pp. 41-67, (1996); 
van der Vleuten C.P.M., Schuwirth L.W.T., Assessing professional competence: From methods to programmes, Medical Education, 39, 3, pp. 309-317, (2005); 
Venkatraman S., A framework for implementing TQM in higher education programs, Quality Assurance in Education, 15, 1, pp. 92-112, (2007); 
von Winterfeldt D., Edwards W., Decision analysis and behavioural research, (1986); 
Winne P.H., Construct and consequential validity for learning analytics based on trace data, Computers in Human Behavior, 112, (2020)#FRF#
