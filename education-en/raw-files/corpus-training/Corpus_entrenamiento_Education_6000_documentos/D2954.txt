#ITI#Estimating the extent of selective reporting: An application to economics#FTI#
#IRE# Using a sample of 70,399 published p-values from 192 meta-analyses, we empirically estimate the counterfactual distribution of p-values in the absence of any biases. Comparing observed p-values with counterfactually expected p-values allows us to estimate how many p-values are published as being statistically significant when they should have been published as non-significant. We estimate the extent of selectively reported p-values to range between 57.7% and 71.9% of the significant p-values. The counterfactual p-value distribution also allows us to assess shifts of p-values along the entire distribution of published p-values, revealing that particularly very small p-values (p < 0.001) are unexpectedly abundant in the published literature. Subsample analysis suggests that the extent of selective reporting is reduced in research fields that use experimental designs, analyze microeconomics research questions, and have at least some adequately powered studies#FRE#
#IPC# meta-research; p-hacking; publication bias; selective reporting#FPC#
#IRF# Havranek T., Measuring intertemporal substitution: the importance of method choices and selective reporting, J Eur Econ Assoc, 13, 6, pp. 1180-1204, (2015); 
Camerer C.F., Dreber A., Forsell E., Et al., Evaluating replicability of laboratory experiments in economics, Science, 351, 6280, pp. 1433-1436, (2016); 
Ioannidis J., Stanley T., Doucouliagos C., The power of bias in economics research, Econ J, 127, pp. F236-F265, (2017); 
Chang A.C., Li P., A preanalysis plan to replicate sixty economics research papers that worked half of the time, Am Econ Rev, 107, 5, pp. 60-64, (2017); 
Bruns S.B., Asanov I., Bode R., Et al., Reporting errors and biases in published empirical findings: evidence from innovation research, Res Policy, 48, 9, (2019); 
Vivalt E., Specification searching and significance inflation across time, methods and disciplines, Oxf Bull Econ Stat, 81, 4, pp. 797-816, (2019); 
Havranek T., Irsova Z., Laslopova L., Zeynalova O., Publication and attenuation biases in measuring skill substitution, Rev Econ Stat, pp. 1-37, (2022); 
Estimating the reproducibility of psychological science, Science, 349, 6251, pp. 253-267, (2015); 
Fanelli D., Costas R., Ioannidis J.P.A., Meta-assessment of bias in science, Proc Natl Acad Sci, 114, 14, pp. 3714-3719, (2017); 
David S.P., Naudet F., Laude J., Et al., Potential reporting bias in neuroimaging studies of sex differences, Sci Rep, 8, 1, pp. 1-8, (2018); 
Ioannidis J.P., Why most published research findings are false, PLoS Med, 2, 8, (2005); 
Glaeser E.L., Researcher incentives and empirical methods, The Foundations of Positive and Normative Economics: A Handbook, pp. 300-319, (2008); 
Stanley T.D., Doucouliagos H., Meta-Regression Analysis in Economics and Business, (2012); 
Simonsohn U., Nelson L.D., Simmons J.P., P-curve: a key to the file-drawer, J Exp Psychol Gen, 143, 2, pp. 534-547, (2014); 
Bruns S.B., Ioannidis J.P.A., P-curve and p-hacking in observational research, PLoS One, 11, 2, (2016); 
Kerr N.L., HARKing: hypothesizing after the results are known, Pers Soc Psychol Rev, 2, 3, pp. 196-217, (1998); 
Rosenthal R., The file drawer problem and tolerance for null results, Psychol Bull, 86, 3, pp. 638-641, (1979); 
Nosek B.A., Spies J.R., Motyl M., Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability, Perspect Psychol Sci, 7, 6, pp. 615-631, (2012); 
Leamer E.E., Leamer E.E., Specification Searches: Ad Hoc Inference with Nonexperimental Data, 53, (1978); 
Hendry D.F., Econometrics – alchemy or science?, Economica, 47, 188, pp. 387-406, (1980); 
Sterling T.D., Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa, J Am Stat Assoc, 54, 285, pp. 30-34, (1959); 
Selvin H.C., Stuart A., Data-dredging procedures in survey analysis, Am Stat, 20, 3, pp. 20-23, (1966); 
Leamer E.E., Sensitivity analysis would help, Am Econ Rev, 75, 3, (1985); 
Bruns S.B., Kalthaus M., Flexibility in the selection of patent counts: implications for p-hacking and evidence-based policymaking, Res Policy, 49, 1, (2020); 
Huntington-Klein N., Arenas A., Beam E., Et al., The influence of hidden researcher decisions in applied microeconomics, Econ Inq, 59, 3, pp. 944-960, (2021); 
Silberzahn R., Uhlmann E.L., Martin D.P., Et al., Many analysts, one data set: making transparent how variations in analytic choices affect results, Adv Methods Pract Psychol Sci, 1, 3, pp. 337-356, (2018); 
Simmons J.P., Nelson L.D., Simonsohn U., False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant, Psychol Sci, 22, 11, pp. 1359-1366, (2011); 
Casey K., Glennerster R., Miguel E., Reshaping institutions: evidence on aid impacts using a preanalysis plan, Q J Econ, 127, 4, pp. 1755-1812, (2012); 
Franco A., Malhotra N., Simonovits G., Publication bias in the social sciences: unlocking the file drawer, Science, 345, 6203, pp. 1502-1505, (2014); 
Kunda Z., The case for motivated reasoning, Psychol Bull, 108, 3, pp. 480-498, (1990); 
Gechert S., Havranek T., Irsova Z., Kolcunova D., Measuring capital-labor substitution: the importance of method choices and publication bias, Rev Econ Dyn, 45, pp. 55-82, (2022); 
Ioannidis J.P., Trikalinos T.A., An exploratory test for an excess of significant findings, Clin Trials, 4, 3, pp. 245-253, (2007); 
Duflo E., Banerjee A.V., Finkelstein A., Katz L.F., Olken B., Sautmann A., In praise of moderation: suggestions for the scope and use of pre-analysis plans for RCTs in economics. NBER Working Paper, (2020); 
Olken B.A., Promises and perils of pre-analysis plans, J Econ Perspect, 29, 3, pp. 61-80, (2015); 
Imbens G.W., Statistical significance, p-values, and the reporting of uncertainty, J Econ Perspect, 35, 3, pp. 157-174, (2021); 
Bom P.R., Rachinger H., A generalized-weights solution to sample overlap in meta-analysis, Res Synth Methods, 11, 6, pp. 812-832, (2020); 
Abadie A., Athey S., Imbens G.W., Wooldridge J., When should you adjust standard errors for clustering?, Q J Econ, 138, pp. 1-35, (2023); 
Putz P., Bruns S.B., The (non-)significance of reporting errors in empirical economics: evidence from three top journals, J Econ Surveys, 35, 1, pp. 348-373, (2021); 
Kraemer H.C., Gardner C., Brooks J.O., Yesavage J.A., Advantages of excluding underpowered studies in meta-analysis: Inclusionist versus exclusionist viewpoints, Psychol Methods, 3, 1, pp. 23-31, (1998); 
Stanley T.D., Doucouliagos H., Meta-regression approximations to reduce publication selection bias, Res Synth Methods, 5, 1, pp. 60-78, (2014); 
Stanley T., Doucouliagos H., Ioannidis J.P., Carter E.C., Detecting publication selection bias through excess statistical significance, Res Synth Methods, 12, pp. 1-20, (2021); 
Kavvoura F.K., McQueen M.B., Khoury M.J., Tanzi R.E., Bertram L., Ioannidis J.P., Evaluation of the potential excess of statistically significant findings in published genetic association studies: application to Alzheimer's disease, Am J Epidemiol, 168, 8, pp. 855-865, (2008); 
Ioannidis J.P., Clarifications on the application and interpretation of the test for excess significance and its extensions, J Math Psychol, 57, 5, pp. 184-187, (2013); 
Brodeur A., Le M., Sangnier M., Zylberberg Y., Star wars: the empirics strike back, Am Econ J Appl Econ, 8, 1, pp. 1-32, (2016); 
Benjamin D.J., Berger J.O., Johannesson M., Et al., Redefine statistical significance, Nat Hum Behav, 2, 1, pp. 6-10, (2018); 
Gerber S., Malhotra N., Publication bias in empirical sociological research: do arbitrary significance levels distort published results?, Sociol Methods Res, 37, 1, pp. 3-30, (2008); 
Elliott G., Kudrin N., Wuthrich K., Detecting p-hacking, Econometrica, 90, 2, pp. 887-906, (2022); 
Yang Y., Sanchez-Tojar A., O'Dea R.E., Et al., Publication bias impacts on effect size, statistical power, and magnitude (Type M) and sign (Type S) errors in ecology and evolutionary biology, BMC Biol, 21, 1, pp. 1-20, (2023); 
Bartos F., Maier M., Shanks D.R., Stanley T., Sladekova M., Wagenmakers E.J., Meta-analyses in psychology often overestimate evidence for and size of effects, R Soc Open Sci, 10, 7, (2023); 
Ioannidis J.P., Cappelleri J.C., Lau J., Issues in comparisons between meta-analyses and large trials, JAMA, 279, 14, pp. 1089-1093, (1998); 
Angrist J.D., Pischke J.S., The credibility revolution in empirical economics: how better research design is taking the con out of econometrics, J Econ Perspect, 24, 2, pp. 3-30, (2010)#FRF#
