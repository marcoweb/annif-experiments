#ITI#Comparison of Computer Scoring Model Performance for Short Text Responses Across Undergraduate Institutional Types#FTI#
#IRE# Constructed response (CR) assessments allow students to demonstrate understanding of complex topics and provide teachers with deeper insight into student thinking. Computer scoring models (CSMs) remove the barrier of increased time and effort, making CR more accessible. As CSMs are commonly created using responses from research-intensive colleges and universities (RICUs), this pilot study examines the effectiveness of seven previously developed CSMs on diverse CRs from RICUs, two-year colleges (TYCs), and primarily undergraduate institutions (PUIs). We asked if accuracy of the CSMs was maintained with a new testing set of CRs and if CSM accuracy differed among different institutional types. A human scorer and the CSMs analytically categorized 444 CRs for the presence or absence of seven ideas relating to weight loss. Comparing human and CSM predictions revealed five CSMs maintained high agreement (Cohen’s kappa > 0.80); however, two CSMs demonstrated reduced agreement (Cohen’s kappa < 0.65). Seventy-one percent of these miscodes were false negatives. RICU responses were 1.4 times more likely to be miscoded than TYCs (p = 0.038) or PUIs (p = 0.047) across all seven CSMs. However, this increased frequency may result from the higher number of ideas in RICU responses in comparison to TYCs (p = 0.082) and PUIs (p = 0.013). Accounting for increased ideas removed the significant difference between RICUs and TYCs (p = 0.23) and PUIs (p = 0.54). Finally, qualitative examination of miscodes provides insight into reduced CSM performance. Collectively, these data support the utility of these CSMs across institutional types and with novel CRs#FRE#
#IPC# Assessment; Automated analysis; Computer scoring models; Constructed response; Institutional types; Machine learning#FPC#
#IRF# Altman D.G., Practical statistics for medical research, (1991); 
Vision and Change in Undergraduate Biology Education: A Call to Action, (2011); 
Automated analysis of constructed response, (2020); 
Buck Bracey Z., Stuhlsatz M., Cheuk T., Santiago M.M., Wilson C., Osborne J., Haudek K.C., Donovan B., Investigating Differential Severity across Linguistic Subgroups in Automated Scoring of Student Argumentation; 
Bridgeman B., Trapani C., Attali Y., Comparison of human and machine scoring of essays: Differences by gender, ethnicity, and country, Applied Measurement in Education, 25, 1, pp. 27-40, (2012); 
Cohen J., A coefficient of agreement for nominal scales, Educational and Psychological Measurement, 20, 1, pp. 37-46, (1960); 
Fleiss J.L., Cohen J., The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability, Educational and Psychological Measurement, 33, 3, pp. 613-619, (1973); 
Gerard L.F., Linn M.C., Using automated scores of student essays to support teacher guidance in classroom inquiry, Journal of Science Teacher Education, 27, 1, pp. 111-129, (2016); 
Ha M., Nehm R.H., The impact of misspelled words on automated computer scoring: A case study of scientific explanations, Journal of Science Education and Technology, 25, 3, pp. 358-374, (2016); 
Ha M., Nehm R.H., Urban-Lurain M., Merrill J.E., Applying computerized-scoring models of written biological explanations across courses and colleges: Prospects and limitations, CBE—Life Sciences Education, 10, 4, pp. 379-393, (2011); 
Haudek K.C., Kaplan J.J., Knight J., Long T., Merrill J.E., Munn A., Nehm R., Smith M., Urban-Lurain M., Harnessing technology to improve formative assessment of student conceptions in STEM: Forging a national network, CBE—Life Sciences Education, 10, 2, pp. 149-155, (2011); 
Haudek K.C., Prevost L.B., Moscarella R.A., Merrill J., Urban-Lurain M., What are they thinking? Automated analysis of student writing about acid–base chemistry in introductory biology, CBE – Life Sciences Education, 11, 3, (2012); 
Holstein K., Wortman Vaughan J., Dudik M., Wallach H., Improving fairness in machine learning systems: What do industry practitioners need?, In Proceedings of the ACM CHI Conference on Human Factors in Computer Systems, Glasgow, UK, pp. 1-16, (2019); 
Hubbard J.K., Potts M.A., Couch B.A., How question types reveal student thinking: An experimental comparison of multiple-true-false and free-response formats, CBE—Life Sciences Education, 16, 2, (2017); 
Jescovitch L.N., Scott E.E., Cerchiara J.A., Merrill J.E., Urban-Luain M., Doherty J.H., Haudek K.C., Comparison of machine learning performance using analytic and holistic coding approaches across constructed response assessments aligned to a science learning progression, Journal of Science Education and Technology, (2020); 
Kanim S., Cid X.C., The demographics of physics education research, (2017); 
Kaplan J.J., Haudek K.C., Ha M., Rogness N., Fisher D.G., Using lexical analysis software to assess student writing in statistics. Technology Innovations in Statistics Education, (2014); 
Koizumi R., Relationships between text length and lexical diversity measures: Can we use short texts of less than 100 tokens?, Vocabulary Learning and Instruction, (2012); 
Landis J., Koch G., The measurement of observer agreement for categorical data, Biometrics, 33, 1, pp. 159-174, (1977); 
Linn M.C., Gerard L.F., Using automated scores of student essays to support teacher guidance in classroom inquiry, Journal of Science Teacher Education, 27, 1, pp. 111-129, (2016); 
Liu O.L., Brew C., Blackmore J., Gerard L., Madhok J., Linn M.C., Automated scoring of constructed-response science items: Prospects and obstacles, Educational Measurement: Issues and Practice, 33, 2, pp. 19-28, (2014); 
Lo S.M., Gardner G.E., Reid J., Napoleon-Fanis V., Carroll P., Smith E., Sato B.K., Prevailing questions and methodologies in biology education research: A longitudinal analysis of research in CBE—Life sciences education and at the society for the advancement of biology education research, CBE—Life Sciences Education, 18, 1, (2019); 
Lyford A., Kaplan J.J., Improving student learning and instructional effectiveness through the innovative use of automated analysis of formative assessments, ICOTS10, (2018); 
Moharreri K., Ha M., Nehm R., EvoGrader: An online formative assessment tool for automatically evaluating written evolutionary explanations, Evolution: Education and Outreach, (2014); 
A framework for K–12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
Nehm R.H., Ha M., Mayfield E., Transforming biology assessment with machine learning: Automated scoring of written evolutionary explanations, Journal of Science Education and Technology, 21, pp. 183-196, (2012); 
Nehm R.H., Haertig H., Human vs. computer diagnosis of students’ natural selection knowledge: Testing the efficacy of text analytic software, Journal of Science Education and Technology, 21, 1, pp. 56-73, (2012); 
Nehm R.H., Reilly L., Biology majors’ knowledge and misconceptions of natural selection, BioScience, 57, 3, pp. 263-272, (2007); 
Nehm R.H., Schonfeld I.S., Measuring knowledge of natural selection: A comparison of the CINS, an open-response instrument, and an oral interview, Journal of Research in Science Teaching, 45, 10, pp. 1131-1160, (2008); 
Nenortas A., Fields L., Editorial: Two-year colleges (TYCs) are essential in science education research and TYC faculty and students should get involved, Journal of College Science Teaching, 49, 1, pp. 9-11, (2019); 
Noyes K., McKay R.L., Neumann M., Haudek K.C., Cooper M.M., Developing computer resources to automate analysis of students’ explanations of London dispersion forces, Journal of Chemical Education, 14, (2020); 
Paquette L., Ocumpaugh J., Li Z., Andres A., Baker R., Who’s learning? Using demographics in EDM research, Journal of Educational Data Mining, 12, 3, pp. 1-30, (2020); 
Pelletreau K.N., Andrews T., Armstrong N., Bedell M.A., Dastoor F., Dean N., Et al., A clicker-based study that untangles student thinking about the processes in the central dogma, CourseSource, (2016); 
Ost L.B., Smith M.K., Knight J.K., Using student writing and lexical analysis to reveal student thinking about the role of stop codons in the central dogma, CBE – Life Sciences Education, (2016); 
Porter M.F., An algorithm for suffix stripping, Program, 14, pp. 130-137, (1980); 
Powers D.E., Escoffery D.S., Duchnowski M.P., Validating automated essay scoring: A (modest) refinement of the “gold standard, Applied Measurement in Education, 28, 2, pp. 130-142, (2015); 
Schinske J.N., Balke V.L., Bangera M.G., Bonney K.M., Brownell S.E., Carter R.S., Et al., Broadening participation in biology education research: Engaging community college students and faculty, CBE—Life Sciences Education, 16, 2, (2017); 
Shapiro D., Dundar A., Chen J., Ziskin M., Park E., Torres V., Chiang Y., Completing College: A National View of Student Attainment Rates. (Signature Report No, (2012); 
Shermis M.D., Contrasting state-of-the-art in the machine scoring of short-form constructed responses, Educational Assessment, 20, 1, pp. 46-65, (2015); 
Sieke S.A., McIntosh B.B., Steele M.M., Knight J.K., Characterizing students’ ideas about the effects of a mutation in a noncoding region of DNA, CBE—Life Sciences Education, (2019); 
Shermis M.D., Mao L., Mulholland M., Kieftenbeld V., Use of automated scoring features to generate hypotheses regarding language-based DIF, International Journal of Testing, 17, 4, pp. 351-371, (2017); 
Sripathi K.N., Moscarella R.A., Yoho R., You H.S., Urban-Lurain M., Merrill J., Haudek K., Mixed student ideas about mechanisms of human weight loss, CBE—Life Sciences Education, 18, 3, (2019); 
Thompson S.K., Hebert S., Berk S., Brunilli R., Creesch C., Drake A.G., Fagbodum S., Garcia-Ojeda M.E., Hall C., Harshman J., Lamb T., Robnett R., Shuster M., Cotner S., Ballen C.J., A call for data-driven networks to address equity in the context for undergraduate biology. CBE—Life Sciences Education, 19(4) Special Section on Cross-Disciplinary Research in Biology Education, (2020); 
Tomas C., Whitt E., Lavelle-Hill R., Severn K., Modeling holistic marks with analytic rubrics, Frontiers in Education, 4, 89, (2019); 
Uhl J.D., Sripathi K.N., Meir E., Merrill J., Urban-Lurain M., Haudek K.C., Automated writing assessments measure undergraduate learning after completion of a computer-based cellular respiration tutorial, CBE - Life Sciences Education, (2021); 
Urban-Lurain M., Moscarella R.A., Haudek K.C., Giese E., Sibley D.F., Merrill J.E., Beyond multiple choice exams: Using computerized lexical analysis to understand students’ conceptual reasoning in STEM disciplines. 39th IEEE Frontiers in Education Conference, San Antonio, TX, 2009, pp. 1-6, (2009); 
Williamson D., Xi X., Breyer J., A framework for evaluation and use of automated scoring, Educational Measurement: Issues and Practice, 31, 1, pp. 2-13, (2012); 
Zhai X., Yin Y., Pellegrino J.W., Haudek K.C., Shi L., Applying machine learning in science assessment: A systematic review, Studies in Science Education, 56, 1, pp. 111-151, (2020)#FRF#
