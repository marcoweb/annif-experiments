#ITI#Automated Feedback and Automated Scoring in the Elementary Grades: Usage, Attitudes, and Associations with Writing Outcomes in a Districtwide Implementation of MI Write#FTI#
#IRE# This study examined a naturalistic, districtwide implementation of an automated writing evaluation (AWE) software program called MI Write in elementary schools. We specifically examined the degree to which aspects of MI Write were implemented, teacher and student attitudes towards MI Write, and whether MI Write usage along with other predictors like demographics and writing self-efficacy explained variability in students’ performance on a proximal and distal measure of writing performance. The participants included 1935 students in Grades 3–5 and 135 writing teachers from 14 elementary schools in a mid-Atlantic school district. Findings indicated that though MI Write was somewhat under-utilized, teachers and students held positive attitudes towards the AWE system. Usage of MI Write had a mixed and limited predictive effect on outcomes: The number of essays written had a small predictive effect on state test performance for Grades 3 and 5; gain on revision had a moderate predictive effect on posttest writing quality and a small predictive effect for Grade 5 state test performance. Students’ average AWE scores showed consistently moderate to large predictive effects for all outcomes. Interpreted in light of the underlying architecture of MI Write, findings have implications for other school districts considering implementing AWE as well as the design of AWE systems intended to support the teaching and learning of writing#FRE#
#IPC# Automated essay scoring; Automated feedback; Automated writing evaluation; Educational technology; Writing assessment#FPC#
#IRF# Allison P.D., Fixed effects regression models, (2009); 
Applebee A.N., Langer J.A., What is happening in the teaching of writing?, English Journal, 98, 5, pp. 18-28, (2009); 
Archer A.L., Hughes C.A., Explicit instruction: Effective and efficient teaching, (2011); 
Attali Y., Exploring the Feedback and Revision Features of Criterion. Paper Presented at the National Council of Measurement in Education (NCME), (2004); 
Bai L., Hu G., In the face of fallible AWE feedback: How do students respond?, Educational Psychology, 37, pp. 67-81, (2017); 
Bejar I.I., Flor M., Futagi Y., Ramineni C., On the vulnerability of automated scoring to construct-irrelevant response strategies (CIRS): an illustration, Assessing Writing, 22, pp. 48-59, (2014); 
Bernoff J., Bad Writing Costs Businesses Billions. Daily Beast, (2017); 
Biber D., Nekrasova T., Horn B., The effectiveness of feedback for L1-English and L2-writing development: A meta-analysis. TOEFL iBT™ research report, (2011); 
Black P., Wiliam D., Developing the theory of formative assessment, Educational Assessment, Evaluation and Accountability, 21, pp. 5-31, (2009); 
Britt M.A., Wiemer-Hastings P., Larson A.A., Perfetti C.A., Using intelligent feedback to improve sourcing and integration in students’ essays, International Journal of Artificial Intelligence in Education, 14, pp. 359-374, (2004); 
Bruning R.H., Kauffman D.F., Self-efficacy beliefs and motivation in writing development, Handbook of writing research, pp. 160-173, (2016); 
Bruning R., Dempsey M., Kauffman D.F., McKim C., Zumbrunn S., Examining dimensions of self-efficacy for writing, Journal of Educational Psychology, 105, pp. 25-38, (2013); 
Caccamise D., Franzke M., Eckhoff A., Kintsch E., Kintsch W., Guided practice in technology-based summary writing, Reading comprehension strategies: Theories, interventions, and technologies, pp. 375-396, (2007); 
Chapelle C.A., Cotos E., Lee J., Validity arguments for diagnostic assessment using automated writing evaluation, Language Testing, 33, pp. 385-405, (2015); 
Chen C.E., Cheng W.E., Beyond the design of automated writing evaluation: pedagogical practices and perceived learning effectiveness in EFL writing classes, Language Learning & Technology, 12, 2, pp. 94-112, (2008); 
Clare L., Valdes R., Patthey-Chavez G.G., Learning to write in urban elementary and middle schools: An investigation of teachers’ written feedback on student compositions (Center for the Study of Evaluation Technical Report No. 526), Los Angeles: University of California, Center for Research on Evaluation, Standards, and Student Testing (CRESST)., (2000); 
Cohen J., Statistical power analysis for the behavioral sciences, (1988); 
Writing Assessment: A Position Statement, (2014); 
Deane P., Wilson J., Zhang M., Li C., van Rijn P., Guo H., Roth A., Winchester E., Richter T., The sensitivity of a scenario-based assessment of written argumentation to school differences in curriculum and instruction, International Journal of Artificial Intelligence in Education, (2020); 
Dujinhower H., Prins F.J., Stokking K.M., Feedback providing improvement strategies and reflection on feedback use: effects on students’ writing motivation, process, and performance, Learning and Instruction, 22, pp. 171-184, (2012); 
Ericcson P.F., Haswell R.J., In machine scoring of student essays: truth and consequences, (2006); 
Fitzgerald J., Research on revision in writing, Review of Educational Research, 57, pp. 481-506, (1987); 
Foltz P.W., Lochbaum K.E., Rosenstein M.B., Analysis of student ELA writing performance for a large scale implementation of formative assessment, Paper Presented at the Annual Meeting of the National Council for Measurement in Education, (2011); 
Foltz P.W., Streeter L.A., Lochbaum K.E., Landauer T.K., Implementation and applications of the intelligent essay assessor, Handbook of automated essay evaluation, pp. 68-88, (2013); 
Franzke M., Kintsch E., Caccamise D., Johnson N., Dooley S., Summary street®: computer support for comprehension and writing, Journal of Educational Computing Research, 33, pp. 53-80, (2005); 
Gansle K.A., VanDerHeyden A.M., Noell G.H., Resetar J.L., Williams K.L., The technical adequacy of curriculum-based and rating-based measures of written expression of elementary school students, School Psychology Review, 35, pp. 435-450, (2006); 
Gilbert J., Graham S., Teaching writing to elementary students in grades 4-6: a national survey, Elementary School Journal, 110, pp. 494-518, (2010); 
Graham S., A revised writer(s)-within-community model of writing, Educational Psychologist, 53, pp. 258-279, (2018); 
Graham S., Perin D., Writing next: Effective strategies to improve writing of adolescents in middle and high schools – A report to Carnegie Corporation of New York, (2007); 
Graham S., Berninger V., Fan W., The structural relationship between writing attitude and writing achievement in first and third grade students, Contemporary Educational Psychology, 32, 3, pp. 516-536, (2007); 
Graham S., Berninger V., Abbott R., Are attitudes toward writing and reading separable constructs? A study with primary grade children, Reading & Writing Quarterly, 28, pp. 51-69, (2012); 
Graham S., Bollinger A., Booth Olson C., D'Aoust C., Macarthur C., McCutchen D., Olinghouse N., Teaching elementary school students to be effective writers: A practice guide (NCEE 2012–4058), Washington, DC: National Center for Education Evaluation and Regional Assistance, Institute of Education Sciences, U.S. Department of Education., (2012); 
Graham S., Hebert M., Harris K.R., Formative assessment and writing: a meta-analysis, Elementary School Journal, 115, pp. 523-547, (2015); 
Graham S., Collins A.A., Rigby-Wells H., Writing characteristics of students with learning disabilities and typically achieving peers: a meta-analysis, Exceptional Children, 83, pp. 199-218, (2017); 
Grimes D., Warschauer M., Utility in a fallible tool: a multi-site case study of automated writing evaluation, Journal of Technology, Learning, and Assessment, 8, 6, pp. 4-43, (2010); 
Hattie J., Timperley H., The power of feedback, Review of Educational Research, 77, pp. 81-112, (2007); 
Hayes J.R., Modeling and remodeling writing, Written Communication, 29, 3, pp. 369-388, (2012); 
Herrington A., Moran C., What happens when machines read our students’ writing?, College English, 63, 4, pp. 480-499, (2001); 
Higgins D., Heilman M., Managing what we can measure: quantifying the susceptibility of automated scoring systems to gaming behavior, Educational Measurement Issues and Practice, 33, 3, pp. 36-46, (2014); 
Hornick K., Approximation capabilities of multilayer feedforward networks, Neural Networks, 4, pp. 359-366, (1991); 
Kellogg R.T., Whiteford A.P., Training advanced writing skills: the case for deliberative practice, Educational Psychologist, 44, pp. 250-266, (2009); 
Kellogg R.T., Whiteford A.P., Quinlan T., Does automated feedback help students learn to write?, Journal of Educational Computing Research, 42, pp. 173-196, (2010); 
Kiuhara S.A., Graham S., Hawken L.S., Teaching writing to high school students: a national survey, Journal of Educational Psychology, 101, pp. 136-160, (2009); 
Klobucar A., Elliot N., Deess P., Rudniy O., Joshi K., Automated scoring in context: rapid assessment for placed students, Assessing Writing, 18, pp. 62-84, (2013); 
Lee V., Using hierarchical linear modeling to study social contexts: the case of school effects, Educational Psychologist, 35, pp. 125-141, (2000); 
Lorah J., Effect size measures for multilevel models: definition, interpretation, and TIMSS example, Large-Scale Assessments in Education, 6, 8, pp. 1-11, (2018); 
MacArthur C.A., Instruction in evaluation and revision, Handbook of writing research, pp. 272-287, (2016); 
Matsumara L.C., Patthey-Chavez G.G., Patthey-Chavez V., M. R., & Garnier, H., Teacher feedback, writing assignment quality, and third-grade students’ revision in lower- and higher-achieving urban schools, Elementary School Journal, 103, pp. 3-25, (2002); 
Mayfield E., Butler S., Districtwide implementations outperform isolated use of automated feedback in high school writing, Paper Presented at the International Conference of the Learning Sciences, (2018); 
Mishra P., Koehler M.J., Technological pedagogical content knowledge: a framework for teacher knowledge, Teachers College Record, 108, pp. 1017-1054, (2006); 
Moore N.S., MacArthur C.A., Student use of automated essay evaluation technology during revision, Journal of Writing Research, 8, pp. 149-175, (2016); 
The Nation’s Report Card: Writing 2011(NCES 2012–470), Institute of Education Sciences, U.S. Department of Education, Washington, D.C., (2012); 
The neglected “R”: The need for a writing revolution, (2003); 
Writing: A ticket to work…or a ticket out. A survey of business leaders, (2004); 
Writing: A powerful message from state government, (2005); 
NCTE position statement on machine scoring, (2013); 
Newman D., Jaciw A.P., Lazarev V., Guidelines for conducting and reporting EdTech impact research in U.S. K-12 schools, Empirical Education, (2018); 
Nguyen H., Xiong W., Litman D., Iterative design and classroom evaluation of automated feedback for improving peer feedback localization, International Journal of Artificial Intelligence in Education, 27, pp. 582-622, (2017); 
An introduction to the 6+1 trait writing assessment model, (2004); 
Pajares F., Self-efficacy beliefs, motivation, and achievement in writing: a review of the literature, Reading & Writing Quarterly, 19, pp. 139-158, (2003); 
Palermo C., Thomson M.M., Teacher implementation of self-regulated strategy development with an automated writing evaluation system: effects on the argumentative writing performance of middle school students, Contemporary Educational Psychology, 54, pp. 255-270, (2018); 
Parr J.M., Timperley H.S., Feedback to writing, assessment for teaching and learning and student progress, Assessing Writing, 15, pp. 68-85, (2010); 
Parra G.L., Calero S.X., Automated writing evaluation tools in the improvement of the writing skill, International Journal of Instruction, 12, 2, pp. 209-226, (2019); 
Perelman L., When the “state of the art” is counting words, Assessing Writing, 21, pp. 104-111, (2014); 
Persky H.R., Daane M.C., Jin Y., The Nation’s report card: Writing 2002. (NCES 2003–529), National Center for Education Statistics, Institute of Education Sciences. U. S. Department for Education, (2002); 
Rapp C., Kauf P., Scaling academic writing instruction: evaluation of a scaffolding tool (thesis writer), International Journal of Artificial Intelligence in Education, 28, pp. 590-615, (2018); 
Raudenbush S.W., Bryk A.S., Hierarchical linear models: Applications and data analysis methods, (2002); 
Roscoe R.D., McNamara D.S., Writing pal: feasibility of an intelligent writing strategy tutor in the high school classroom, Journal of Educational Psychology, 105, pp. 1010-1025, (2013); 
Roscoe R.D., Varner L.K., Crossley S.A., McNamara D.S., Developing pedagogically-guided algorithms for intelligent writing feedback, International Journal of Learning Technology, 8, pp. 362-381, (2013); 
Roscoe R.D., Jacovina M.E., Allen L.K., Johnson A.C., McNamara D.S., Towards revision-sensitive feedback in automated writing evaluation,  International Conference on Educational Data Mining, pp. 628-629, (2016); 
Roscoe R.D., Wilson J., Johnson A.C., Mayra C.R., Presentation, expectations, and experience: sources of student perceptions of automated writing evaluation, Computers in Human Behavior, 70, pp. 207-221, (2017); 
Roscoe R.D., Allen L.K., Johnson A.C., McNamara D.S., Automated writing instruction and feedback: Instructional mode, attitudes, and revising, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, pp. 2089-2093, (2018); 
Salahu-Din D., Persky H., Miller J., The Nation’s Report Card: Writing 2007 (NCES, pp. 2008-2468, (2008); 
Shermis M.D., State-of-the-art automated essay scoring: competition, results, and future directions from a United States demonstration, Assessing Writing, 20, pp. 53-76, (2014); 
Shermis M.D., Koch C.M., Page E.B., Keith T.Z., Harrington S., Trait ratings for automated essay grading, Educational and Psychological Measurement, 62, pp. 5-18, (2002); 
Shermis M.D., Burstein J.C., Bliss L., The impact of automated essay scoring on high stakes writing assessments, Paper Presented at the Annual Meeting of the National Council on Measurement in Education, (2004); 
Shute V.J., Focus on formative feedback, Review of Educational Research, 78, pp. 153-189, (2008); 
Smarter Balanced Assessment Consortium: 2017–18 summative technical report, (2018); 
Snijders T.A.B., Bosker R.J., Multilevel analysis: An introduction to basic and advanced multilevel modeling, (2012); 
Stevenson M., A critical interpretative synthesis: the integration of automated writing evaluation into classroom writing instruction, Computers and Composition, 42, pp. 1-16, (2016); 
Stevenson M., Phakiti A., The effects of computer-generated feedback on the quality of writing, Assessing Writing, 19, pp. 51-65, (2014); 
Tansomboon C., Gerard L.F., Vitale J.M., Linn M.C., Designing automated guidance to promote productive revision of science explanations, International Journal of Artificial Intelligence in Education, 27, pp. 729-757, (2017); 
Troia G.A., Harbaugh A.G., Shankland R.K., Wolbers K.A., Lawrence A.M., Relationships between writing motivation, writing activity, and writing performance: effects of grade, sex, and ability, Reading and Writing, 26, pp. 17-44, (2013); 
Troia G.A., Olinghouse N.G., Zhang M., Wilson J., Stewart K.A., Mo Y., Hawkins L., Content and alignment of state writing standards and assessments as predictors of student writing achievement: an analysis of 2007 National Assessment of Educational Progress data, Reading and Writing, 31, pp. 835-864, (2018); 
Vandermeulen N., Leijten M., van Waes L., Reporting writing process feedback in the classroom: Using keystroke logging data to reflect on writing processes, Journal of Writing Research, 12, 1, pp. 109-140, (2020); 
Warschauer M., Grimes D., Automated writing assessment in the classroom, Pedagogies, 3, pp. 22-36, (2008); 
Williamson D.M., Xi X., Breyer F.J., A framework for evaluation and use of automated scoring, Educational Measurement: Issues and Practice, 31, pp. 2-13, (2012); 
Wilson J., Associated effects of automated essay evaluation software on growth in writing quality for students with and without disabilities, Reading and Writing, 30, pp. 691-718, (2017); 
Wilson J., Universal screening with automated essay scoring: evaluating classification accuracy in Grades 3 and 4, Journal of School Psychology, 68, pp. 19-37, (2018); 
Wilson J., Andrada G.N., Using automated feedback to improve writing quality: Opportunities and challenges, Handbook of research on technology tools for real-world skill development, pp. 678-703, (2016); 
Wilson J., Czik A., Automated essay evaluation software in English language arts classrooms: effects on teacher feedback, student motivation, and writing quality, Computers and Education, 100, pp. 94-109, (2016); 
Wilson J., Roscoe R.D., Automated writing evaluation and feedback: Multiple metrics of efficacy, Journal of Educational Computing Research, 58, pp. 87-125, (2020); 
Wilson J., Olinghouse N.G., Andrada G.N., Does automated feedback improve writing quality?, Learning Disabilities: A Contemporary Journal, 12, pp. 93-118, (2014); 
Wilson J., Olinghouse N.G., McCoach D.B., Andrada G.N., Santangelo T., Comparing the accuracy of different scoring methods for identifying sixth graders at risk of failing a state writing assessment, Assessing Writing, 27, pp. 11-23, (2016); 
Zellermayer M., Salomon G., Globerson T., Givon H., Enhancing writing-related metacognition through a computerized writing partner, American Educational Research Journal, 28, pp. 373-391, (1991); 
Zhang H., Magooda A., Litman D., Correnti R., Wang E., Matsumara L.C., Quintanta R., ERevise: Using natural language processing to provide formative feedback on text evidence usage in student writing, Proceedings of the AAAI Conference on Innovative Applications of Artificial Intelligence, 33, pp. 9619-9625, (2019)#FRF#
