#ITI#Detecting Rating Scale Malfunctioning With the Partial Credit Model and Generalized Partial Credit Model#FTI#
#IRE# Rating scale analysis techniques provide researchers with practical tools for examining the degree to which ordinal rating scales (e.g., Likert-type scales or performance assessment rating scales) function in psychometrically useful ways. When rating scales function as expected, researchers can interpret ratings in the intended direction (i.e., lower ratings mean “less” of a construct than higher ratings), distinguish between categories in the scale (i.e., each category reflects a unique level of the construct), and compare ratings across elements of the measurement instrument, such as individual items. Although researchers have used these techniques in a variety of contexts, studies are limited that systematically explore their sensitivity to problematic rating scale characteristics (i.e., “rating scale malfunctioning”). I used a real data analysis and a simulation study to systematically explore the sensitivity of rating scale analysis techniques based on two popular polytomous item response theory (IRT) models: the partial credit model (PCM) and the generalized partial credit model (GPCM). Overall, results indicated that both models provide valuable information about rating scale threshold ordering and precision that can help researchers understand how their rating scales are functioning and identify areas for further investigation or revision. However, there were some differences between models in their sensitivity to rating scale malfunctioning in certain conditions. Implications for research and practice are discussed.#FRE#
#IPC# generalized partial credit model; partial credit model; performance assessment; rating scale; survey research#FPC#
#IRF# Adams R.J., Wu M.L., Wilson M., The Rasch rating model and the disordered threshold controversy, Educational and Psychological Measurement, 72, 4, pp. 547-573, (2012); 
Andrich D.A., A rating formulation for ordered response categories, Psychometrika, 43, 4, pp. 561-573, (1978); 
Andrich D.A., The detection of a structural halo when multiple criteria have the same generic categories for rating, (2010); 
Andrich D.A., An expanded derivation of the threshold structure of the polytomous Rasch model that dispels any “threshold disorder controversy, Educational and Psychological Measurement, 73, 1, pp. 78-124, (2013); 
Andrich D.A., The problem with the step metaphor for polytomous models for ordinal assessments, Educational Measurement: Issues and Practice, 34, 2, pp. 8-14, (2015); 
Bodner T.E., Missing data: Prevalence and reporting practices, Psychological Reports, 99, 3, pp. 675-680, (2006); 
Bond T.G., Yan Z., Heene M., Applying the Rasch model: Fundamental measurement in the human sciences, (2020); 
Bonifay W., Multidimensional item response theory, (2019); 
Bozdag F., Bilge F., Scale Adaptation for refugee children: Sense of school belonging and social contact, Journal of Psychoeducational Assessment, (2022); 
Briggs D.C., Wilson M., An introduction to multidimensional measurement using Rasch Models, Journal of Applied Measurement, 4, 1, pp. 87-100, (2003); 
Buchholz J., Hartig J., Comparing attitudes across groups: An IRT-Based Item-Fit Statistic for the analysis of measurement invariance, Applied Psychological Measurement, 43, 3, pp. 241-250, (2019); 
Chen W.-H., Thissen D., Local dependence indexes for item pairs using item response theory, Journal of Educational and Behavioral Statistics, 22, 3, pp. 265-289, (1997); 
Crocker L., Algina J., Introduction to classical and modern test theory, (1986); 
DeAyala R.J., The theory and practice of item response theory, (2009); 
Embretson S.E., Reise S.P., Item response theory for psychologists, (2000); 
Engelhard G., Wind S.A., Invariant measurement with raters and rating scales: Rasch models for rater-mediated assessments, (2018); 
Engelhard G., Wang J., Wind S.A., A tale of two models: Psychometric and cognitive perspectives on rater-mediated assessments using accuracy ratings, Psychological Test and Assessment Modeling, 60, 1, pp. 33-52, (2018); 
Finch H., The impact of missing data on the detection of nonuniform differential item functioning, Educational and Psychological Measurement, 71, 4, pp. 663-683, (2011); 
Forrest C.B., Tucker C.A., Ravens-Sieberer U., Pratiwadi R., Moon J., Teneralli R.E., Becker B., Bevans K.B., Concurrent validity of the PROMIS® pediatric global health measure, Quality of Life Research, 25, 3, pp. 739-751, (2016); 
Green S.B., Yang Y., Evaluation of dimensionality in the assessment of internal consistency reliability: Coefficient alpha and omega coefficients, Educational Measurement: Issues and Practice, 34, 4, pp. 14-20, (2015); 
Haddad C., Khoury C., Salameh P., Sacre H., Hallit R., Kheir N., Obeid S., Hallit S., Validation of the Arabic version of the eating attitude test in Lebanon: A population study, Public Health Nutrition, 24, 13, pp. 4132-4143, (2021); 
Hagedoorn E.I., Paans W., Jaarsma T., Keers J.C., van der Schans C.P., Luttik M.L., Krijnen W.P., Translation and psychometric evaluation of the Dutch Families Importance in Nursing Care: Nurses’ attitudes scale based on the Generalized Partial Credit Model, Journal of Family Nursing, 24, 4, pp. 538-562, (2018); 
Hambleton R.K., van der Linden W.J., Wells C.S., IRT models for the analysis of polytomously scored data: Brief and selected history of model building advances, Handbook of polytomous item response theory models, pp. 21-42, (2010); 
Kornetti D.L., Fritz S.L., Chiu Y.-P., Light K.E., Velozo C.A., Rating scale analysis of the Berg balance scale, Archives of Physical Medicine and Rehabilitation, 85, 7, pp. 1128-1135, (2004); 
Linacre J.M., Many-facet Rasch measurement, (1989); 
Linacre J.M., Optimizing rating scale category effectiveness, Journal of Applied Measurement, 3, 1, pp. 85-106, (2002); 
Linacre J.M., Optimizing rating scale category effectiveness, Introduction to Rasch measurement theory: Models and applications, pp. 258-278, (2004); 
Luo G., The relationship between the Rating Scale and Partial Credit models and the implication of disordered thresholds of the Rasch models for polytomous responses, Journal of Applied Measurement, 6, 4, pp. 443-455, (2005); 
Masters G.N., A Rasch model for partial credit scoring, Psychometrika, 47, 2, pp. 149-174, (1982); 
Mellenbergh G.J., Conceptual notes on models for discrete polytomous item responses, Applied Psychological Measurement, 19, 1, pp. 91-100, (1995); 
Molenaar I.W., Nonparametric models for polytomous responses, Handbook of modern item response theory, pp. 369-380, (1997); 
Moors G., Exploring the effect of a middle response category on response style in attitude measurement, Quality & Quantity, 42, 6, pp. 779-794, (2008); 
Muraki E., A generalized partial credit model, Handbook of modern item response theory, pp. 153-164, (1997); 
Muraki E., Muraki M., Generalized partial credit model, Handbook of item response theory, 1, pp. 127-138, (2018); 
Nering M.L., Ostini R., Handbook of polytomous item response theory models, (2010); 
R: A language and environment for statistical computing, (2021); 
Reckase M.D., Multidimensional item response theory, (2009); 
Robitzsch A., Kiefer T., Wu M., TAM: Test analysis modules, (2020); 
Samejima F., Estimation of latent ability using a response pattern of graded scores, Psychometrika Monograph Supplement, 34, pp. 1-97, (1969); 
Seol H., Using the bootstrap method to evaluate the critical range of misfit for polytomous Rasch fit statistics, Psychological Reports, 118, 3, pp. 937-956, (2016); 
Smith R.M., Fit analysis in latent trait models, Introduction to Rasch measurement, pp. 73-92, (2004); 
Van Zile-Tamsen C., Using Rasch analysis to inform rating scale development, Research in Higher Education, 58, 8, pp. 922-933, (2017); 
Walker A.A., Jennings J.K., Engelhard G., Using person response functions to investigate areas of person misfit related to item characteristics, Educational Assessment, 23, 1, pp. 47-68, (2018); 
Waugh R.F., Creating a scale to measure motivation to achieve academically: Linking attitudes and behaviours using Rasch measurement, British Journal of Educational Psychology, 72, 1, pp. 65-86, (2002); 
Wesolowski B.C., Wind S.A., Engelhard G., Examining rater precision in music performance assessment: An analysis of rating scale structure using the Multifaceted Rasch Partial Credit Model, Music Perception, 33, 5, pp. 662-678, (2016); 
Wind S.A., Guo W., Exploring the combined effects of rater misfit and differential rater functioning in performance assessments, Educational and Psychological Measurement, 79, 5, pp. 962-987, (2019); 
Wind S.A., Jones E., The effects of incomplete rating designs in combination with rater effects, Journal of Educational Measurement, 56, 1, pp. 76-100, (2019); 
Wind S.A., Tsai C.-L., Grajeda S.B., Bergin C., Principals’ use of rating scale categories in classroom observations for teacher evaluation, School Effectiveness and School Improvement, 29, 3, pp. 485-510, (2018); 
Wind S.A., Walker A.A., Exploring the correspondence between traditional score resolution methods and person fit indices in rater-mediated writing assessments, Assessing Writing, 39, pp. 25-38, (2019); 
Wolfe E.W., A bootstrap approach to evaluating person and item fit to the Rasch model, Journal of Applied Measurement, 14, 1, pp. 1-9, (2013); 
Wolfe E.W., Jiao H., Song T., A family of rater accuracy models, Journal of Applied Measurement, 16, 2, pp. 153-160, (2014); 
Wright B.D., Masters G.N., Rating scale analysis: Rasch measurement, (1982); 
Wright B.D., Mok M.M.C., An overview of the family of Rasch measurement models, Introduction to Rasch measurement, pp. 1-24, (2004); 
Wu M., Adams R.J., Properties of Rasch residual fit statistics, Journal of Applied Measurement, 14, 4, pp. 339-355, (2013)#FRF#
