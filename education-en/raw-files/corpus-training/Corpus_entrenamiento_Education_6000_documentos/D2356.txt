#ITI#The Impact and Detection of Uniform Differential Item Functioning for Continuous Item Response Models#FTI#
#IRE# Psychometricians have devoted much research and attention to categorical item responses, leading to the development and widespread use of item response theory for the estimation of model parameters and identification of items that do not perform in the same way for examinees from different population subgroups (e.g., differential item functioning [DIF]). With the increasing use of computer-based measurement, use of items with a continuous response modality is becoming more common. Models for use with these items have been developed and refined in recent years, but less attention has been devoted to investigating DIF for these continuous response models (CRMs). Therefore, the purpose of this simulation study was to compare the performance of three potential methods for assessing DIF for CRMs, including regression, the MIMIC model, and factor invariance testing. Study results revealed that the MIMIC model provided a combination of Type I error control and relatively high power for detecting DIF. Implications of these findings are discussed.#FRE#
#IPC# continuous response model; differential item functioning; factor invariance; mimic model#FPC#
#IRF# Baker F.B., Kim S.H., Item response theory: Parameter estimation techniques, (2004); 
Brown T.A., Confirmatory factor analysis for applied research, (2015); 
Chen F.F., Sensitivity of goodness of fit indexes to lack of measurement invariance, Structural Equation Modeling, 14, 3, pp. 464-504, (2007); 
Cheung G.W., Rensvold R.B., Evaluating goodness-of-fit indexes for testing measurement invariance, Structural Equation Modeling, 9, 2, pp. 233-255, (2002); 
Dorans N.J., Holland P.W., DIF detection and description: Mantel-Haenszel and standardization, Differential item functioning, pp. 35-66, (1993); 
Dorans N.J., Kulick E., Demonstrating the utility of the standardization approach to assessing unexpected differential item performance on the Scholastic Aptitude Test, Journal of Educaitonal Measurement, 23, 4, pp. 355-368, (1986); 
Ferrando P.J., Theoretical and empirical comparisons between two models for continuous item responses, Multivariate Behavioral Research, 37, 4, pp. 521-542, (2002); 
Ferrando P.J., Difficulty, discrimination, and information indices in the linear factor analysis model for continuous item responses, Applied Psychological Measurement, 33, 1, pp. 9-24, (2009); 
Ferrando P.J., A general linear framework for modeling continuous responses with error in persons and items, Methodology, 9, pp. 150-161, (2013); 
Ferrando P.J., Navarro-Gonzalez D., A multidimensional item response theory model for continuous and graded responses with error in persons and items, Educational and Psychological Measurement, 81, 6, pp. 1029-1053, (2021); 
Finch H., The MIMIC model as a method for detecting DIF: Comparison with Mantel-Haenszel, SIBTEST, and the IRT likelihood ratio, Applied Psychological Measurement, 29, pp. 278-295, (2005); 
Finch W.H., French B.F., A Simulation Investigation of the Performance of Invariance Assessment using Equivalence Testing Procedures, Structural Equation Modeling, 25, 5, pp. 673-686, (2018); 
Freedle R., Correcting the SATâ€™s ethnic and social-class bias: A method for reestimating SAT scores, Harvard Educational Review, 73, pp. 1-43, (2003); 
French B.F., Finch W.H., Confirmatory factor analytic procedures for the determination of measurement invariance, Structural Equation Modeling, 13, pp. 378-402, (2006); 
Hambleton R.K., Swaminathan H., Item response theory: Principles and applications, (1985); 
Holland P.W., Thayer D.T., Differential item performance and the Mantel-Haenszel procedure, Test validity, pp. 129-145, (1988); 
Jin K.-Y., Chen H.-F., MIMIC approach to assessing differential item functioning with control of extreme response style, Behavior Research Methods, 52, pp. 23-35, (2020); 
Jodoin M.G., Gierl M.J., Evaluating type I error and power rates using an effect size measure with the logistic regression procedure for DIF detection, Applied Measurement in Education, 14, 4, pp. 329-349, (2001); 
Joreskog K.G., Statistical analysis of sets of congeneric tests, Psychometrika, 36, pp. 109-123, (1971); 
Kite B.A., Jorgensen T.D., Chen P.-Y., Random permutation testing applied to measurement invariance testing with ordered-categorical indicators, Structural Equation Modeling: A Multidisciplinary Journal, 25, 4, pp. 573-587, (2018); 
Kline R.B., Principles and practice of structural equation modeling, (2016); 
Linn R.L., Evaluating the validity of assessments: The consequences of use, Educational Measurement Issues and Practice, 16, 2, pp. 14-16, (2005); 
Lord F.M., Applications of item response theory to practical testing problems, (1980); 
Lord F.M., Novick M.R., Statistical theories of mental test scores, (1968); 
Lubke G.H., Muthen B.O., Applying Multigroup Confirmatory Factor Models for Continuous Outcomes to Likert Scale Data Complicates Meaningful Group Comparisons, Structural Equation Modeling, 11, 4, pp. 514-534, (2004); 
Marcoulides K.M., Yuan K.-H., New ways to evaluate goodness of fit: A note on using equivalence testing to assess structural equation models, Structural Equation Modeling: A Multidisciplinary Journal, 24, 1, pp. 1-6, (2016); 
Marcoulides K.M., Yuan K.-H., New ways to evaluate goodness of fit: A note on using equivalence testing to assess structural equation models, Structural Equation Modeling: A multidisciplinary journal, 34, 1, pp. 1-6, (2017); 
McAllister P.H., Testing, DIF, and public policy, Differential item functioning, pp. 389-396, (1993); 
McDonald R.P., Test theory: A unified treatment, (1999); 
Mellenbergh G.J., Models for continuous responses, Handbook of item response theory, pp. 153-163, (2016); 
Meredith W., Measurement invariance, factor analysis and factorial invariance, Psychometrika, 58, pp. 525-543, (1993); 
Messick S., Validity, Educational measurement, pp. 13-103, (1989); 
Michaelides M.P., A review of the effects on IRT item parameter estimates with a focus on misbehaving common items in test equating, Frontiers in Psychology, 1, (2010); 
Millsap R.E., Statistical approaches to measurement invariance, (2011); 
Millsap R.E., Yun-Tein J., Assessing factorial invariance in ordered-categorical measures, Multivariate Behavioral Research, 39, 3, pp. 479-515, (2004); 
Minchen N.D., de la Torre J., Liu Y., A cognitive diagnosis model for continuous response, Journal of Educational and Behavioral Statistics, 42, 6, pp. 651-677, (2017); 
Montoya A.K., Jeon M., MIMIC models for uniform and nonuniform DIF as moderated mediated models, Applied Psychological Measurement, 44, 2, pp. 118-136, (2020); 
Mulaik S.A., Linear causal modeling with structural equations, (2009); 
Muller H., A Rasch model for continuous ratings, Psychometrika, 52, pp. 165-181, (1987); 
Muraki E., A generalized partial credit model: Application of an EM algorithm, ETS Research Report Series, 1992, 1, pp. 1-30, (1992); 
Narayanon P., Swaminathan H., Identification of items that show nonuniform DIF, Applied Psychological Measurement, 20, 3, pp. 257-274, (1996); 
Noel Y., Dauvier B., A Beta item response model for continuous bounded responses, Applied Psychological Measurement, 31, 1, pp. 47-73, (2007); 
Osterlind S.J., Everson H.T., Differential item functioning, (2009); 
Putnick D.L., Bornstein M.H., Measurement invariance conventions and reporting: The state of the art and future directions for psychological research, Developmental Review, 41, pp. 71-90, (2016); 
Raju N.S., van der Linden W.J., Fleer P.F., An IRT-based internal measure of test bias with applications for differential item functioning, Applied Psychological Mea surement, 19, pp. 353-368, (1995); 
Rasch G., Probabilistic model for some intelligence and achievement tests, (1980); 
R: A language and environment for statistical computing, (2020); 
Reise S.P., Widaman K.F., Pugh R.H., Confirmatory factor analysis and item response theory: Two approaches for exploring measurement invariance, Psychological Bulletin, 114, 3, pp. 552-566, (1993); 
Rogers H.J., Swaminathan H., A comparison of logistic regression and Mantel-Haenszel procedures for detecting differential item functioning, Applied Psychological Measurement, 17, 2, pp. 105-116, (1993); 
Roussos L.A., Stout W.F., Simulation studies of the effects of small sample size and studied item parameters on SIBTEST and Mantel-Haenszel type I error performance, Journal of Educational Measurement, 33, 2, pp. 215-230, (1996); 
Rutkowski L., Svetina D., Measurement invariance in international surveys: Categorical indicators and fit measure performance, Applied Measurement in Education, 30, 1, pp. 39-51, (2017); 
Samejima F., Estimation of latent ability using a response pattern of graded scores, (1969); 
Samejima F., Homogeneous case of the continuous response model, Psychometrika, 38, pp. 203-219, (1973); 
Scherbaum C.A., Goldstein H.W., Examining the relationship between race-based differential item functioning and item difficulty, Educational and Psychological Measurement, 68, 4, pp. 537-553, (2008); 
Shealy R., Stout W., A model-based standardization approach that separates true bias/DIF from group ability differences and detects test bias/DTF as well as item bias/DIF, Psychometrika, 58, 2, pp. 159-194, (1993); 
Shojima K., A noniterative item parameter solution in each EM cycle of the continuous response model, Educational Technology Research, 28, pp. 11-22, (2005); 
Swaminathan H., Rogers H.J., Detecting differential item functioning using logistic regression procedures, Journal of Educational Measurement, 27, pp. 361-370, (1990); 
Thissen D., Steinberg L., Wainer H., Use of item response theory in the study of group differences in trace lines, Test validity, pp. 147-169, (1988); 
Wang T., Zeng L., Item parameter estimation for a continuous response model using an EM algorithm, Applied Psychological Measurement, 22, 4, pp. 333-344, (1998); 
Wicherts J.M., Dolan C.V., Measurement invariance in confirmatory factor analysis: An illustration using IQ test performance of minorities, Educational Measurement Issues and Practices, 29, 3, pp. 39-47, (2010); 
Woods C.M., Empirical selection of anchors for tests of differential item functioning, Applied Psychological Measurement, 33, pp. 42-57, (2009); 
Woods C.M., Grimm K.J., Testing for nonuniform differential item functioning with multiple indicator multiple cause models, Applied Psychological Measurement, 35, 5, pp. 339-361, (2011); 
Yuan K.-H., Bentler P.M., On chi-square difference and z tests in mean and covariance structure analysis when the base model is misspecified, Educational and Psychological Measurement, 64, 5, pp. 737-757, (2004); 
Yuan K.-H., Chan W., Marcoulides G.A., Bentler P.M., Assessing structural equation models by equivalence testing with adjusted fit indexes, Structural Equation Modeling: A Multidisciplinary Journal, 23, 3, pp. 319-330, (2016); 
Zopluoglu C., EstCRM: An R package for Samejimaâ€™s continuous IRT model, Applied Psychological Meausrement, 36, 2, pp. 149-150, (2012); 
Zopluoglu C., A finite mixture item response theory model for continuous measurement outcomes, 80, 2, pp. 346-364, (2020); 
Zumbo B.D., A handbook on the theory and methods of differential item functioning (DIF): Logistic regression modeling as a unitary framework for Binary and Likert-Type (Ordinal) Item Scores, (1999)#FRF#
