#ITI#Fixed Effects or Mixed Effects Classifiers? Evidence From Simulated and Archival Data#FTI#
#IRE# This study seeks to compare fixed and mixed effects models for the purposes of predictive classification in the presence of multilevel data. The first part of the study utilizes a Monte Carlo simulation to compare fixed and mixed effects logistic regression and random forests. An applied examination of the prediction of student retention in the public-use U.S. PISA data set was considered to verify the simulation findings. Results of this study indicate fixed effects models performed comparably with mixed effects models across both the simulation and PISA examinations. Results broadly suggest that researchers should be cognizant of the type of predictors and data structure being used, as these factors carried more weight than did the model type.#FRE#
#IPC# mixed effects models; Monte Carlo simulation; predictive classification; Program for International Student Assessment#FPC#
#IRF# Abu-Nimeh S., Nappa D., Wang X., Nair S., A comparison of machine learning techniques for phishing detection, Proceedings of the Anti-Phishing Working Groups 2nd Annual eCrime Researchers Summit (eCrime ‘07), pp. 60-69, (2007); 
Bauer E., Kohavi R., An empirical comparison of voting classification algorithms: Bagging, boosting, and variants, Machine Learning, 36, 1-2, pp. 105-139, (1999); 
Beleites C., Neugebauer U., Bocklitz T., Krafft C., Popp J., Sample size planning for classification models, Analytica Chimica Acta, 760, pp. 25-33, (2012); 
Bolin J., Finch W., Supervised classification in the presence of misclassified training data: A Monte Carlo simulation study in the three group case, Frontiers in Psychology, 5, (2014); 
Breiman L., Random forests, Machine Learning, 45, 1, pp. 5-32, (2001); 
Capitaine L., Genuer R., Thiebaut R., Random forests for high-dimensional longitudinal data, Statistical Methods in Medical Research, 30, 1, pp. 166-184, (2021); 
Choi A., Gil M., Mediavilla M., Valbuena J., Predictors and effects of grade repetition, Revista De Economía Mundial, 48, pp. 21-42, (2018); 
Corman H., The effects of state policies, individual characteristics, family characteristics, and neighbourhood characteristics on grade repetition in the United States, Economics of Education Review, 22, 4, pp. 409-420, (2003); 
Crane-Droesch A., Semiparametric panel data models using neural networks, arXiv preprint arXiv, (2017); 
Dietterich T.G., An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization, Machine Learning, 40, 2, pp. 139-157, (2000); 
Downes M., Carlin J.B., Multilevel regression and poststratification as a modeling approach for estimating population quantities in large population health studies: A simulation study, Biometrical Journal, 62, 2, pp. 479-491, (2020); 
Eisemon T.O., Reducing repetition: Issues and strategies, (1997); 
Fawcett T., An introduction to ROC analysis, Pattern Recognition Letters, 27, 8, pp. 861-874, (2006); 
Figueroa R.L., Zeng-Treitler Q., Kandula S., Ngo L.H., Predicting sample size required for classification performance, BMC Medical Informatics and Decision Making, 12, 1, (2012); 
Glick P., Sahn D.E., Early academic performance, grade repetition, and school attainment in senegal: A panel data analysis, The World Bank Economic Review, 24, 1, pp. 93-120, (2010); 
Hajjem A., Bellavance F., Larocque D., Mixed-effects random forest for clustered data, Journal of Statistical Computation and Simulation, 84, 6, pp. 1313-1328, (2014); 
Harrison R.L., Introduction to Monte Carlo simulation, 1204, 1, pp. 17-21, (2010); 
Hastie T., Tibshirani R., Friedman J., The elements of statistical learning: Data mining, inference, and prediction, (2009); 
Ikeda M., Garcia E., Grade repetition: A comparative study of academic and non-academic consequences, OECD Journal: Economic Studies, 2013, 1, pp. 269-315, (2014); 
James G., Witten D., Hastie T., Tibshirani R., An introduction to statistical learning, 112, (2013); 
Karpievitch Y.V., Hill E.G., Leclerc A.P., Dabney A.R., Almeida J.S., An introspective comparison of random forest-based classifiers for the analysis of cluster-correlated data by way of RF, PLOS ONE, 4, 9, (2009); 
Kilham P., Hartebrodt C., Kandler G., Generating tree-level harvest predictions from forest inventories with random forests, Forests, 10, 1, (2019); 
Kreft I.G., Are multilevel techniques necessary? An overview, including simulation studies, (1996); 
Lavery M.R., Acharya P., Sivo S.A., Xu L., Number of predictors and multicollinearity: What are their effects on error and bias in regression?, Communications in Statistics-Simulation and Computation, 48, 1, pp. 27-38, (2019); 
Lee J., Stankov L., Non-cognitive predictors of academic achievement: Evidence from TIMSS and PISA, Learning and Individual Differences, 65, pp. 50-64, (2018); 
Lei P.W., Koehly L.M., Linear discriminant analysis versus logistic regression: A comparison of classification errors in the two-group case, The Journal of Experimental Education, 72, 1, pp. 25-49, (2003); 
Little R.J., In praise of simplicity not mathematistry! Ten simple powerful ideas for the statistical scientist, Journal of the American Statistical Association, 108, 502, pp. 359-369, (2013); 
Luke D.A., Multilevel modeling, 143, (2019); 
Maas C.J.M., Hox J.J., Sufficient sample sizes for multilevel modeling, Methodology, 1, 3, pp. 86-92, (2005); 
Mangino A.A., Finch W.H., Prediction with mixed effects models: A Monte Carlo simulation study, Educational and Psychological Measurement, 81, 6, pp. 1118-1142, (2021); 
Maroco J., Silva D., Rodrigues A., Guerreiro M., Santana I., de Mendonca A., Data mining methods in the prediction of dementia: A real-data comparison of the accuracy, sensitivity and specificity of linear discriminant analysis, logistic regression, neural networks, support vector machines, classification trees and random forests, BMC Research Notes, 4, 1, pp. 299-313, (2011); 
McMahon S.D., Parnes A.L., Keys C.B., Viola J.J., School belonging among low-income urban youth with disabilities: Testing a theoretical model, Psychology in the Schools, 45, 5, pp. 387-401, (2008); 
McNeish D., Kelley K., Fixed effects models versus mixed effects models for clustered data: Reviewing the approaches, disentangling the differences, and making recommendations, Psychological Methods, 24, 1, pp. 20-35, (2019); 
McNeish D., Stapleton L.M., Modeling clustered data with very few clusters, Multivariate Behavioral Research, 51, 4, pp. 495-518, (2016); 
McNeish D., Stapleton L.M., Silverman R.D., On the unnecessary ubiquity of hierarchical linear modeling, Psychological Methods, 22, 1, pp. 114-140, (2017); 
Muchlinski D., Siroky D., He J., Kocher M., Comparing random forest with logistic regression for predicting class-imbalanced civil war onset data, Political Analysis, 24, 1, pp. 87-103, (2016); 
Murtaugh P.A., Simplicity and complexity in ecological data analysis, Ecology, 88, 1, pp. 56-62, (2007); 
Ngufor C., Vira: Virtual Intelligent Robot Assistant, (2019); 
Ngufor C., Van Houten H., Caffo B.S., Shah N.D., McCoy R.G., Mixed effect machine learning: A framework for predicting longitudinal change in hemoglobin A1c, Journal of Biomedical Informatics, 89, pp. 56-67, (2019); 
Paccagnella O., Sample size and accuracy of estimates in multilevel models: New simulation results, Methodology, 7, 3, pp. 111-120, (2011); 
Palvanov A., Cho Y.I., Comparisons of deep learning algorithms for MNIST in real-time environment, International Journal of Fuzzy Logic and Intelligent Systems, 18, 2, pp. 126-134, (2018); 
Ramos D., Franco-Pedroso J., Lozano-Diez A., Gonzalez-Rodriguez J., Ramos D., Gonzalez-Rodriguez J., Franco-Pedroso J., Lozano-Diez A., Deconstructing cross-entropy for probabilistic binary classifiers, Entropy, 20, 3, (2018); 
Raudenbush S.W., Bryk A.S., Hierarchical linear models: Applications and data analysis methods, 1, (2002); 
Raudys S.J., Jain A.K., Small sample size effects in statistical pattern recognition: Recommendations for practitioners, IEEE Transactions on Pattern Analysis and Machine Intelligence, 13, 3, pp. 252-264, (1991); 
R: A language and environment for statistical computing, (2020); 
Shah A.D., Bartlett J.W., Carpenter J., Nicholas O., Hemingway H., Comparison of random forest and parametric imputation models for imputing missing data using MICE: A CALIBER study, American Journal of Epidemiology, 179, 6, pp. 764-774, (2014); 
Speiser J.L., Wolf B.J., Chung D., Karvellas C.J., Koch D.G., Durkalski V.L., BiMM forest: A random forest method for modeling clustered and longitudinal binary outcomes, Chemometrics and Intelligent Laboratory Systems, 185, pp. 122-134, (2019); 
Speiser J.L., Wolf B.J., Chung D., Karvellas C.J., Koch D.G., Durkalski V.L., BiMM tree: A decision tree method for modeling clustered and longitudinal binary outcomes, Communications in Statistics-Simulation and Computation, 49, 4, pp. 1004-1023, (2020); 
Steyerberg E.W., Clinical prediction models, (2019); 
Strobl C., Malley J., Tutz G., An introduction to recursive partitioning: Rationale, application, and characteristics of classification and regression trees, bagging, and random forests, Psychological Methods, 14, 4, pp. 323-348, (2009); 
VanDerHeyden A.M., Universal screening may not be for everyone: Using a threshold model as a smarter way to determine risk, School Psychology Review, 42, 4, pp. 402-414, (2013); 
Waljee A.K., Mukherjee A., Singal A.G., Zhang Y., Warren J., Balis U., Marrero J., Zhu J., Higgins P.D., Comparison of imputation methods for missing laboratory data in medicine, BMJ Open, 3, 8, (2013); 
Westreich D., Lessler J., Funk M.J., Propensity score estimation: Machine learning and classification methods as alternatives to logistic regression, Journal of Clinical Epidemiology, 63, 8, pp. 826-833, (2010); 
Wossmann L., Schooling resources, educational institutions and student performance: The international evidence, Oxford Bulletin of Economics and Statistics, 65, 2, pp. 117-170, (2003); 
Wu M., Zhang Z., Handwritten digit classification using the MNIST data set, (2010); 
Yan P., Anomaly detection in categorical data with interpretable machine learning: A random forest approach to classify imbalanced data, (2019); 
Zellner A., Keuzenkamp H.A., McAleer M., Simplicity, inference and modelling: Keeping it sophisticatedly simple, (2001); 
Zhang J.L., Haerdle W.K., The Bayesian additive classification tree applied to credit risk modelling, Computational Statistics & Data Analysis, 54, 5, pp. 1197-1205, (2010); 
Zigler E., Phillips L., Psychiatric diagnosis: A critique, Journal of Abnormal and Social Psychology, 63, 3, pp. 607-618, (1961)#FRF#
