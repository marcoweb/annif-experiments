#ITI#Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models#FTI#
#IRE# This study investigates the potential of automated classification using prompt-based learning approaches with transformer models (large language models trained in an unsupervised manner) for a domain-specific classification task. Prompt-based learning with zero or few shots has the potential to (1) make use of artificial intelligence without sophisticated programming skills and (2) make use of artificial intelligence without fine-tuning models with large amounts of labeled training data. We apply this novel method to perform an experiment using so-called zero-shot classification as a baseline model and a few-shot approach for classification. For comparison, we also fine-tune a language model on the given classification task and conducted a second independent human rating to compare it with the given human ratings from the original study. The used dataset consists of 2,088 email responses to a domain-specific problem-solving task that were manually labeled for their professional communication style. With the novel prompt-based learning approach, we achieved a Cohen’s kappa of.40, while the fine-tuning approach yields a kappa of.59, and the new human rating achieved a kappa of.58 with the original human ratings. However, the classifications from the machine learning models have the advantage that each prediction is provided with a reliability estimate allowing us to identify responses that are difficult to score. We, therefore, argue that response ratings should be based on a reciprocal workflow of machine raters and human raters, where the machine rates easy-to-classify responses and the human raters focus and agree on the responses that are difficult to classify. Further, we believe that this new, more intuitive, prompt-based learning approach will enable more people to use artificial intelligence#FRE#
#IPC# Artificial intelligence in education; classification; machine learning; natural language processing; prompt-based learning; transformer-based language models#FPC#
#IRF# Abid A., Farooqi M., Zou J., Persistent anti-Muslim bias in large language models, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 298-306, (2021); 
Attali Y., Burstein J., Automated essay scoring with e-rater® V.2, The Journal of Technology, Learning and Assessment, 4, 3, (2006); 
Beseiso M., Alzubi O.A., Rashaideh H., A novel automated essay scoring approach for reliable higher educational assessments, Journal of Computing in Higher Education, 33, 3, pp. 727-746, (2021); 
Becker A., Post-collection data capture, scoring, and processing, Reliability and validity of international large-scale assessment, 10, pp. 151-167, (2020); 
Bin L., Jian-Min Y., Automated essay scoring using multi-classifier fusion, Computing and intelligent systems, 233, pp. 151-157, (2011); 
pp. 2104-2119, (2018); 
Brandt S., Rausch A., Kogler K., A scoring procedure for complex assessments focusing on validity and appropriate reliability, (2016); 
Brown T.B., Mann B., Ryder N., Subbiah M., Kaplan J., Dhariwal P., Neelakantan A., Shyam P., Sastry G., Askell A., Agarwal S., Herbert-Voss A., Krueger G., Henighan T., Child R., Ramesh A., Ziegler D.M., Wu J., Winter C., Amodei D., Language models are few-shot learners, (2020); 
Chan B., Schweter S., Moller T., German’s next language model, (2020); 
Cohen J., Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit, Psychological Bulletin, 70, 4, pp. 213-220, (1968); 
Devlin J., Chang M.-W., Lee K., Toutanova K., BERT: Pre-training of deep bidirectional transformers for language understanding, (2018); 
Gao L., Biderman S., Black S., Golding L., Hoppe T., Foster C., Phang J., He H., Thite A., Nabeshima N., The pile: An 800gb dataset of diverse text for language modeling, (2020); 
Gao T., Fisch A., Chen D., Making pre-trained language models better few-shot learners, (2021); 
Gillioz A., Casas J., Mugellini E., Khaled O.A., Overview of the transformer-based models for NLP tasks, pp. 179-183, (2020); 
Guan C., Mou J., Jiang Z., Artificial intelligence innovation in education: A twenty-year data-driven historical analysis, International Journal of Innovation Studies, 4, 4, pp. 134-147, (2020); 
Han K., Wang Y., Chen H., Chen X., Guo J., Liu Z., Tang Y., Xiao A., Xu C., Xu Y., Yang Z., Zhang Y., Tao D., A survey on vision transformer, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, (2022); 
Hochreiter S., The vanishing gradient problem during learning recurrent neural nets and problem solutions, International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6, 2, pp. 107-116, (1998); 
Hochreiter S., Schmidhuber J., Long short-term memory, Neural Computation, 9, 8, pp. 1735-1780, (1997); 
Ifenthaler D., AKOVIA: Automated knowledge visualization and assessment, Technology, Knowledge and Learning, 19, 1-2, pp. 241-248, (2014); 
Ifenthaler D., Automated essay grading systems, Handbook of open, distance and digital education, pp. 1-15, (2022); 
Ifenthaler D., Schumacher C., Student perceptions of privacy principles for learning analytics, Educational Technology Research and Development, 64, 5, pp. 923-938, (2016); 
Ifenthaler D., Tracey M.W., Exploring the relationship of ethics and privacy in learning analytics and design: Implications for the field of educational technology, Educational Technology Research and Development, 64, 5, pp. 877-880, (2016); 
Jiang Z., Xu F.F., Araki J., Neubig G., How can we know what language models know?, Transactions of the Association for Computational Linguistics, 8, pp. 423-438, (2020); 
Joshi M., Choi E., Weld D.S., Zettlemoyer L., TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension, ArXiv:1705.03551 [Cs, (2017); 
Kaplan J., McCandlish S., Henighan T., Brown T.B., Chess B., Child R., Gray S., Radford A., Wu J., Amodei D., Scaling laws for neural language models, (2020); 
Ke Z., Ng V., Automated essay scoring: A survey of the state of the art, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), (2019); 
Krippendorff K., Reliability in content analysis: Some common misconceptions and recommendations, Human Communication Research, 30, 3, pp. 411-433, (2004); 
Landis J.R., Koch G.G., The measurement of observer agreement for categorical data, Biometrics, 33, 1, pp. 159-174, (1977); 
Leahy C., (2022); 
Lester B., Al-Rfou R., Constant N., The power of scale for parameter-efficient prompt tuning, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, (2021); 
Lewis M., Liu Y., Goyal N., Ghazvininejad M., Mohamed A., Levy O., Stoyanov V., Zettlemoyer L., Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, (2019); 
Liang G., On B.-W., Jeong D., Kim H.-C., Choi G., Automated essay scoring: A Siamese bidirectional LSTM neural network architecture, Symmetry, 10, 12, (2018); 
Lind Pantzare A., Interrater reliability in large-scale assessments–Can teachers score national tests reliably without external controls?, Practical Assessment, Research, and Evaluation, 20, 1, (2015); 
Liu P., Yuan W., Fu J., Jiang Z., Hayashi H., Neubig G., Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, (2021); 
Liu X., Ji K., Fu Y., Du Z., Yang Z., Tang J., P-Tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks, (2021); 
Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer L., Stoyanov V., RoBERTa: A robustly optimized BERT pretraining approach, (2019); 
Lucy L., Bamman D., Gender and representation bias in GPT-3 generated stories, Proceedings of the Third Workshop on Narrative Understanding, pp. 48-55, (2021); 
Ludwig S., Mayer C., Hansen C., Eilers K., Brandt S., Automated essay scoring using transformer models, Psych, 3, 4, pp. 897-915, (2021); 
Ndukwe I.G., Daniel B.K., Amadi C.E., A machine learning grading system using chatbots, Artificial intelligence in education, 11626, pp. 365-368, (2019); 
Paperno D., Kruszewski G., Lazaridou A., Pham Q.N., Bernardi R., Pezzelle S., Baroni M., Boleda G., Fernandez R., The LAMBADA dataset: Word prediction requiring a broad discourse context, (2016); 
Pethig F., Kroenung J., Biased Humans, (Un)Biased Algorithms?, Journal of Business Ethics, (2022); 
Petroni F., Rocktaschel T., Lewis P., Bakhtin A., Wu Y., Miller A.H., Riedel S., Language models as knowledge bases?, (2019); 
Popenici S.A.D., Kerr S., Exploring the impact of artificial intelligence on teaching and learning in higher education, Research and Practice in Technology Enhanced Learning, 12, 1, (2017); 
Puri R., Catanzaro B., Zero-shot text classification with generative language models, (2019); 
Pushp P.K., Srivastava M.M., Train once, test anywhere: Zero-shot learning for text classification, (2017); 
Qiu X., Sun T., Xu Y., Shao Y., Dai N., Huang X., Pre-trained models for natural language processing: A survey, Science China Technological Sciences, 63, 10, pp. 1872-1897, (2020); 
Radford A., Narasimhan K., Salimans T., Sutskever I., Improving language understanding by generative pre-training, (2018); 
Radford A., Wu J., Child R., Luan D., Amodei D., Sutskever I., Language models are unsupervised multitask learners, OpenAI Blog, (2019); 
Raffel C., Shazeer N., Roberts A., Lee K., Narang S., Matena M., Zhou Y., Li W., Liu P.J., Exploring the limits of transfer learning with a unified text-to-text transformer, (2019); 
Ramineni C., Trapani C.S., Williamson D.M., Davey T., Bridgeman B., Evaluation of the e-rate® scoring engine for the TOEFL® independent and integrated prompts, ETS Research Report Series, 2012, 1, (2012); 
Rausch A., Seifried J., Wuttke E., Kogler K., Brandt S., Reliability and validity of a computer-based assessment of cognitive and non-cognitive facets of problem-solving competence in the business domain, Empirical Research in Vocational Education and Training, 8, 1, (2016); 
Rausch A., Wuttke E., Development of a multi-faceted model of domain-specific problem-solving competence and its acceptance by different stakeholders in the business domain, Unterrichtswissenschaft, 44, pp. 169-184, (2016); 
Rokach L., Ensemble-based classifiers, Artificial Intelligence Review, 33, pp. 1-39, (2010); 
Romera-Paredes B., Torr P., An embarrassingly simple approach to zero-shot learning, Proceedings of the 32nd International Conference on Machine Learning, pp. 2152-2161, (2015); 
Sainz O., Rigau G., Ask2Transformers: Zero-shot domain labelling with pretrained language models, Proceedings of the 11th Global Wordnet Conference, pp. 44-52, (2021); 
Sanh V., Debut L., Chaumond J., Wolf T., DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, (2019); 
Says U., (2021); 
Scao T.L., Rush A.M., How many data points is a prompt worth?, arXiv Preprint Arxiv:2103.08493, (2021); 
Schick T., Schutze H., Exploiting cloze questions for few shot text classification and natural language inference, (2021); 
Schick T., Schutze H., It’s not just size that matters: Small language models are also few-shot learners, (2021); 
Seifried J., Brandt S., Kogler K., Rausch A., The computer-based assessment of domain-specific problem-solving competence—A three-step scoring procedure, Cogent Education, 7, 1, (2020); 
Sembill D., Rausch A., Wuttke E., Seifried J., Wolf K.D., Martens T., Brandt S., (2016); 
Shermis M.D., Burstein J., Handbook on automated essay evaluation: Current applications and new directions, (2013); 
Shin T., Razeghi Y., Logan R.L., Wallace E., Singh S., AutoPrompt: Eliciting knowledge from language models with automatically generated prompts, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222-4235, (2020); 
Silva A., Tambwekar P., Gombolay M., Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2383-2389, (2021); 
Snell J., Swersky K., Zemel R., Prototypical networks for few-shot learning, (2017); 
Situational Awareness for Low Resource Languages: The LORELEI Situation Frame Annotation Task, pp. 32-41, (2017); 
Taghipour K., Ng H.T., A neural approach to automated essay scoring, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1882-1891, (2016); 
Thorne J., Vlachos A., Christodoulopoulos C., Mittal A., FEVER: A large-scale dataset for Fact Extraction and VERification, ArXiv:1803.05355 [Cs, (2018); 
Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need, pp. 5998-6008, (2017); 
Wang A., Singh A., Michael J., Hill F., Levy O., Bowman S.R., GLUE: A multi-task benchmark and analysis platform for natural language understanding. ArXiv:1804.07461 [Cs]. http://arxiv.org/abs/1804.07461; 
Wang B., Komatsuzaki A., (2021); 
Williams A., Nangia N., Bowman S.R., A broad-coverage challenge corpus for sentence understanding through inference; 
Wu Y., Schuster M., Chen Z., Le Q.V., Norouzi M., Macherey W., Krikun M., Cao Y., Gao Q., Macherey K., Klingner J., Shah A., Johnson M., Liu X., Kaiser L., Gouws S., Kato Y., Kudo T., Kazawa H., Dean J., Google’s neural machine translation system: Bridging the gap between human and machine translation, (2016); 
Yamamoto M., Umemura N., Kawano H., Automated essay scoring system based on rubric, Applied computing & information technology, 727, pp. 177-190, (2018); 
Yang X., Zhang L., Yu S., Can short answers to open response questions be auto-graded without a grading rubric, Artificial intelligence in education, 10331, pp. 594-597, (2017); 
Yang Z., Dai Z., Yang Y., Carbonell J., Salakhutdinov R.R., Le Q.V., Xlnet: Generalized autoregressive pretraining for language understanding, (2019); 
Yin W., Hay J., Roth D., Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3912-3921, (2019); 
Zhang X., Zhao J., LeCun Y., Character-level convolutional networks for text classiﬁcation, ArXiv:1502.01710; 
Zhong R., Lee K., Zhang Z., Klein D., Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections, (2021); 
Zou K.H., O'Malley A.J., Mauri L., Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models, Circulation, 115, 5, pp. 654-657, (2007)#FRF#
