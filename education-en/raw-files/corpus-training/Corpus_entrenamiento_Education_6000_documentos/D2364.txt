#ITI#Relative Robustness of CDMs and (M)IRT in Measuring Growth in Latent Skills#FTI#
#IRE# Previous studies have demonstrated evidence of latent skill continuity even in tests intentionally designed for measurement of binary skills. In addition, the assumption of binary skills when continuity is present has been shown to potentially create a lack of invariance in item and latent ability parameters that may undermine applications. In this article, we examine measurement of growth as one such application, and consider multidimensional item response theory (MIRT) as a competing alternative. Motivated by prior findings concerning the effects of skill continuity, we study the relative robustness of cognitive diagnostic models (CDMs) and (M)IRT models in the measurement of growth under both binary and continuous latent skill distributions. We find CDMs to be a less robust way of quantifying growth under misspecification, and subsequently provide a real-data example suggesting underestimation of growth as a likely consequence. It is suggested that researchers should regularly attend to the assumptions associated with the use of latent binary skills and consider (M)IRT as a potentially more robust alternative if unsure of their discrete nature.#FRE#
#IPC# (multidimensional) item response theory; cognitive diagnosis models; growth insensitivity; relative robustness#FPC#
#IRF# Akaike H., A new look at the statistical model identification, IEEE Transactions on Automatic Control, 19, pp. 716-723, (1974); 
Bolt D.M., Bifactor MIRT as an Appealing and related alternative to CDMs in the presence of skill attribute continuity, Handbook of diagnostic classification models, pp. 395-420, (2019); 
Bolt D.M., Kim J.S., Parameter invariance and skill attribute continuity in the DINA model, Journal of Educational Measurement, 55, 2, pp. 264-280, (2018); 
Chalmers R.P., mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Chen Y., Culpepper S.A., Wang S., Douglas J., A hidden Markov model for learning trajectories in cognitive diagnosis with application to spatial rotation skills, Applied Psychological Measurement, 42, 1, pp. 5-23, (2018); 
Hansen M., Cai L., Monroe S., Li Z., Limited-information goodness-of-fit testing of diagnostic classification item response models, British Journal of Mathematical and Statistical Psychology, 69, 3, pp. 225-252, (2016); 
Huang H.Y., Multilevel cognitive diagnosis models for assessing changes in latent attributes, Journal of Educational Measurement, 54, 4, pp. 440-480, (2017); 
Huang Q., Bolt D.M., The potential for interpretational confounding in discrete skills models, (2020); 
Huang Q., Bolt D.M., The potential for interpretational confounding in cognitive diagnosis models, Applied Psychological Measurement, 46, 4, pp. 303-320, (2022); 
Kaya Y., Leite W.L., Assessing change in latent skills across time with longitudinal cognitive diagnosis modeling: An evaluation of model performance, Educational and Psychological Measurement, 77, pp. 369-388, (2016); 
Li F., Cohen A., Bottge B., Templin J., A latent transition analysis model for assessing change in cognitive skills, Educational and Psychological Measurement, 76, pp. 181-204, (2015); 
Liu Y., Tian W., Xin T., An application of M2 statistic to evaluate the fit of cognitive diagnostic models, Journal of Educational and Behavioral Statistics, 41, pp. 3-26, (2016); 
Ma W., Minchen N., de la Torre J., Choosing between CDM and unidimensional IRT: The proportional reasoning test case, Measurement: Interdisciplinary Research and Perspectives, 18, 2, pp. 87-96, (2020); 
Madison M.J., Bradshaw L.P., Assessing growth in a diagnostic classification model framework, Psychometrika, 83, 4, pp. 963-990, (2018); 
Madison M.J., Bradshaw L.P., The effect of Q-matrix design on classification accuracy in the log-linear cognitive diagnosis model, Educational and Psychological Measurement, 75, 3, pp. 491-511, (2015); 
Maydeu-Olivares A., Goodness-of-fit assessment of item response theory models, Measurement: Interdisciplinary Research and Perspectives, 11, pp. 71-101, (2013); 
Reckase M.D., The difficulty of test items that measure more than one ability, Applied Psychological Measurement, 21, pp. 25-36, (1985); 
Reckase M.D., A linear logistic multidimensional model, Handbook of modern item response theory, pp. 271-296, (1997); 
Rizopoulos D., ltm: An R package for latent variables modeling and item response theory analyses, Journal of Statistical Software, 17, 5, pp. 1-25, (2006); 
Schwarz G., Estimating the dimension of a model, The Annals of Statistics, 6, pp. 461-464, (1978); 
Tatsuoka K., Analysis of errors in fraction addition and subtraction problems, (1984); 
Templin J., Bradshaw L., The use and misuse of psychometric models, Psychometrika, 79, 2, pp. 347-354, (2014); 
von Davier M., Haberman S.J., Hierarchical diagnostic classification models morphing into unidimensional “diagnostic” classification models—A commentary, Psychometrika, 79, 2, pp. 340-346, (2014); 
Wang S., Yang Y., Culpepper S.A., Douglas J.A., Tracking skill acquisition with cognitive diagnosis models: A higher-order, hidden Markov model with covariates, Journal of Educational and Behavioral Statistics, 43, 1, pp. 57-87, (2018); 
Wang S., Zhang S., Douglas K., Culpepper S., Using Response Times to Assess learning progress: A joint mode for responses and response times, Measurement: Interdisciplinary Research and Perspectives, 16, 1, pp. 45-58, (2018); 
Yoon S.Y., Psychometric properties of the revised Purdue Spatial Visualization Test: Visualization of rotations (the revised PSVT-R), (2011); 
Zhan P., Jiao H., Liao D., Li F., A longitudinal higher-order diagnostic classification model, Journal of Educational and Behavioral Statistics, 44, 3, pp. 251-281, (2019); 
Zhang S., Chang H.-H., A multilevel logistic hidden Markov model for learning under cognitive diagnosis, Behavior Research Methods, 52, 1, pp. 408-421, (2019)#FRF#
