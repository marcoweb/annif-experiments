#ITI#Still a fallible tool? Revisiting effects of automated writing evaluation from activity theory perspective#FTI#
#IRE# Despite the popularity of automated writing evaluation (AWE) that has provoked an increased scholarly interest, synthesized research to comprehensively understand its pedagogical effects is still in paucity. To fill the gap, this study aims to meta-analyse the overall effect of AWE on learners' writing skill development and whether the effect differs from how it has been used. From activity theory (AT) perspective, this study reported results based on a meta-analysis of 33 valid effect sizes involving 2995 participants from 25 experimental and quasi-experimental studies published during 2000–22. The results of the study showed that the overall effect was g = 0.429, 95% CI [0.266, 0.592], suggesting that AWE for writing skill development is more effective than traditional methods, such as traditional classroom interaction, teacher-led interaction and peer interaction. Regarding AT-related moderators for the overall effect, different moderator effects of educational levels, proficiency levels, sample size types, target language types, intervention durations, text genre types, human–computer interaction types, feedback types and program types were reported. Informed by the results obtained, this study also provided some insights into the pedagogical implications for practice. Practitioner notes What is already known about this topic The facilitative effects and educational affordances of automated writing evaluation (AWE) for writing skill development have been well-documented. Existing studies have been helpful in affording insights into the use of AWE for writing skill development. Few studies to date published have directly and quantitatively meta-analyse the use of AWE for writing skill development. It is difficult to understand the effect size and potential AT-related moderators of AWE for writing skill development. What this paper adds A thorough AT-driven meta-analysis of the publications on AWE for writing skill development was conducted. AWE is more effective than traditional methods for writing skill development with an effect size of g = 0.429. The pedagogical effect of AWE is affected by such moderators as educational levels, intervention durations, human–computer interaction types and feedback types. Implications for practice and/or policy Pedagogical potentials of AWE should be encouraged to explore among learners of different educational levels. When using AWE, a combination of human interaction with AWE is necessary. Researchers should understand how such factors as learner individual differences, writing assignment, AWE program and research design may impact the pedagogical effects of AWE#FRE#
#IPC# activity theory (AT); automated writing evaluation (AWE); evidence-based applied linguistics; meta-analysis; writing skill#FPC#
#IRF# Barrot J.S., Using automated written corrective feedback in the writing classrooms: Effects on L2 writing accuracy, Computer Assisted Language Learning, pp. 1-24, (2021); 
Biernacki P., Waldorf D., Snowball sampling: Problems and techniques of chain referral sampling, Sociological Methods & Research, 10, 2, pp. 141-163, (1981); 
Borenstein M., Hedges L.V., Higgins J.P.T., Rothstein H.R., Comprehensive meta-analysis version 2.0 (Computer software), (2005); 
Borenstein M., Hedges L.V., Higgins J.P.T., Rothstein H.R., Introduction to meta-analysis, (2009); 
Chen M.H., Huang S.T., Chang J.S., Liou H.C., Developing a corpus-based paraphrase tool to improve EFL learners' writing skills, Computer Assisted Language Learning, 28, 1, pp. 22-40, (2015); 
Cheng G., The impact of online automated feedback on students' reflective journal writing in an EFL course, The Internet and Higher Education, 34, pp. 18-27, (2017); 
Cheng G., Exploring the effects of automated tracking of student responses to teacher feedback in draft revision: Evidence from an undergraduate EFL writing course, Interactive Learning Environments, 30, 2, pp. 353-375, (2022); 
Cotos E., Genre-based automated writing evaluation for L2 research writing from design to evaluation and enhancement, (2014); 
Deeva G., Bogdanova D., Serral E., Snoeck M., De Weerdt J., A review of automated feedback systems for learners: Classification framework, challenges and opportunities, Computers & Education, 162, pp. 1-43, (2021); 
Engestrom Y., Expansive learning at work: Toward an activity-theoretical reconceptualization, Journal of Education and Work, 14, 1, pp. 133-156, (2001); 
Frost K.L., The effects of automated essay scoring as a high school classroom intervention, (2009); 
Fu Q., Zou D., Xie H., Cheng G., A review of AWE feedback: Types, learning outcomes, and implications, Computer Assisted Language Learning, pp. 1-43, (2022); 
Gao J., Ma S., The effect of two forms of computer-automated metalinguistic corrective feedback, Language Learning & Technology, 23, 2, pp. 65-83, (2019); 
Grimes D., Warschauer M., Utility in a fallible tool: A multi-site case study of automated writing evaluation, Journal of Technology, Learning, and Assessment, 8, 6, pp. 1-43, (2010); 
Han T., Sari E., An investigation on the use of automated feedback in Turkish EFL students' writing classes, Computer Assisted Language Learning, pp. 1-25, (2022); 
Han Y., Zhao S., Ng L., How technology tools impact writing performance, lexical complexity, and perceived self-regulated learning strategies in EFL academic writing: A comparative study, Frontiers in Psychology, 12, pp. 1-18, (2021); 
Hassanzadeh M., Fotoohnejad S., Implementing an automated feedback program for a foreign language writing course: A learner-centric study, Journal of Computer Assisted Learning, 37, 5, pp. 1494-1507, (2021); 
Holman L.D., Automated writing evaluation program's effect on student writing achievement, (2011); 
Huang S., Renandya W.A., Exploring the integration of automated feedback among lower-proficiency EFL learners, Innovation in Language Learning and Teaching, 14, 1, pp. 15-26, (2020); 
Hwang G.J., Fu Q.K., Trends in the research design and application of mobile language learning: A review of 2007-2016 publications in selected SSCI journals, Interactive Learning Environments, 27, 4, pp. 567-581, (2019); 
Kang E., Han Z., The efficacy of written corrective feedback in improving L2 written accuracy: A meta-analysis, The Modern Language Journal, 99, 1, pp. 1-18, (2015); 
Lai Y., Which do students prefer to evaluate their essays: Peers or computer program, British Journal of Educational Technology, 41, 3, pp. 432-454, (2010); 
Lee C., Wong K.C.K., Cheung W.K., Lee F.S.L., Web-based essay critiquing system and EFL students' writing: A quantitative and qualitative investigation, Computer Assisted Language Learning, 22, 1, pp. 57-72, (2009); 
Li R., Modeling the continuance intention to use automated writing evaluation, SAGE Open, 11, 4, pp. 1-13, (2021); 
Li R., Effects of mobile-assisted language learning on EFL/ESL reading comprehension, Educational Technology & Society, 25, 3, pp. 15-29, (2022); 
Li R., Effects of blended language learning on EFL learners' language performance: An activity theory approach, Journal of Computer Assisted Learning, 38, pp. 1273-1285, (2022); 
Li R., Research trends of blended language learning: A bibliometric synthesis of SSCI-indexed journal articles during 2000–2019, ReCALL, 34, 3, pp. 309-326, (2022); 
Li R., Effects of mobile-assisted language learning on EFL learners' listening skill development, Educational Technology & Society, 26, 2, pp. 36-49, (2023); 
Li R., Meng Z., Tian M., Zhang Z., Ni C., Xiao W., Examining EFL learners' individual antecedents on the adoption of automated writing evaluation in China, Computer Assisted Language Learning, 32, 7, pp. 784-804, (2019); 
Li R., Meng Z., Tian M., Zhang Z., Xiao W., Modelling Chinese EFL learners' flow experiences in digital game-based vocabulary learning: The roles of learner and contextual factors, Computer Assisted Language Learning, 34, 4, pp. 483-505, (2021); 
Lipsey M.W., Wilson D.B., Practical meta-analysis, (2001); 
Liu C., Hou J., Tu Y., Wang Y., Hwang G., Incorporating a reflective thinking promoting mechanism into artificial intelligence-supported English writing environments, Interactive Learning Environments, pp. 1-19, (2021); 
Loncar M., Schams W., Liang J., Multiple technologies, multiple sources: Trends and analyses of the literature on technology-mediated feedback for L2 English writing published from 2015–2019, Computer Assisted Language Learning, pp. 1-63, (2021); 
Ma K., Improving EFL graduate students' proficiency in writing through an online automated essay assessing system, English Language Teaching, 6, 7, pp. 158-167, (2013); 
Moher D., Shamseer L., Clarke M., Ghersi D., Liberati A., Petticrew M., Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement, Systematic Reviews, 4, 1, pp. 1-9, (2015); 
Morch A.I., Engeness I., Cheng V.C., Cheung W.K., Wong K.C., EssayCritic: Writing to learn with a knowledge-based design critiquing system, Educational Technology & Society, 20, 2, pp. 213-223, (2017); 
Nunes A., Cordeiro C., Limpo T., Castro S.L., Effectiveness of automated writing evaluation systems in school settings: A systematic review of studies from 2000 to 2020, Journal of Computer Assisted Learning, 38, 2, pp. 599-620, (2022); 
Plonsky L., Oswald F., How big is “big”? Interpreting effect sizes in L2 research, Language Learning, 64, pp. 878-912, (2014); 
Qiang Z., An experimental research on applying automated essay scoring system to college English writing course, International Journal of English Language Teaching, 1, 2, pp. 35-41, (2014); 
Sari E., Han T., Automated L2 writing performance assessment: A literature review, The Reading Matrix, 21, 2, pp. 66-84, (2021); 
Shang H.-F., Exploring online peer feedback and automated corrective feedback on EFL writing performance, Interactive Learning Environments, 30, 1, pp. 4-16, (2022); 
Stevenson M., A critical interpretative synthesis: The integration of automated writing evaluation into classroom writing instruction, Computers and Composition, 42, pp. 1-16, (2016); 
Stevenson M., Phakiti A., The effects of computer-generated feedback on the quality of writing, Assessing Writing, 19, pp. 51-65, (2014); 
Sun B., Fan T., The effects of an AWE-aided assessment approach on business English writing performance and writing anxiety: A contextual consideration, Studies in Educational Evaluation, 72, pp. 1-10, (2022); 
Sun C., The impact of online automated writing evaluation: A case study from Dalian, Chinese Journal of Applied Linguistics, 35, 1, pp. 63-79, (2012); 
Tang J., Sun C., Automated writing evaluation in an EFL setting: Lessons from China, The JALT CALL Journal, 13, 2, pp. 117-146, (2017); 
Toranj S., Ansari D.N., Automated versus human essay scoring: A comparative study, Theory and Practice in Language Studies, 2, 4, pp. 719-725, (2012); 
Valentine J.C., Incorporating judgments about study quality into research syntheses, The handbook of research synthesis and meta-analysis, pp. 129-140, (2019); 
Wade-Stein D., Kintsch E., Summary street: Interactive computer support for writing, Cognition and Instruction, 22, 3, pp. 333-362, (2004); 
Waer H., The effect of integrating automated writing evaluation on EFL writing apprehension and grammatical knowledge, Innovation in Language Learning and Teaching, pp. 1-25, (2021); 
Wang J., A comparative study on the washback effects of teacher feedback plus intelligent feedback versus teacher feedback on English writing teaching in higher vocational college, Theory and Practice in Language Studies, 9, 12, pp. 1555-1561, (2019); 
Ware P., Feedback for adolescent writers in the English classroom: Exploring pen-and-paper, electronic, and automated options, Writing & Pedagogy, 6, 2, pp. 223-249, (2014); 
Wilson D.B., Systematic coding for research synthesis, The handbook of research synthesis and meta-analysis, pp. 153-172, (2019); 
Wilson J., Czik A., Automated essay evaluation software in English language arts classrooms: Effects on teacher feedback, student motivation, and writing quality, Computers & Education, 100, pp. 94-109, (2016); 
Wilson J., Potter A., Cordero T.C., Myers M.C., Integrating goal-setting and automated feedback to improve writing outcomes: A pilot study, Innovation in Language Learning and Teaching, pp. 1-17, (2022); 
Wilson J., Roscoe R.D., Automated writing evaluation and feedback: Multiple metrics of efficacy, Journal of Educational Computing Research, 58, 1, pp. 87-125, (2020); 
Zheng L., Bhagat K.K., Zhen Y., Zhang X., The effectiveness of the flipped classroom on students' learning achievement and learning motivation: A meta-analysis, Educational Technology & Society, 23, 1, pp. 1-15, (2020)#FRF#
