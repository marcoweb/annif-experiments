#ITI#Evaluating the Construct Validity of an Automated Writing Evaluation System with a Randomization Algorithm#FTI#
#IRE# This study evaluated the construct validity of six scoring traits of an automated writing evaluation (AWE) system called MI Write. Persuasive essays (N = 100) written by students in grades 7 and 8 were randomized at the sentence-level using a script written with Python’s NLTK module. Each persuasive essay was randomized 30 times (n = 3000 total randomizations), and the mean trait scores for each set of randomized iterations were compared to those of the control text across all traits. We were specifically interested in evaluating the effects of randomization on the high-level traits of idea development and organization. Given the rubrics and qualitative feedback provided by MI Write, we hypothesized that these high-level traits ought to be sensitive to sentence-level randomization (i.e., scores should decrease). Overall, complete randomizations did not consistently significantly impact trait scoring for these high-level writing traits. In fact, more than a third of the essays saw significant increases in one or both high-level traits despite randomization, indicating a disconnect between MI Write’s formative feedback and its underlying constructs. Findings have implications for consumers and developers of AWE#FRE#
#IPC# Automated essay scoring; Automated writing evaluation; Feedback; Validity; Writing assessment#FPC#
#IRF# Bai L., Hu G., In the face of fallible AWE feedback: How do students respond?, Educational Psychology, 37, pp. 67-81, (2017); 
Bejar I., A validity-based approach to quality control and assurance of automated scoring, Assessment in Education: Principles, Policy & Practice, 18, 3, pp. 319-341, (2011); 
Bejar I., Flor M., Futagi Y., Ramineni C., On the vulnerability of automated scoring to construct-irrelevant response strategies (CIRS): An illustration, Assessing Writing, 22, pp. 48-59, (2014); 
Carless D., Trust and its role in facilitating dialogic feedback, Feedback in higher and professional education: Understanding it and doing it well, pp. 90-103, (2012); 
Chodorow M., Burstein J., Beyond essay length: Evaluating e-rater® ’s performance on TOEFL® essays, ETS Research Reports., 2004, (2004); 
CCCC position statement on teaching, learning and assessing writing in digital environments, (2014); 
Deane P., On the relation between automated essay scoring and modern views of the writing construct, Assessing Writing, 18, pp. 7-24, (2013); 
Dujinhower H., Prins F.J., Stokking K.M., Feedback providing improvement strategies and reflection on feedback use: Effects on students’ writing motivation, process, and performance, Learning and Instruction, 22, pp. 171-184, (2012); 
Graham S., Hebert M., Harris K.R., Formative assessment and writing: A meta-analysis, Elementary School Journal, 115, pp. 523-547, (2015); 
Graesser A.C., McNamara D.S., Louwerse M.M., What do readers need to learn in order to process coherence relations in narrative and expository text?, Rethinking reading comprehension, pp. 82-98, (2003); 
Graesser A.C., McNamara D.S., Louwerse M.M., Cai Z., Coh-Metrix: Analysis of text on cohesion and language, Behavioral Research Methods, Instruments, and Computers, 36, 2, pp. 193-202, (2004); 
Higgins D., Heilman M., Managing what we can measure: Quantifying the susceptibility of automated essay scoring systems to gaming behavior, Educational Measurement: Issues and Practice, 33, 3, pp. 36-46, (2014); 
Huang Y., Wilson J., Using automated feedback to develop writing proficiency, Computers and Composition, 62, (2021); 
Kane M.T., An argument-based approach to validity, Psychological Bulletin, 112, pp. 527-535, (1992); 
Kellogg R.T., Whiteford A.P., Quinlan T., Does automated feedback help students learn to write?, Journal of Educational Computing Research, 42, 2, pp. 173-196, (2010); 
Kumar V.S., Boulanger D., Automated essay scoring and the deep learning black box: How are rubric scores determined?, International Journal of Artificial Intelligence in Education, 31, pp. 538-584, (2021); 
Liaqat A., Munteanu C., Epp C.D., Collaborating with mature English language learners to combine peer and automated feedback: A user-centered approach to designing writing support, International Journal of Artificial Intelligence in Education, 31, pp. 638-679, (2021); 
MacArthur C.A., Jennings A., Philippakos Z.A., Which linguistic features predict quality of argumentative writing for college basic writers, and how do those features change with instruction?, Reading and Writing, 32, pp. 1553-1574, (2019); 
The Nation’s Report Card: Writing 2011 (NCES 2012–470), (2012); 
NCTE position statement on machine scoring, (2013); 
An introduction to the 6+1 trait writing assessment model, (2004); 
Pajares F., Self-efficacy beliefs, motivation, and achievement in writing: A review of the literature, Reading & Writing Quarterly, 19, pp. 139-158, (2003); 
Palermo C., Thomson M.M., Teacher implementation of self-regulated strategy development with an automated writing evaluation system: Effects on the argumentative writing performance of middle school students, Contemporary Educational Psychology, 54, pp. 255-270, (2018); 
Palermo C., Wilson J., Implementing automated writing evaluation in different instructional contexts: A mixed-methods study, Journal of Writing Research, 12, 1, pp. 63-108, (2020); 
Parekh S., Singla Y.K., Chen C., Li J.J., Shah R.R., My teacher thinks the world is flat! Interpreting automatic essay scoring mechanism, (2020); 
Perelman L., When the “state of the art” is counting words, Assessing Writing, 21, pp. 104-111, (2014); 
Perelman L., The BABEL generator and E-Rater: 21<sup>st</sup> century writing constructs and automated essay scoring (AES), The Journal of Writing Assessment, 13, 1, (2020); 
Perfetti C.A., The limits of co-occurrence: Tools and theories in language research, Discourse Processes, 25, pp. 363-377, (1998); 
Persky H.R., Daane M.C., Jin Y., The Nation’s Report Card: Writing 2002. (NCES, pp. 2003-2529, (2002); 
Powers D.E., Burstein J.C., Chodorow M., Fowles M.E., Kukich K., Stumping e-rater: Challenging the validity of automated essay scoring, Computers in Human Behavior, 18, pp. 103-134, (2002); 
Raczynski K., Cohen A., Appraising the scoring performance of automated essay scoring systems—Some additional considerations: Which essays? Which human raters? Which scores?, Applied Measurement in Education, 31, 3, pp. 233-240, (2018); 
Ramineni C., Williamson D.M., Automated essay scoring: Psychometric guidelines and practices, Assessing Writing, 18, pp. 25-39, (2013); 
Saal F.E., Downey R.G., Lahey M.A., Rating the ratings: Assessing the psychometric quality of rating data, Psychological Bulletin, 88, pp. 413-428, (1980); 
Shermis M.D., The challenges of emulating human behavior in writing assessment, Assessing Writing, 22, pp. 91-99, (2014); 
Shermis M.D., Burstein J.C., Automated essay scoring: A cross-disciplinary perspective, (2003); 
Shermis M.D., Burstein J., Handbook of automated essay evaluation: Current applications and new directions, (2013); 
Shute V.J., Focus on formative feedback, Review of Educational Research, 78, pp. 153-189, (2008); 
Stevenson M., A critical interpretative synthesis: The integration of automated writing evaluation into classroom writing instruction, Computers and Composition, 42, pp. 1-16, (2016); 
Stevenson M., Phakiti A., The effects of computer-generated feedback on the quality of writing, Assessing Writing, 19, pp. 51-65, (2014); 
Vajjala S., Automated assessment of non-native learner essays: Investigating the role of linguistic features, International Journal of Artificial Intelligence in Education, 28, pp. 79-105, (2018); 
Wang E.L., Matsumura L.C., Correnti R., Litman D., Zhang H., Howe E., Magooda A., Quintana R., eRevis(ing): Students’ revision of text evidence use in an automated writing evaluation system, Assessing Writing, 44, (2020); 
Warschauer M., Grimes D., Automated writing assessment in the classroom, Pedagogies, 3, pp. 22-36, (2008); 
Weigle S.C., Investigating rater/prompt interactions in writing assessment: Quantitative and qualitative approaches, Assessing Writing, 6, pp. 145-178, (1999); 
Williamson D.M., Xi X., Breyer F.J., A framework for evaluation and use of automated scoring, Educational Measurement: Issues and Practice, 31, pp. 2-13, (2012); 
Wilson J., Ahrendt C., Fudge E., Raiche A., Beard G., MacArthur C.A., Elementary teachers’ perceptions of automated feedback and automated scoring: Transforming the teaching and learning of writing using automated writing evaluation, Computers & Education, 168, (2021); 
Wilson J., Chen D., Sandbank M.P., Hebert M., Generalizability of automated scores of writing quality in grades 3–5, Journal of Educational Psychology, 111, pp. 619-640, (2019); 
Wilson J., Czik A., Automated essay evaluation software in English language arts classrooms: Effects on teacher feedback, student motivation, and writing quality, Computers and Education, 100, pp. 94-109, (2016); 
Wilson J., Huang Y., Palermo C., Beard G., MacArthur C.A., Automated feedback and automated scoring in the elementary grades: Usage, attitudes, and associations with writing outcomes in a districtwide implementation of MI Write, International Journal of Artificial Intelligence in Education, (2021); 
Wilson J., Myers M.C., Potter A.H., Investigating the promise of automated writing evaluation for supporting formative writing assessment at scale, Assessment in Education: Principles, Policy & Practice., (2022); 
Wilson J., Roscoe R.D., Automated writing evaluation and feedback: Multiple metrics of efficacy, Journal of Educational Computing Research, 58, pp. 87-125, (2020); 
Wolfe E.W., Uncovering raters’ cognitive processing and focus using think-aloud protocols, Journal of Writing Assessment, 2, pp. 37-56, (2005)#FRF#
