#ITI#Uncovering Student Strategies for Solving Scientific Inquiry Tasks: Insights from Student Process Data in PISA#FTI#
#IRE#The advancement of technology has led to a growing interest in assessing scientific inquiry within digital platforms. This shift towards dynamic and interactive inquiry assessments enables researchers to investigate not only the accuracy of student responses (product data) but also their steps and actions leading to those responses (process data). This is done by analyzing computer-generated log files that capture student activity during the assessment. The present study leverages this opportunity by drawing insights from student log files of the Programme for International Student Assessment (PISA). It demonstrates the potential of process data in uncovering typically unobserved students’ problem-solving processes by focusing on two critical scientific inquiry skills: coordinating the effects of multiple variables and coordinating a theory with evidence. This study presents two examples for analyzing process data. The first example examined data from the PISA field trial study and showcased the advantage of using a process mining approach to visualize the sequence of students’ steps and actions in conducting investigations. The second example linked student log files and questionnaire data from the PISA 2015. It applied latent profile analysis to identify unique patterns of students’ inquiry performance and examined their relationships to their school-based inquiry experiences. Findings from both examples indicate that students often encounter considerable challenges in solving complex inquiry tasks, especially in applying multivariable reasoning and constructing scientific explanations. This study highlights the profound potential of process data in facilitating a deeper understanding of how students interact with scientific inquiry tasks in a digital-based environment. © The Author(s) 2023.#FRE#
#IPC#Inquiry; Latent profile analysis; Log files; Process mining; Science assessment; Simulation#FPC#
#IRF#Bogarin A., Cerezo R., Romero C., A survey on educational process mining, WIREs Data Mining and Knowledge Discovery, 8, 1, (2018); 
De Jong T., van Joolingen W.R., Scientific discovery learning with computer simulations of conceptual domains, Review of Educational Research, 68, 2, pp. 179-201, (1998); 
De Jong T., Lazonder A.W., Chinn C.A., Fischer F., Gobert J., Hmelo-Silver C.E., Koedinger K.R., Krajcik J.S., Kyza E.A., Linn M.C., Pedaste M., Scheiter K., Zacharia Z.C., Let’s talk evidence – The case for combining inquiry-based and direct instruction, Educational Research Review, 39, (2023); 
Delen I., Krajcik J., What do students’ explanations look like when they use second-hand data?, International Journal of Science Education, 37, 12, pp. 1953-1973, (2015); 
Goldhammer F., Hahnel C., Kroehne U., Zehner F., From byproduct to design factor: On validating the interpretation of process indicators based on log data, Large-scale Assessments in Education, 9, (2021); 
Greiff S., Niepel C., Scherer R., Martin R., Understanding students’ performance in a computer-based assessment of complex problem solving: An analysis of behavioural data from computer-generated log files, Computers in Human Behaviour, 61, pp. 36-46, (2016); 
Gunther C.W., Rozinat A., Disco: Discover your processes, BPM (Demos), 940, 1, pp. 40-44, (2012); 
Kuang H., Sahin F., Comparison of disengagement levels and the impact of disengagement on item parameters between PISA 2015 and PISA 2018 in the United States, Large-scale Assessments in Education, 11, 1, (2023); 
Kuhn D., What do young science students need to learn about variables?, Science Education, 100, 2, pp. 392-403, (2016); 
Kuhn D., Arvidsson T.S., Lesperance R., Corprew R., Can engaging in science practices promote deep understanding of them?, Science education, 101, 2, pp. 232-250, (2017); 
Lesperance R., Kuhn D., Breadth and relevance of multivariable inquiry supports deep understanding of science practice, Science Education, 107, 1, pp. 71-88, (2023); 
Masyn K.E., Latent class analysis and finite mixture modeling, The Oxford handbook of quantitative methods in psychology, pp. 551-611, (2013); 
McLure F., The thinking frames approach: Improving high school students’ written explanations of phenomena in science, Research in Science Education, 53, pp. 173-191, (2023); 
McLure F., Won M., Treagust D.F., Analysis of students’ diagrams explaining scientific phenomena, Research in Science Education, 52, pp. 1225-1241, (2022); 
Mullis I.V.S., Martin M.O., TIMSS 2019 assessment frameworks, (2017); 
Muthen L.K., Muthen B.O., Mplus version 8.5, (1998); 
Next generation science standards: For states, by states, (2013); 
PISA 2015 assessment and analytical framework: Science, reading, mathematic and financial literacy, (2016); 
Osborne J., Henderson J.B., MacPherson A., Szu E., Wild A., Yao S.-Y., The development and validation of a learning progression for argumentation in science, Journal of Research in Science Teaching, 53, 6, pp. 821-846, (2016); 
Provasnik S., Process data, the new frontier for assessment development: Rich new soil or a quixotic quest?, Large-scale Assessments in Education, 9, (2021); 
Reis Costa D., Leoncio Netto W., Process data analysis in ILSAs, International handbook of comparative large-scale studies in education: Perspectives, methods and findings, pp. 1-27, (2022); 
Ronnebeck S., Bernholt S., Ropohl M., Searching for a common ground–A literature review of empirical research on scientific inquiry activities, Studies in Science Education, 52, 2, pp. 161-197, (2016); 
Sandoval W.A., Millwood K.A., The quality of students’ use of evidence in written scientific explanations, Cognition and Instruction, 23, 1, pp. 23-55, (2005); 
Scalise K., Clarke-Midura J., The many faces of scientific inquiry: Effectively measuring what students do and not only what they say, Journal of Research in Science Teaching, 55, 10, pp. 1469-1496, (2018); 
Schwartz R.S., Lederman J.S., Enderle P.J., Scientific inquiry literacy: The missing link on the continuum from science literacy to scientific literacy, Handbook of Research on Science Education, pp. 749-782, (2023); 
Smetana L.K., Bell R.L., Computer simulations to support science instruction and learning: A critical review of the literature, International Journal of Science Education, 34, 9, pp. 1337-1370, (2012); 
Stadler M., Brandl L., Greiff S., 20 years of interactive tasks in large-scale assessments: Process data as a way towards sustainable change?, Journal of Computer Assisted Learning, (2023); 
Teig N., Scherer R., Nilsen T., More isn’t always better: The curvilinear relationship between inquiry-based teaching and student achievement in science, Learning and Instruction, 56, pp. 20-29, (2018); 
Teig N., Scherer R., Kjaernsli M., Identifying patterns of students’ performance on simulated inquiry tasks using PISA 2015 log-file data, Journal of Research in Science Teaching, 57, 9, pp. 1400-1429, (2020); 
Teig N., Scherer R., Olsen R.V., A systematic review of studies investigating science teaching and learning: Over two decades of TIMSS and PISA, International Journal of Science Education, 44, 12, pp. 2035-2058, (2022); 
Tschirgi J.E., Sensible reasoning: A hypothesis about hypotheses, Child Development, pp. 1-10, (1980); 
Ulitzsch E., Ulitzsch V., He Q., Ludtke O., A machine learning-based procedure for leveraging clickstream data to investigate early predictability of failure on interactive tasks, Behaviour Research Methods, 55, 3, pp. 1392-1412, (2023); 
van der Aalst W.M.P., Process mining: Data science in action, (2016); 
Zacharia Z.C., Manoli C., Xenofontos N., de Jong T., Pedaste M., van Riesen S.A.N., Kamp E.T., Maeots M., Siiman L., Tsourlidaki E., Identifying potential types of guidance for supporting student inquiry when using virtual and remote labs in science: A literature review, Educational Technology Research and Development, 63, 2, pp. 257-302, (2015); 
Zimmerman C., The development of scientific thinking skills in elementary and middle school, Developmental Review, 27, 2, pp. 172-223, (2007); 
Zumbo B.D., Maddox B., Care N.M., Process and product in computer-based assessments, European Journal of Psychological Assessment, 39, pp. 252-262, (2023)#FRF#
