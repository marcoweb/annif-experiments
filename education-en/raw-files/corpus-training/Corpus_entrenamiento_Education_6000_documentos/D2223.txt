#ITI#Identifying Effortful Individuals With Mixture Modeling Response Accuracy and Response Time Simultaneously to Improve Item Parameter Estimation#FTI#
#IRE# The responses of non-effortful test-takers may have serious consequences as non-effortful responses can impair model calibration and latent trait inferences. This article introduces a mixture model, using both response accuracy and response time information, to help differentiating non-effortful and effortful individuals, and to improve item parameter estimation based on the effortful group. Two mixture approaches are compared with the traditional response time mixture model (TMM) method and the normative threshold 10 (NT10) method with response behavior effort criteria in four simulation scenarios with regard to item parameter recovery and classification accuracy. The results demonstrate that the mixture methods and the TMM method can reduce the bias of item parameter estimates caused by non-effortful individuals, with the mixture methods showing more advantages when the non-effort severity is high or the response times are not lognormally distributed. An illustrative example is also provided.#FRE#
#IPC# hierarchical model; mixture method; non-effortful individuals; response time#FPC#
#IRF# Beguin A.A., Bayesian IRT equating with correction for unmotivated respondents on the anchor-test, (2005); 
Beguin A.A., Maan A., IRT linking of high-stakes tests with a low-stakes anchor, (2007); 
Bolt D., Cohen A., Wollack J., Item parameter estimation under conditions of test speededness: Application of a mixture Rasch model with ordinal constraints, Journal of Educational Measurement, 39, pp. 331-348, (2002); 
Bridgeman B., Cline F., Effects of differentially time-consuming tests on computer-adaptive test scores, Journal of Educational Measurement, 41, pp. 137-148, (2004); 
Cheng Y., Diao Q., Behrens J.T., A simplified version of the maximum information per time unit method in computerized adaptive testing, Behavior Research Methods, 49, pp. 502-512, (2017); 
Choe E.M., Kern J.L., Chang H.H., Optimizing the use of response times for item selection in computerized adaptive testing, Journal of Educational and Behavioral Statistics, 43, pp. 135-158, (2018); 
Debeer D., Buchholz J., Hartig J., Janssen R., Student, school, and country differences in sustained test-taking effort in the 2009 PISA reading assessment, Journal of Educational & Behavioral Statistics, 39, pp. 502-523, (2014); 
Entink R.K., van Der Linden W.J., Fox J.P., A Box–Cox normal model for response times, British Journal of Mathematical and Statistical Psychology, 62, pp. 621-640, (2009); 
Fan Z., Wang C., Chang H.H., Douglas J., Utilizing response time distributions for item selection in CAT, Journal of Educational and Behavioral Statistics, 37, pp. 655-670, (2012); 
Fisher R.A., Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population, Biometrika, 10, pp. 507-521, (1915); 
Fraley C., Raftery A., Murphy T., Scrucca L., Mclust—Normal mixture modeling for model-based clustering, classification, and density estimation, (2014); 
Gelman A., Rubin D.B., Inference from iterative simulation using multiple sequences (with discussion), Statistical Science, 7, pp. 457-511, (1992); 
Hauser C., Kingsbury G.G., Individual score validity in a modest-stakes adaptive educational testing setting, (2009); 
Hong M.R., Cheng Y., Robust maximum marginal likelihood (RMML) estimation for item response theory models, Behavior Research Methods, 51, pp. 573-588, (2019); 
Karabatsos G., Comparing the aberrant response detection performance of thirty-six person-fit statistics, Applied Measurement in Education, 16, pp. 277-298, (2003); 
Kolen M.J., Brennan R.L., Test equating, scaling, and linking: Methods and practices, (2004); 
Kong X., Wise S.L., Bhola D.S., Setting the response time threshold parameter to differentiate solution behavior from rapid-guessing behavior, Educational and Psychological Measurement, 67, pp. 606-619, (2007); 
Lathrop Q.N., Cheng Y., Two approaches to estimation of classification accuracy rate under item response theory, Applied Psychological Measurement, 37, pp. 226-241, (2013); 
Lee W., Classification consistency and accuracy for complex assessments using item response theory, Journal of Educational Measurement, 47, pp. 1-17, (2010); 
Masyn K.E., latent class analysis and finite mixture modeling, The Oxford handbook of quantitative methods, pp. 551-611, (2013); 
Meijer R.R., Sijtsma K., Methodology review: Evaluating person fit, Applied Psychological Measurement, 25, pp. 107-135, (2001); 
Meyer J.P., A mixture Rasch model with item response time components, Applied Psychological Measurement, 34, pp. 521-538, (2010); 
Mittelhaeuser M.A., Beguin A.A., Sijtsma K., The effect of differential motivation on irt linking, Journal of Educational Measurement, 52, pp. 339-358, (2015); 
Molenaar D., Bolsinova M., Rozsa S., De Boeck P., Response mixture modeling of intraindividual differences in responses and response times to the Hungarian WISC-IV Block Design test, Journal of Intelligence, 4, 3, (2016); 
Molenaar D., Bolsinova M., Vermunt J.K., A semi-parametric within-subject mixture approach to the analyses of responses and response times, British Journal of Mathematical and Statistical Psychology, 71, pp. 205-228, (2018); 
Molenaar D., de Boeck P., Response mixture modeling: Accounting for heterogeneity in item characteristics across response times, Psychometrika, 83, pp. 279-297, (2018); 
Molenaar D., Oberski D., Vermunt J., De Boeck P., Hidden Markov item response theory models for responses and response times, Multivariate Behavioral Research, 51, pp. 606-626, (2016); 
Morgenthaler S., A survey of robust statistics, Statistical Methods & Applications, 15, pp. 271-293, (2007); 
Muthen L.K., Muthen B.O., Statistical analysis with latent variables using Mplus, (2012); 
Oberski D., Mixture models: Latent profile and latent class analysis, Modern statistical methods for HCI, pp. 275-287, (2016); 
PISA 2015 technical report, (2017); 
Oshima T.C., The effect of speededness on parameter estimation in item response theory, Journal of Educational Measurement, 31, pp. 200-219, (1994); 
Pastor D.A., Ong T.Q., Strickman S.N., Patterns of solution behavior across items in low-stakes assessments, Educational Assessment, 24, pp. 189-212, (2019); 
Patton J.M., Cheng Y., Effects of item calibration error on applications of item response theory, Advancing methodologies to support both summative and formative assessments, pp. 89-105, (2014); 
Patton J.M., Cheng Y., Hong M., Diao Q., Detection and treatment of careless responses to improve item parameter estimation, Journal of Educational and Behavioral Statistics, 44, pp. 309-341, (2019); 
Patton J.M., Cheng Y., Yuan K.H., Diao Q., The influence of item calibration error on variable-length computerized adaptive testing, Applied Psychological Measurement, 37, pp. 24-40, (2013); 
Plummer M., JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling, (2003); 
Pokropek A., Grade of membership response time model for detecting guessing behaviors, Journal of Educational and Behavioral Statistics, 41, pp. 300-325, (2016); 
R: A language and environment for statistical computing [Computer software manual], (2009); 
Ranger J., Kuhn J.T., Detecting unmotivated individuals with a new model-selection approach for Rasch models, Psychological Test and Assessment Modeling, 59, pp. 269-295, (2017); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of careless responding on aggregated-scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, pp. 74-104, (2017); 
Schnipke D.L., Assessing speededness in computer-based tests using item response times, (1995); 
Schnipke D.L., Scrams D., Modeling item response times with a two-state mixture model: A new method of measuring speededness, Journal of Educational Measurement, 34, pp. 213-232, (1997); 
Schwarz G., Estimating the dimension of a model, Annals of Statistics, 6, pp. 461-464, (1978); 
van der Linden W.J., A hierarchical framework for modeling speed and accuracy on test items, Psychometrika, 72, pp. 287-308, (2007); 
van der Linden W.J., Glas C.A.W., Capitalization on item calibration error in adaptive testing, Applied Measurement in Education, 13, pp. 35-53, (2000); 
Wang C., Fan Z., Chang H.-H., Douglas J.A., A semiparametric model for jointly analyzing response times and accuracy in computerized testing, Journal of Educational and Behavioral Statistics, 38, pp. 381-417, (2013); 
Wang C., Xu G., A mixture hierarchical model for response times and response accuracy, British Journal of Mathematical and Statistical Psychology, 68, pp. 456-477, (2015); 
Wang C., Xu G., Shang Z., A two-stage approach to differentiating normal and aberrant behavior in computer based testing, Psychometrika, 83, pp. 223-254, (2018); 
Wang C., Xu G., Shang Z., Kuncel N., Detecting aberrant behavior and item preknowledge: A comparison of mixture modeling method and residual method, Journal of Educational and Behavioral Statistics, 43, pp. 469-501, (2018); 
Weirich S., Hecht M., Penk C., Roppelt A., Hme K.B., Item position effects are moderated by changes in test-taking effort, Applied Psychological Measurement, 41, pp. 115-129, (2016); 
Wise S.L., An investigation of the differential effort received by items on a low-stakes, computer-based test, Applied Measurement in Education, 19, pp. 93-112, (2006); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, pp. 237-252, (2015); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement: Issues and Practice, 36, pp. 52-61, (2017); 
Wise S.L., DeMars C.E., An application of item response time: The effort-moderated IRT model, Journal of Educational Measurement, 43, pp. 19-38, (2006); 
Wise S.L., DeMars C.E., Examinee non-effort and the validity of program assessment results, Educational Assessment, 15, pp. 27-41, (2010); 
Wise S.L., Gao L., A general approach to measuring test-taking effort on computer-based tests, Applied Measurement in Education, 30, pp. 343-354, (2017); 
Wise S.L., Kingsbury G.G., Thomason J., Kong X., An investigation of motivation filtering in a statewide achievement testing program, (2004); 
Wise S.L., Kong X.J., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, pp. 163-183, (2005); 
Wise S.L., Ma L., Setting response time thresholds for a CAT item pool: The normative threshold method, (2012); 
Woods C.M., Ramsay curve IRT for Likert-type data, Applied Psychological Measurement, 31, pp. 195-212, (2007); 
Yamamoto K., Everson H., Modeling the effects of test length and test time on parameter estimation using the HYBRID model, Applications of latent trait and latent class models in the social sciences, pp. 89-98, (1997); 
Yu X., Cheng Y., A change-point analysis procedure based on weighted residuals to detect back random responding, Psychological Methods, 24, pp. 658-674, (2019)#FRF#
