#ITI#Leveraging Granularity: Hierarchical Reinforcement Learning for Pedagogical Policy Induction#FTI#
#IRE# In interactive e-learning environments such as Intelligent Tutoring Systems, pedagogical decisions can be made at different levels of granularity. In this work, we focus on making decisions at two levels: whole problems vs. single steps and explore three types of granularity: problem-level only (Prob-Only), step-level only (Step-Only) and both problem and step levels (Both). More specifically, for Prob-Only, our pedagogical agency decides whether the next problem should be a worked example (WE) or a problem-solving (PS). In WEs, students observe how the tutor solves a problem while in PSs students solve the problem themselves. For Step-Only, the agent decides whether to elicit the student’s next solution step or to tell the step directly. Here the student and the tutor co-construct the solution and we refer to this type of task as collaborative problem-solving (CPS). For Both, the agency first decides whether the next problem should be a WE, a PS, or a CPS and based on the problem-level decision, the agent then makes step-level decisions on whether to elicit or tell each step. In a series of classroom studies, we compare the three types of granularity under random yet reasonable pedagogical decisions. Results showed that while Prob-Only may be less effective for High students, Step-Only may be less effective for Low ones, Both can be effective for both High and Low students. Motivated by these findings, we propose and apply an offline, off-policy Gaussian Processes based Hierarchical Reinforcement Learning (HRL) framework to induce a hierarchical pedagogical policy that makes adaptive, effective decisions at both the problem and step levels. In an empirical classroom study, our results showed that the HRL policy is significantly more effective than a Deep Q-Network (DQN) induced step-level policy and a random yet reasonable step-level baseline policy#FRE#
#IPC# Decision granularity; Hierarchical reinforcement learning; Pedagogical policy#FPC#
#IRF# Anderson J.R., Problem solving and learning, American Psychologist, 48, 1, (1993); 
Anderson J.R., Corbett A.T., Koedinger K.R., Pelletier R., Cognitive tutors: Lessons learned, The journal of the learning sciences, 4, 2, pp. 167-207, (1995); 
Azizsoltani H., Kim Y.J., Ausin M.S., Barnes T., Chi M., Unobserved is not equal to non-existent: Using gaussian processes to infer immediate rewards across contexts, IJCAI, pp. 1974-1980, (2019); 
Azizsoltani H., Sadeghi E., Adaptive sequential strategy for risk estimation of engineering systems using gaussian process regression active learning, Engineering Applications of Artificial Intelligence, 74, July, pp. 146-165, (2018); 
Barto A.G., Mahadevan S., Recent advances in hierarchical reinforcement learning, Discrete event dynamic systems, 13, 1-2, pp. 41-77, (2003); 
Beck J., Woolf B.P., Beal C.R., Advisor: a machine learning architecture for intelligent tutor construction, AAAI/IAAI, 2000, 552-557, pp. 1-2, (2000); 
Chaiklin S., Et al., The zone of proximal development in vygotsky’s analysis of learning and instruction, Vygotsky’s educational theory in cultural context, 1, pp. 39-64, (2003); 
Chi M., Vanlehn K., Accelerated future learning via explicit instruction of a problem solving strategy, Frontiers In Artificial Intelligence And Applications, 158, (2007); 
Chi M., VanLehn K., Meta-cognitive strategy instruction in intelligent tutoring systems: how, when, and why, Educational Technology & Society, 13, 1, pp. 25-39, (2010); 
Chi M., VanLehn K., Litman D., Jordan P., Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies, User Modeling and User-Adapted Interaction, 21, 1-2, pp. 137-180, (2011); 
Clement B., Oudeyer P.Y., Lopes M., A comparison of automatic teaching strategies for heterogeneous student populations, EDM 16-9Th International Conference on Educational Data Mining, (2016); 
Cronbach L.J., Snow R.E., Aptitudes and Instructional Methods: A Handbook for Research on Interactions, (1977); 
Cuayahuitl H., Dethlefs N., Frommberger L., Richter K.F., Bateman J., Generating adaptive route instructions using hierarchical reinforcement learning, International Conference on Spatial Cognition, pp. 319-334, (2010); 
Doroudi S., Aleven V., Brunskill E., Robust evaluation matrix: Towards a more principled offline exploration of instructional policies, In Proceedings of the Fourth (2017) ACM Conference on Learning@ Scale, pp. 3-12, (2017); 
Doroudi S., Aleven V., Brunskill E., Where’s the reward?, International Journal of Artificial Intelligence in Education, 29, 4, pp. 568-620, (2019); 
Eaton M.L., Multivariate statistics: a vector space approach, John Wiley & Sons, Inc., 605 Third Ave., New York, NY 10158, USA, 1983, 512, pp. 116-117, (1983); 
Feller W., An introduction to probability theory and its applications, Vol. 2, (2008); 
Goldberg P.W., Williams C.K., Et al., Regression with input-dependent noise: A gaussian process treatment, NIPS, pp. 493-499, (1998); 
Guo D., Shamai S., Verdu S., Mutual information and minimum mean-square error in gaussian channels, IEEE Transactions on Information Theory, 51, 4, pp. 1261-1282, (2005); 
Haarnoja T., Zhou A., Et al., Soft actor-critic algorithms and applications, (2018); 
Iglesias A., Martinez P., Aler R., Fernandez F., Learning teaching strategies in an adaptive and intelligent educational system through reinforcement learning, Applied Intelligence, 31, 1, pp. 89-106, (2009); 
Iglesias A., Martinez P., Aler R., Fernandez F., Reinforcement learning of pedagogical policies in adaptive and intelligent educational systems, Knowledge-Based Systems, 22, 4, pp. 266-270, (2009); 
Kalyuga S., Renkl A., Expertise reversal effect and its instructional implications: Introduction to the special issue, Instructional Science, 38, 3, pp. 209-215, (2010); 
Kulkarni T.D., Narasimhan K., Saeedi A., Tenenbaum J., Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation, Advances in Neural Information Processing Systems, pp. 3675-3683, (2016); 
Lillicrap T.P., Hunt J.J., Et al., Continuous Control with Deep Reinforcement Learning, (2015); 
Lillicrap T.P., Hunt J.J., Et al., Continuous Control with Deep Reinforcement Learning, (2015); 
Mandel T., Liu Y.E., Levine S., Brunskill E., Popovic Z., Offline policy evaluation across representations with applications to educational games, Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems, pp. 1077-1084, (2014); 
McLaren B.M., van Gog T., Ganoe C., Yaron D., Karabinos M., Exploring the assistance dilemma: Comparing instructional support in examples and problems, Intelligent Tutoring Systems, pp. 354-361, (2014); 
McLaren B.M., Isotani S., When is it best to learn with all worked examples?, International Conference on Artificial Intelligence in Education, pp. 222-229, (2011); 
McLaren B.M., Lim S.J., Koedinger K.R., When and how often should worked examples be given to students? New results and a summary of the current state of research, Cogsci, pp. 2176-2181, (2008); 
Mnih V., Kavukcuoglu K., Silver D., Rusu A.A., Veness J., Bellemare M.G., Graves A., Riedmiller M., Fidjeland A.K., Ostrovski G., Et al., Human-level control through deep reinforcement learning, Nature, 518, 7540, (2015); 
Najar A.S., Mitrovic A., McLaren B.M., Adaptive support versus alternating worked examples and tutored problems: Which leads to better learning?, UMAP, pp. 171-182, (2014); 
Peng X.B., Berseth G., Yin K., Van De Panne M., Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning, ACM Transactions on Graphics (TOG), 36, 4, (2017); 
Phobun P., Vicheanpanya J., Adaptive intelligent tutoring systems for e-learning systems, Procedia-Social and Behavioral Sciences, 2, 2, pp. 4064-4069, (2010); 
Rafferty A.N., Brunskill E., Griffiths T.L., Shafto P., Faster teaching via pomdp planning, Cognitive science, 40, 6, pp. 1290-1332, (2016); 
Rasmussen C.E., Gaussian processes in machine learning, Advanced Lectures on Machine Learning, pp. 63-71, (2004); 
Renkl A., Atkinson R.K., Maier U.H., Staley R., From example study to problem solving: Smooth transitions help learning, The Journal of Experimental Education, 70, 4, pp. 293-315, (2002); 
Rowe J., Mott B., Lester J., Optimizing player experience in interactive narrative planning: A modular reinforcement learning approach, Tenth Artificial Intelligence and Interactive Digital Entertainment Conference, (2014); 
Rowe J.P., Lester J.C., Improving student problem solving in narrative-centered learning environments: A modular reinforcement learning framework, International Conference on Artificial Intelligence in Education, pp. 419-428, (2015); 
Ryan M., Reid M., Learning to fly: An application of hierarchical reinforcement learning, Proceedings of the 17Th International Conference on Machine Learning, (2000); 
Salden R.J., Aleven V., Schwonke R., Renkl A., The expertise reversal effect and worked examples in tutored problem solving, Instructional Science, 38, 3, pp. 289-307, (2010); 
Schaul T., Quan J., Antonoglou I., Silver D., Prioritized experience replay, (2015); 
Schulman J., Levine S., Abbeel P., Jordan M., Moritz P., Trust region policy optimization, International Conference on Machine Learning, pp. 1889-1897, (2015); 
Schulman J., Wolski F., Dhariwal P., Radford A., Klimov O., Proximal policy optimization algorithms, (2017); 
Schwab D., Ray S., Offline reinforcement learning with task hierarchies, Machine Learning, 106, 9-10, pp. 1569-1598, (2017); 
Schwonke R., Renkl A., Krieg C., Wittwer J., Aleven V., Salden R., The worked-example effect: Not an artefact of lousy control conditions, Computers in Human Behavior, 25, 2, pp. 258-266, (2009); 
Shen S., Ausin M.S., Mostafavi B., Chi M., Improving learning & reducing time: A constrained action-based reinforcement learning approach, Proceedings of the 26Th Conference on User Modeling, Adaptation and Personalization, pp. 43-51, (2018); 
Shen S., Chi M., Reinforcement learning: The sooner the better, or the later the better?, Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization, pp. 37-44, (2016); 
Shih B., Koedinger K.R., Scheines R., A response time model for bottom-out hints as worked examples, Handbook of Educational Data Mining, pp. 201-212, (2011); 
Silver D., Huang A., Maddison C.J., Guez A., Sifre L., Van Den Driessche G., Schrittwieser J., Antonoglou I., Panneershelvam V., Lanctot M., Et al., Mastering the game of go with deep neural networks and tree search, Nature, 529, 7587, (2016); 
Silver D., Hubert T., Schrittwieser J., Et al., A general reinforcement learning algorithm that masters chess, shogi, and go through self-play, Science, 362, 6419, pp. 1140-1144, (2018); 
Snow R.E., Aptitude-treatment interaction as a framework for research on individual differences in psychotherapy, Journal of Consulting and Clinical Psychology, 59, 2, (1991); 
Stamper J.C., Eagle M., Barnes T., Croy M., Experimental evaluation of automatic hint generation for a logic tutor, International Conference on Artificial Intelligence in Education, pp. 345-352, (2011); 
Sutton R.S., Precup D., Singh S., Between mdps and semi-mdps: a framework for temporal abstraction in reinforcement learning, Artificial Intelligence, 112, 1-2, pp. 181-211, (1999); 
Sweller J., Cooper G.A., The use of worked examples as a substitute for problem solving in learning algebra, Cognition and Instruction, 2, 1, pp. 59-89, (1985); 
Swetz F., To know and to teach: Mathematical pedagogy from a historical context, Educational Studies in Mathematics, 29, 1, pp. 73-88, (1995); 
Swetz F.J., Capitalism and arithmetic: The new math of the 15th century, including the full text of the Treviso arithmetic of 1478, Translated by David Eugene Smith Open Court Publishing, (1987); 
Van Gog T., Kester L., Paas F., Effects of worked examples, example-problem, and problem-example pairs on novices’ learning, Contemporary Educational Psychology, 36, 3, pp. 212-218, (2011); 
van Hasselt H., Guez A., Silver D., Deep reinforcement learning with double q-learning, AAAI. Phoenix, AZ, 2, (2016); 
Vanlehn K., The behavior of tutoring systems, IJAIED, 16, 3, pp. 227-265, (2006); 
Vanlehn K., Bhembe D., Chi M., Lynch C., Schulze K., Shelby R., Taylor L., Treacy D., Weinstein A., Wintersgill M., Implicit versus explicit learning of strategies in a non-procedural cognitive skill, International Conference on Intelligent Tutoring Systems, pp. 521-530, (2004); 
Vinyals O., Babuschkin I., Czarnecki W., Et al., Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature, 575, (2019); 
Wang X., Chen W., Wu J., Wang Y.F., Yang Wang W., Video captioning via hierarchical reinforcement learning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4213-4222, (2018); 
Wang Z., Schaul T., Hessel M., van Hasselt H., Lanctot M., de Freitas N., Dueling network architectures for deep reinforcement learning, (2015); 
Williams J.D., The best of both worlds: Unifying conventional dialog systems and pomdps, INTERSPEECH, pp. 1173-1176, (2008); 
Zhou G., Azizsoltani H., Ausin M.S., Barnes T., Chi M., Hierarchical reinforcement learning for pedagogical policy induction, International Conference on Artificial Intelligence in Education, (2019); 
Zhou G., Chi M., The impact of decision agency & granularity on aptitude treatment interaction in tutoring, Proceedings of the 39Th Annual Conference of the Cognitive Science Society, pp. 3652-3657, (2017); 
Zhou G., Lynch C., Price T.W., Barnes T., Chi M., The impact of granularity on the effectiveness of students’ pedagogical decision, Proceedings of the 38Th Annual Conference of the Cognitive Science Society, pp. 2801-2806, (2016); 
Zhou G., Price T.W., Lynch C., Barnes T., Chi M., The impact of granularity on worked examples and problem solving, Proceedings of the 37Th Annual Conference of the Cognitive Science Society, pp. 2817-2822, (2015); 
Zhou G., Wang J., Lynch C., Chi M., Towards closing the loop: Bridging machine-induced pedagogical policies to learning theories, (2017); 
Zhou G., Yang X., Azizsoltani H., Barnes T., Chi M., Improving student-tutor interaction through data-driven explanation of hierarchical reinforcement induced pedagogical policies, Proceedings of the 28Th Conference on User Modeling, Adaptation and Personalization, (2020)#FRF#
