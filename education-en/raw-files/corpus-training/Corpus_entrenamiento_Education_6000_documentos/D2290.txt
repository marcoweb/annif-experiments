#ITI#Effects of Response Option Order on Likert-Type Psychometric Properties and Reactions#FTI#
#IRE# The effects of different response option orders on survey responses have been studied extensively. The typical research design involves examining the differences in response characteristics between conditions with the same item stems and response option orders that differ in valence—either incrementally arranged (e.g., strongly disagree to strongly agree) or decrementally arranged (e.g., strongly agree to strongly disagree). The present study added two additional experimental conditions—randomly incremental or decremental and completely randomized. All items were presented in an item-by-item format. We also extended previous studies by including an examination of response option order effects on: careless responding, correlations between focal predictors and criteria, and participant reactions, all the while controlling for false discovery rate and focusing on the size of effects. In a sample of 1,198 university students, we found little to no response option order effects on a recognized personality assessment vis-à-vis measurement equivalence, scale mean differences, item-level distributions, or participant reactions. However, the completely randomized response option order condition differed on several careless responding indices suggesting avenues for future research.#FRE#
#IPC# careless responding; Likert-type scales; online surveys; participant reactions; response option order; survey responses#FPC#
#IRF# Arvey R.D., Strickland W., Drauden G., Martin C., Motivational components of test taking, Personnel Psychology, 43, 4, pp. 695-716, (1990); 
Ashton M.C., Lee K., The HEXACO-60: A short measure of the major dimensions of personality, Journal of Personality Assessment, 91, 4, pp. 340-345, (2009); 
Benjamini Y., Hochberg Y., Controlling the false discovery rate: A practical and powerful approach to multiple testing, Journal of the Royal Statistical Society. Series B (Methodological), 57, 1, pp. 289-300, (1995); 
Chan J.C., Response-order effects in Likert-type scales, Educational and Psychological Measurement, 51, 3, pp. 531-540, (1991); 
Chen F.F., Sensitivity of goodness of fit indexes to lack of measurement invariance, Structural Equation Modeling: A Multidisciplinary Journal, 14, 3, pp. 464-504, (2007); 
Christian L.M., Parsons N.L., Dillman D.A., Designing scalar questions for web surveys, Sociological Methods & Research, 37, 3, pp. 393-425, (2009); 
Cohen J., Statistical power analysis for the behavioral sciences, (1988); 
Cohen J., The earth is round (p<.05), American Psychologist, 49, 12, pp. 997-1003, (1994); 
Desimone J.A., Harms P.D., Desimone A.J., Best practice recommendations for data screening, Journal of Organizational Behavior, 36, 2, pp. 171-181, (2015); 
Diedenhofen B., Musch J., cocron: A web interface and R package for the statistical comparison of Cronbach’s alpha coefficients, International Journal of Internet Science, 11, 1, pp. 51-60, (2016); 
Dunn A.M., Heggestad E.D., Shanock L.R., Theilgard N., Intra-individual response variability as an indicator of insufficient effort responding: Comparison to other indicators and relationships with individual differences, Journal of Business and Psychology, 33, 1, pp. 105-121, (2018); 
Fleiss J.L., The statistical basis of meta-analysis, Statistical Methods in Medical Research, 2, 2, pp. 121-145, (1993); 
Garbarski D., Schaeffer N.C., Dykema J., The effects of features of survey measurement on self-rated health: Response option order and scale orientation, Applied Research in Quality of Life, 14, pp. 545-560, (2019); 
Gummer T., Kunz T., Using only numeric labels instead of verbal labels: Stripping rating scales to their bare minimum in web surveys, Social Science Computer Review, 39, 5, pp. 1003-1029, (2021); 
Hakstian A.R., Farrell S., Tweed R.G., The assessment of counterproductive tendencies by means of the California Psychological Inventory, International Journal of Selection and Assessment, 10, 1-2, pp. 58-86, (2002); 
Hohne J.K., Krebs D., Scale direction effects in agree/disagree and item-specific questions: A comparison of question formats, International Journal of Social Research Methodology, 21, 1, pp. 91-103, (2018); 
Holbrook A., Response order effects, Encyclopedia of survey research methods, 1, pp. 754-756, (2008); 
Holtrop D., Born M., de Vries A., de Vries R.E., A matter of context: A comparison of two types of contextualized measures, Personality and Individual Differences, 68, pp. 234-240, (2014); 
Hu J., Horizontal or vertical? The effects of visual orientation of categorical response options on survey responses in web surveys, Social Science Computer Review, 38, 6, pp. 779-792, (2020); 
Hu L., Bentler P.M., Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6, 1, pp. 1-55, (1999); 
Jackson D.N., Jackson Vocational Interest Survey manual, (1977); 
Jebb A.T., Ng V., Tay L., A review of key Likert scale development advances: 1995-2019, Frontiers in Psychology, 12, (2021); 
Johnson J., Ascertaining the validity of individual protocols from web-based personality inventories, Journal of Research in Personality, 39, 1, pp. 103-129, (2005); 
Kam C.C.S., Careless responding threatens factorial analytic results and construct validity of personality measure, Frontiers in Psychology, 10, (2019); 
Kim Y., Dykema J., Stevenson J., Black P., Moberg D.P., Straightlining: Overview of measurement, comparison of indicators, and effects in mail-web mixed-mode surveys, Social Science Computer Review, 37, 2, pp. 214-233, (2019); 
Kline R.B., Principles and practices of structural equation modeling, (2011); 
Krebs D., Bachner Y.G., Effects of rating scale direction under the condition of different reading direction, Methods, Data, Analyses, 12, 1, pp. 105-126, (2018); 
Krebs D., Hoffmeyer-Zlotnik J.H.P., Positive or negative first? Effects of the order of answering categories on response behavior, Methodology, 6, 3, pp. 118-127, (2010); 
Kuncel N.R., Crede M., Thomas L.L., The validity of self-reported grade point averages, class ranks, and test scores: A meta-analysis and review of the literature, Review of Educational Research, 75, 1, pp. 63-82, (2005); 
Lai K., Green S.B., The problems with having two watches: Assessment of fit when RMSEA and CFI disagree, Multivariate Behavioral Research, 51, 2-3, pp. 220-239, (2016); 
Leppink J., Paas F., Van der Vleuten C.P.M., Van Gog T., Merrienboer J.J.G., Development of an instrument for measuring different types of cognitive load, Behavior Research Methods, 45, 4, pp. 1058-1072, (2013); 
Li C., Confirmatory factor analysis with ordinal data: Comparing robust maximum likelihood and diagonally weighted least squares, Behavior Research Methods, 48, pp. 936-949, (2016); 
Likert R., A technique for the measurement of attitudes, (1932); 
Malhotra N., Completion time and response order effects in web surveys, Public Opinion Quarterly, 72, 5, pp. 914-934, (2008); 
Marjanovic Z., Holden R., Struthers W., Cribbie R., Greenglass E., The inter-item standard deviation (ISD): An index that discriminates between conscientious and random responders, Personality and Individual Differences, 84, pp. 79-83, (2015); 
Marsh H.W., Hau K., Wen Z., In search of golden rules: Comment on hypothesis-testing approaches to setting cutoff values for fit indexes and dangers in overgeneralizing Hu and Bentler’s (1999) findings, Structural Equation Modeling, 11, 3, pp. 320-341, (2004); 
Meade A.W., Craig S.B., Identifying careless responses in survey data, Psychological Methods, 17, 3, pp. 437-455, (2012); 
Menold N., Response bias and reliability in verbal agreement rating scales: Does polarity and verbalization of the middle category matter?, Social Science Computer Review, 39, 1, pp. 130-147, (2021); 
Nicolaou A.I., Masoner M.M., Sample size requirements in structural equation models under standard conditions, International Journal of Accounting Information Systems, 14, 4, pp. 256-274, (2013); 
Nunnally J.C., Bernstein I.H., Psychometric theory, (1994); 
Rammstedt B., Krebs D., Does response scale format affect the answering of personality scales? Assessing the Big Five dimensions of personality with different response scales in a dependent sample, European Journal of Psychological Assessment, 23, 1, pp. 32-38, (2007); 
Rosseel Y., lavaan: An R Package for structural equation modeling, Journal of Statistical Software, 48, 2, pp. 1-36, (2012); 
Smither J.W., Reilly R.R., Millsap R.E., Pearlman K., Stoffey R.W., Applicant reactions to selection procedures, Personnel Psychology, 46, 1, pp. 49-76, (1993); 
Spratto E.M., Leventhal B.C., Bandalos D.L., Seeing the forest and the trees: Comparison of two IRTree models to investigate the impact of full versus endpoint-only response option labelling, Educational and Psychological Measurement, 81, 1, pp. 39-60, (2021); 
Steinberg L., Rogers A., Changing the scale: The effect of modifying response scale labels on the measurement of personality and affect, Multivariate Behavioral Research; 
Terentev E., Maloshonok N., The impact of response options ordering on respondents’ answers to rating questions: Results of two experiments, International Journal of Social Research Methodology, 22, 2, pp. 179-198, (2019); 
Wanous J.P., Reichers A.E., Estimating the reliability of a single-item measure, Psychological Reports, 78, 2, pp. 631-634, (1996); 
Weaver B., Wuensch K.L., SPSS and SAS programs for comparing Pearson correlations and OLS regression coefficients, Behavior Research Methods, 45, pp. 880-895, (2013); 
Weigold A., Weigold I.K., Dykema S.A., Drakeford N.M., Completing surveys with different item formats: Testing equivalence, Social Science Computer Review, 39, 6, pp. 1179-1202, (2021); 
Weng L.J., Cheng C.P., Effects of response order on Likert-type scales, Educational and Psychological Measurement, 60, 6, pp. 908-924, (2000); 
Wiechmann D., Ryan A.M., Reactions to computerized testing in selection contexts, International Journal of Selection and Assessment, 11, 2-3, pp. 215-229, (2003); 
Wolf E.J., Harrington K.M., Clark S.L., Miller M.W., Sample size requirements for structural equation models: An evaluation of power, bias, and solution propriety, Educational and Psychological Measurement, 73, 6, pp. 913-934, (2013); 
Yentes R.D., Wilhelm F., careless: Procedures for computing indices of careless responding, (2018); 
Zeglovits E., Schwarzer S., Presentation matters: How mode effects in item non-response depend on the presentation of response options, International Journal of Social Research Methodology, 19, 2, pp. 191-203, (2016); 
Zhang X., Savalei V., Improving the factor structure of psychological scales: The expanded format as an alternative to the Likert scale format, Educational and Psychological Measurement, 76, 3, pp. 357-386, (2016)#FRF#
