#ITI#Using Differential Item Functioning to Test for Interrater Reliability in Constructed Response Items#FTI#
#IRE# The purpose of this study was to investigate a new way of evaluating interrater reliability that can allow one to determine if two raters differ with respect to their rating on a polytomous rating scale or constructed response item. Specifically, differential item functioning (DIF) analyses were used to assess interrater reliability and compared with traditional interrater reliability measures. Three different procedures that can be used as measures of interrater reliability were compared: (1) intraclass correlation coefficient (ICC), (2) Cohen’s kappa statistic, and (3) DIF statistic obtained from Poly-SIBTEST. The results of this investigation indicated that DIF procedures appear to be a promising alternative to assess the interrater reliability of constructed response items, or other polytomous types of items, such as rating scales. Furthermore, using DIF to assess interrater reliability does not require a fully crossed design and allows one to determine if a rater is either more severe, or more lenient, in their scoring of each individual polytomous item on a test or rating scale.#FRE#
#IPC# classical test theory; constructed response items; differential item functioning; interrater reliability; rater severity#FPC#
#IRF# Bolt D.M., A Monte Carlo comparison of parametric and nonparametric polytomous DIF detection methods, Applied Measurement in Education, 15, pp. 113-141, (2002); 
Chang H., Mazzeo J., Roussos L., Detecting DIF for polytomously scored items: An adaptation of the SIBTEST procedure, Journal of Educational Measurement, 33, pp. 333-353, (1996); 
Cicchetti D.V., Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology, Psychological Assessment, 6, pp. 284-290, (1994); 
Cohen J., A coefficient of agreement for nominal scales, Educational and Psychological Measurement, 20, pp. 37-46, (1960); 
Frisbie D., NCME Instructional module reliability of scores from teacher-made tests, Instructional Topics in Educational Measurement, Module 3, pp. 55-65, (1988); 
Gierl M., Using dimensionality-based DIF analyses to identify and interpret constructs that elicit group differences, Educational Measurement: Issues and Practices, 24, 1, pp. 3-14, (2005); 
Hidalgo M.D., Lopez-Pina J.A., Differential item functioning detection and effect size: A comparison between logistic regression and Mantel-Haenszel procedures, Educational and Psychological Measurement, 64, pp. 903-915, (2004); 
Howell D.C., Statistical methods for psychology, (2010); 
Jiang H., Stout W., Improve Type 1 error control and reduce estimation bias for DIF detection using SIBTEST, Journal of Educational and Behavioral Statistics, 23, pp. 291-322, (1998); 
Landis J.R., Koch G.G., The measurement of observer agreement for categorical data, Biometrics, 33, pp. 159-174, (1977); 
Lane S., Stone C.A., Performance assessment, Educational Measurement, pp. 387-431, (2006); 
Mehrens W.A., Using performance assessment for accountability purposes, Educational Measurement: Issues and Practice, 2, 1, pp. 3-9, (1992); 
Miller T.B., Using differential item functioning to test for inter-rater reliability in constructed response items, (2015); 
Patz R.J., Junker B.W., Johnson M.S., Mariano M., Louis T., The hierarchical rater model for rated test items and its application to large-scale educational assessment data, Journal of Educational and Behavioral Statistics, 27, pp. 341-384, (2002); 
Penfield R.D., Gattamora K., Childs R.A., An NCME instructional module on using differential step functioning to refine the analysis of DIF on polytomous items, Educational Measurement: Issues and Practice, 28, 1, pp. 38-49, (2009); 
Shealy R., Stout W., A model-based standardization approach that separates true bias/DIF from group ability differences and detects test bias/DTF as well as item bias/DIF, Psychometrika, 58, pp. 159-194, (1993); 
Shrout P.E., Fleiss J.L., Intraclass correlations: Uses in assessing rater reliability, Psychological Bulletin, 86, pp. 420-428, (1979); 
Uebersax J., Kappa coefficient: A critical appraisal, (2010); 
Walker C.M., What’s the DIF? Why differential analyses are an important part of instrument development and validation, Journal of Psychoeducational Assessment, 29, pp. 364-376, (2011); 
Welch C.J., Item and prompt development in performance testing, Handbook of test development, pp. 303-327, (2006)#FRF#
