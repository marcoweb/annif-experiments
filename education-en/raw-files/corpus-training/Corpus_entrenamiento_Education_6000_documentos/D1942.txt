#ITI#Exploring Gender Differences in Computational Thinking Learning in a VR Classroom: Developing Machine Learning Models Using Eye-Tracking Data and Explaining the Models#FTI#
#IRE# Understanding existing gender differences in the development of computational thinking skills is increasingly important for gaining valuable insights into bridging the gender gap. However, there are few studies to date that have examined gender differences based on the learning process in a realistic classroom context. In this work, we aim to investigate gender classification using students’ eye movements that reflect temporal human behavior during a computational thinking lesson in an immersive VR classroom. We trained several machine learning classifiers and showed that students’ eye movements provide discriminative information for gender classification. In addition, we employed a Shapley additive explanation (SHAP) approach for feature selection and further model interpretation. The classification model trained with the selected (best) eye movement feature set using SHAP achieved improved performance with an average accuracy of over 70 %. The SHAP values further explained the classification model by identifying important features and their impacts on the model output, namely gender. Our findings provide insights into the use of eye movements for in-depth investigations of gender differences in learning activities in VR classroom setups that are ecologically valid and may provide clues for providing personalized learning support and tutoring in such educational systems or optimizing system design#FRE#
#IPC# Computational thinking; Explainable AI; Eye movements; Gender classification; Machine learning; Virtual reality#FPC#
#IRF# Abdi Sargezeh B., Tavakoli N., Daliri M.R., Gender-based eye movement differences in passive indoor picture viewing: An eye-tracking study, Physiology & Behavior, 206, pp. 43-50, (2019); 
Agtzidis I., Startsev M., Dorr M., 360-degree video gaze behaviour: A ground-truth data set and a classification algorithm for eye movements, Proceedings of the 27Th ACM International Conference on Multimedia, pp. 1007-1015, (2019); 
Al Zaidawi S.M.K., Prinzler M.H., Schroder C., Et al., Gender classification of prepubescent children via eye movements with reading stimuli. In: Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 1–6). ACM: New York, ICMI ’20 Companion, (2020); 
Angeli C., Valanides N., Developing young children’s computational thinking with educational robotics: An interaction effect between gender and scaffolding strategy, Computers in Human Behavior, 105, 105, (2020); 
Appel T., Scharinger C., Gerjets P., Et al., Cross-subject workload classification using pupil-related measures. In: Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications. ACM: New York, ETRA ’18, (2018); 
Appel T., Sevcenko N., Wortha F., Et al., Predicting cognitive load in an emergency simulation based on behavioral and physiological measures, : 2019 International Conference on Multimodal Interaction (, pp. 154-163, (2019); 
Ashraf H., Sodergren M.H., Merali N., Et al., Eye-tracking technology in medical education: A systematic review, Medical Teacher, 40, 1, pp. 62-69, (2018); 
Atmatzidou S., Demetriadis S., Advancing students’ computational thinking skills through educational robotics: A study on age and gender relevant differences, Robotics and Autonomous Systems, 75, pp. 661-670, (2016); 
Baser M., Attitude, gender and achievement in computer programming, Middle East Journal of Scientific Research, 14, pp. 248-255, (2013); 
Bell T., Andreae P., Robins A., A case study of the introduction of computer science in NZ schools, ACM Trans Comput Educ, 14, 2, (2014); 
Berkovsky S., Taib R., Koprinska I., Et al., Detecting personality traits using eye-tracking data, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-12, (2019); 
Bozkir E., Geisler D., Kasneci E., Assessment of driver attention during a safety critical situation in VR to generate VR-based training, ACM Symposium on Applied Perception, 2019, (2019); 
Bozkir E., Gunlu O., Fuhl W., Et al., Differential privacy for eye tracking with temporal correlations, Plos One, 16, 8, pp. 1-22, (2021); 
Bozkir E., Stark P., Gao H., Et al., Exploiting object-of-interest information to understand attention in VR classrooms, 2021 IEEE Virtual Reality and 3D User Interfaces (VR), pp. 597-605, (2021); 
Bulling A., Ward J.A., Gellersen H., Et al., Eye movement analysis for activity recognition using electrooculography, IEEE Transactions on Pattern Analysis and Machine Intelligence, 33, 4, pp. 741-753, (2011); 
Castner N., Appel T., Eder T., Et al., Pupil diameter differentiates expertise in dental radiography visual search, Plos One, 15, 5, pp. 1-19, (2020); 
Casu A., Spano L.D., Sorrentino F., Et al., RiftArt: Bringing masterpieces in the classroom through immersive virtual reality, Smart Tools and Apps for Graphics - Eurographics Italian Chapter Conference. the Eurographics Association, pp. 77-84, (2015); 
Chalmers C., Robotics and computational thinking in primary school, International Journal of Child-Computer Interaction, 17, pp. 93-100, (2018); 
Chien K.P., Tsai C.Y., Chen H.L., Et al., Learning differences and eye fixation patterns in virtual and physical science laboratories, Computers & Education, 82, pp. 191-201, (2015); 
Cryer A., Kapellmann-Zafra G., Abrego-Hernandez S., Et al., Advantages of virtual reality in the teaching and training of radiation protection during interventions in harsh environments, 2019 24Th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA) (Pp. 784–789). IEEE, (2019); 
Datta A., Sen S., Zick Y., Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems, 2016 IEEE Symposium on Security and Privacy (SP) (Pp, pp. 598-617, (2016); 
Dumais S.T., Buscher G., Cutrell E., Individual differences in gaze patterns for web search. In: Proceedings of the third symposium on Information interaction in context (pp. 185–194). ACM, New York, Iiix ’10, (2010); 
Eivazi S., Bednarik R., Predicting problem-solving behavior and performance levels from visual attention data, Proceedings of 2Nd Workshop on Eye Gaze in Intelligent Human Machine Interaction at IUI, pp. 9-16, (2011); 
Gao H., Bozkir E., Hasenbein L., Et al., Digital transformations of classrooms in virtual reality, In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems., (2021); 
Garcia-Penalvo F.J., Mendes A.J., Exploring the computational thinking effects in pre-university education, Computers in Human Behavior, 80, pp. 407-411, (2018); 
Grodotzki J., Ortelt T.R., Tekkaya A.E., Remote and virtual labs for engineering education 4.0: Achievements of the ELLI project at the TU dortmund university, Procedia Manufacturing, 26, pp. 1349-1360, (2018); 
Hernandez-de Menendez M., Guevara A.V., Morales-Menendez R., Virtual reality laboratories: A review of experiences, International Journal on Interactive Design and Manufacturing (IJIDeM), 13, 3, pp. 947-966, (2019); 
Hirt C., Eckard M., Kunz A., Stress generation and non-intrusive measurement in virtual environments using eye tracking, Journal of Ambient Intelligence and Humanized Computing, 11, 1, pp. 1-13, (2020); 
Holmqvist K., Nystrom M., Andersson R., Et al., Eye Tracking: A Comprehensive Guide to Methods and Measures, (2011); 
Hoppe S., Loetscher T., Morey S.A., Et al., Eye movements during everyday behavior predict personality traits, Frontiers in Human Neuroscience, 12, (2018); 
Hsu T.C., Chang S.C., Hung Y.T., How to learn and how to teach computational thinking: Suggestions based on a review of the literature, Computers & Education, 126, pp. 296-310, (2018); 
Hwang Y.M., Lee K.C., Using an eye-tracking approach to explore gender differences in visual attention and shopping attitudes in an online shopping environment, International Journal of Human-Computer Interaction, 34, 1, pp. 15-24, (2018); 
Kasneci E., Kasneci G., Trautwein U., Et al., Do your eye movements reveal your performance on an iq test? A study linking eye movements and socio-demographic information to fluid intelligence, Plos One, 17, 3, pp. 1-23, (2022); 
Kong S.C., Chiu M.M., Lai M., A study of primary school students’ interest, collaboration attitude, and programming empowerment in computational thinking education, Computers & Education, 127, pp. 178-189, (2018); 
Liao H., Dong W., An exploratory study investigating gender effects on using 3D maps for spatial orientation in wayfinding, ISPRS International Journal of Geo-Information, 6, 3, (2017); 
Lin F., Wu Y., Zhuang Y., Et al., Human gender classification: A review, International Journal of Biometrics, 8, 3-4, pp. 275-300, (2016); 
Lundberg S.M., Lee S.I., A unified approach to interpreting model predictions. In: Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 4768–4777). Curran Associates Inc., Red Hook, NIPS’17, (2017); 
Lundberg S.M., Erion G., Chen H., Et al., From local explanations to global understanding with explainable AI for trees, Nature Machine Intelligence, 2, 1, pp. 56-67, (2020); 
Mathot S., Fabius J., Van Heusden E., Et al., Safe and sensible preprocessing and baseline correction of pupil-size data, Behavior Research Methods, 50, 1, pp. 94-106, (2018); 
McGuire L., Mulvey K.L., Goff E., Et al., STEM gender stereotypes from early childhood through adolescence at informal science centers, Journal of Applied Developmental Psychology, 67, 101, (2020); 
Mercer Moss F.J., Baddeley R., Canagarajah N., Eye movements to natural images as a function of sex and personality, Plos One, 7, 11, pp. 1-9, (2012); 
Molina A.I., Oscar N., Ortega M., Et al., Evaluating multimedia learning materials in primary education using eye tracking, Computer Standards & Interfaces, 59, pp. 45-60, (2018); 
Negi S., Mitra R., Fixation duration and the learning process: An eye tracking study with subtitled videos, Journal of Eye Movement Research, 13, 6, (2020); 
Nourbakhsh I., Hamner E., Crowley K., Et al., Formal measures of learning in a secondary school mobile robotics course, IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA ’04. 2004 (Vol 2, pp. 1831-1836, (2004); 
Obaidellah U., Haek M.A., Evaluating gender difference on algorithmic problems using eye-tracker. In: Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications. ACM, New York,Etra ’18, (2018); 
Papavlasopoulou S., Sharma K., Giannakos M.N., Coding activities for children: Coupling eye-tracking with qualitative data to investigate gender differences, Computers in Human Behavior, 105, 105, (2020); 
Polat E., Hopcan S., Kucuk S., Et al., A comprehensive assessment of secondary school students’ computational thinking skills, British Journal of Educational Technology, 52, 5, (2021); 
Raptis G.E., Fidas C.A., Avouris N.M., On implicit elicitation of cognitive strategies using gaze transition entropies in pattern recognition tasks. In: Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (pp. 1993–2000). ACM, New York, CHI EA ’17, (2017); 
Reilly D., Neumann D.L., Andrews G., Gender differences in spatial ability: Implications for STEM education and approaches to reducing the gender gap for parents and educators (pp, 195–224), (2017); 
Salvucci D.D., Goldberg J.H., Identifying fixations and saccades in eye-tracking protocols, In: Proceedings of the 2000 Symposium on Eye Tracking Research & Applications (Pp, pp. 71-78, (2000); 
Sammaknejad N., Pouretemad H., Eslahchi C., Et al., Gender classification based on eye movements: A processing effect during passive face viewing, Advances in Cognitive Psychology, 13, 3, pp. 232-240, (2017); 
Savitzky A., Golay M.J.E., Smoothing and differentiation of data by simplified least squares procedures, Analytical Chemistry, 36, pp. 1627-1639, (1964); 
Sentance S., Csizmadia A., Teachers’ perspectives on successful strategies for teaching computing in school, IFIP TC3 Working Conference 2015: A New Culture of Learning: Computing and Next Generations, (2015); 
Seo S.H., Kim E., Mundy P., Et al., Joint attention virtual classroom: A preliminary study, Psychiatry Investigation, 16, pp. 292-299, (2019); 
Seow P., Looi C.K., How M.L., Et al., Educational Policy and Implementation of Computational Thinking and Programming: Case Study of Singapore, pp. 345-361, (2019); 
Shapley L.S., A value for n-person games, Contributions to the Theory of Games, 2, pp. 307-317, (1953); 
Steil J., Hagestedt I., Huang M.X., Et al., Privacy-aware eye tracking using differential privacy. In: Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications. ACM, New York, ETRA ’19, (2019); 
Strohmaier A.R., MacKay K.J., Obersteiner A., Et al., Eye-tracking methodology in mathematics education research: A systematic literature review, Educational Studies in Mathematics, 104, 2, pp. 147-200, (2020); 
Strumbelj E., Kononenko I., Explaining prediction models and individual predictions with feature contributions, Knowledge and information systems, 41, 3, pp. 647-665, (2014); 
Sullivan A., Bers M., Girls, boys, and bots: Gender differences in young children’s performance on robotics and programming tasks, Journal of Information Technology Education: Innovations in Practice, 15, pp. 145-165, (2016); 
Sundararajan M., Najmi A., The many shapley values for model explanation. In: I.I.I. HD, A. Singh (eds) Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research (Vol. 119. pp 9269–9278), PMLR, (2020); 
Wang M.T., Degol J.L., Gender gap in science, technology, engineering, and mathematics (STEM): Current knowledge, implications for practice, policy, and future directions, Educational Psychology Review, 29, 1, pp. 119-140, (2017); 
Wing J., Research notebook: Computational thinking–what and why, The Link Magazine, 6, pp. 20-23, (2011); 
Yoshida Y., Ohwada H., Mizoguchi F., Et al., Classifying cognitive load and driving situation with machine learning, International Journal of Machine Learning and Computing, 4, pp. 210-215, (2014); 
Zhou F., Yang X.J., de Winter J.C.F., Using eye-tracking data to predict situation awareness in real time during takeover transitions in conditionally automated driving, IEEE Transactions on Intelligent Transportation Systems, pp. 1-12, (2021); 
Zhou J., Sun J., Chen F., Et al., Measurable decision making with GSR and pupillary analysis for intelligent user interface, ACM Trans Comput-Hum Interact, 21, 6, (2015)#FRF#
