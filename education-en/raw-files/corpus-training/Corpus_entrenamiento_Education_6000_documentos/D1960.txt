#ITI#Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers#FTI#
#IRE# Autograding short textual answers has become much more feasible due to the rise of NLP and the increased availability of question-answer pairs brought about by a shift to online education. Autograding performance is still inferior to human grading. The statistical and black-box nature of state-of-the-art machine learning models makes them untrustworthy, raising ethical concerns and limiting their practical utility. Furthermore, the evaluation of autograding is typically confined to small, monolingual datasets for a specific question type. This study uses a large dataset consisting of about 10 million question-answer pairs from multiple languages covering diverse fields such as math and language, and strong variation in question and answer syntax. We demonstrate the effectiveness of fine-tuning transformer models for autograding for such complex datasets. Our best hyperparameter-tuned model yields an accuracy of about 86.5%, comparable to the state-of-the-art models that are less general and more tuned to a specific type of question, subject, and language. More importantly, we address trust and ethical concerns. By involving humans in the autograding process, we show how to improve the accuracy of automatically graded answers, achieving accuracy equivalent to that of teaching assistants. We also show how teachers can effectively control the type of errors made by the system and how they can validate efficiently that the autograder’s performance on individual exams is close to the expected performance#FRE#
#IPC# Autograding; BERT; Multi-lingual; NLP; Question-answering; Validation#FPC#
#IRF# Akhtar N., Mian A., Threat of adversarial attacks on deep learning in computer vision: A survey, IEEE Access, 6, (2018); 
Amatriain X., Basilico J., Netflix recommendations: Beyond the 5 stars., (2012); 
Attali Y., Powers D., Freedman M., Harrison M., Obetz S., Automated scoring of short-answer open-ended GRE subject test items, ETS Research Report Series, 2008, 1, pp. i-22, (2008); 
Azad S., Chen B., Fowler M., West M., Zilles C., Strategies for deploying unreliable AI graders in high-transparency high-stakes exams, In: International Conference on Artificial Intelligence in Education, (2020); 
Baral S., Botelho A., Erickson J., Benachamardi P., Heffernan N., Improving automated scoring of student open responses in mathematics, In: Proceedings of the International Conference on Educational Data Mining, (2021); 
Baroni M., Dinu G., Kruszewski G., Don’t count, predict! A systematic comparison of context-counting vs, Context-Predicting Semantic Vectors, (2014); 
Barz B., Denzler J., Deep learning on small datasets without pre-training using cosine loss. IEEE Winter Conf. Appl. Comput, Vision (WACV), (2020); 
Basu S., Jacobs C., Vanderwende L., Powergrading: a clustering approach to amplify human effort for short answer grading, Transactions of the Association for Computational Linguistics, 1, pp. 391-402, (2013); 
Bin L., Jun L., Jian-Min Y., Qiao-Ming Z., Automated essay scoring using the KNN algorithm, Proceedings of the Int. Conf. Computer Science Software Engineering (CSSE), 1, (2008); 
Burrows S., Gurevych I., Stein B., The eras and trends of automatic short answer grading, International Journal of Artificial Intelligence in Education, 25, 1, pp. 60-117, (2015); 
Burstein J., Leacock C., Swartz R., Automated evaluation of essays and short answers, In: Proceedings of the International Computer Assisted Assessment Conference, (2001); 
White Paper on Artificial Intelligence., (2020); 
Cornell D.G., Krosnick J.A., Chang L., Student reactions to being wrongly informed of failing a high-stakes test: The case of the Minnesota basic standards test, Educational Policy, 20, 5, (2006); 
Devlin J., Chang M.W., Lee K., Toutanova K., Bert: Pre-training of deep bidirectional transformers for language understanding, In: Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (2019); 
Dietvorst B.J., Simmons J.P., Massey C., Algorithm aversion: People erroneously avoid algorithms after seeing them err, Journal of Experimental Psychology: General, 144, 1, (2015); 
Dikli S., The nature of automated essay scoring feedback, Calico Journal, 28, 1, pp. 99-134, (2010); 
Dzindolet M.T., Peterson S.A., Pomranky R.A., Pierce L.G., Beck H.P., The role of trust in automation reliance, International Journal of Human-Computer Studies, 58, 6, (2003); 
Ezen-Can A., A Comparison of Lstm and Bert for Small Corpus. Arxiv, 2009, (2020); 
Feng F., Yang Y., Cer D., Arivazhagan N., Wang W., Language-Agnostic BERT Sentence Embedding. Arxiv, 2007, (2020); 
Fooling automatic short answer grading systems, In: International Conference on Artificial Intelligence in Education, (2020); 
A comparative study of fairness-enhancing interventions in machine learning, In: Proceedings of the Conference on Fairness, Accountability, and Transparency, (2019); 
Hoskens M., Wilson M., Real-time feedback on rater drift in constructed-response items: An example from the golden state examination, Journal of Educational Measurement, 38, 2, pp. 121-145, (2001); 
). Attitudes surrounding an imperfect AI autograder, In: Proc. of CHI Conference on Human Factors in Computing Systems, (2021); 
Kenton J.D.M.W.C., Toutanova L.K., Bert: Pre-training of deep bidirectional transformers for language understanding, In: Proceedings of NAACL-HLT, (2019); 
Kumar S., Chakrabarti S., Roy S., Earth mover’s distance pooling over siamese LSTMs for Automatic short answer grading, International Joint Conference on Artificial Intelligence, (2017); 
Kumar V., Boulanger D., Explainable automated essay scoring: Deep learning really has pedagogical value, : Frontiers in Education, 5, (2020); 
Lan Z., Chen M., Goodman S., Gimpel K., Sharma P., Soricut R., Albert: A lite bert for self-supervised learning of language representations, In: International Conference on Learning Representations, (2019); 
Landauer T.K., McNamara D.S., Dennis S.W., Handbook of latent semantic analysis, Taylor & Francis, (2013); 
Madnani N., Cahill A., Automated scoring: Beyond natural language processing, In: Proceedings of the International Conference on Computational Linguistics, (2018); 
Building better open-source tools to support fairness in automated scoring, : Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, (2017); 
Mayfield E., Black A.W., Should You Fine-Tune Bert for Automated Essay Scoring? In: Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications, (2020); 
Mizumoto T., Ouchi H., Isobe Y., Reisert P., Nagata R., Sekine S., Inui K., Analytic score prediction and justification identification in automated short answer scoring, In: Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications, (2019); 
Nagata R., Nakatani K., Evaluating performance of grammatical error detection to maximize learning effect, In: Proceedings of the 23Rd International Conference on Computational Linguistics: Posters, pp. 894-900, (2010); 
Pado U., Question difficulty–how to estimate without norming, how to use for automated grading, Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications, (2017); 
Page E.B., Statistical and linguistic strategies in the computer grading of essays, Conference Internationale Sur Le Traitement Automatique Des Langues, (1967); 
Rajpurkar P., Jia R., Liang P., Know what you don’t know: Unanswerable questions for squad, In: Proceedings of the 56Th Annual Meeting of the Association for Computational Linguistics, 2, pp. 784-789, (2018); 
Reimers N., Gurevych I., Making monolingual sentence embeddings multilingual using knowledge distillation, In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), (2020); 
Reimers N., Gurevych I., Reimers N., Gurevych I., Thakur N., Reimers N., Daxenberger J., Gurevych I., Reimers N., Gurevych I., Et al., Sentence-bert: Sentence embeddings using siamese bert-networks, In: Proceedings of the Conference on Empirical Methods in Natural Language Processing, (2019); 
Rhoades K., Madaus G., Errors in standardized tests: A systemic problem, The National Board on Educational Testing and Public Policy., (2003); 
Investigating neural architectures for short answer scoring, In: Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications, (2017); 
Rolnick D., Veit A., Belongie S., Shavit N., Deep learning is robust to massive label noise, Arxiv, 1705, (2017); 
Schneider J., Human-to-AI coach: Improving human inputs to AI systems, International Symposium on Intelligent Data Analysis, pp. 431-443, (2020); 
Schneider J., Handali J., Personalized explanation in machine learning: A conceptualization, In: European Conference on Information Systems, (2019); 
Schneider J., Handali J.P., Vom Brocke J., Increasing trust in (Big) data analytics, In: International Conference on Advanced Information Systems Engineering (Pp., pp. 70-84, (2018); 
Schneider J., Vlachos M., Personalization of deep learning, In: 3Rd International Data Science Conference (Idsc), (2020); 
Shehab A., Faroun M., Rashad M., An automatic Arabic essay grading system based on text similarity algorithms, International Journal of Advanced Computer Science and Applications, 9, 3, (2018); 
Smith D.J., Reliability, maintainability and risk: Practical methods for engineers, Butterworth-Heinemann, (2017); 
Sultan M.A., Salazar C., Sumner T., Fast and easy short answer grading with high accuracy, In: Proc. of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (2016); 
Sung C., Dhamecha T., Saha S., Ma T., Reddy V., Arora R., Pre-training bert on domain resources for short answer grading, In: Proceedings of the Conf. on Empirical Methods in Natural Language Processing and the Int. Joint Conf. on Natural Language Processing (EMNLP-IJCNLP), (2019); 
Suzen N., Gorban A.N., Levesley J., Mirkes E.M., Automatic short answer grading and feedback using text mining methods, Procedia Computer Science, 169, pp. 726-743, (2020); 
Thorndike R.M., Thorndike-Christ T.M., Measurement and evaluation in psychology and education, Pearson, (2010); 
Uto M., Xie Y., Ueno M., Neural automated essay scoring incorporating handcrafted features, In: Proceedings of the International Conference on Computational Linguistics, (2020); 
Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need, In: Adv. Neural Inf. Process. Syst., (2017); 
Vittorini P., Menini S., Tonelli S., An ai-based system for formative and summative assessment in data science courses, International Journal of Artificial Intelligence in Education, pp. 1-27, (2020); 
Assessment2vec: Learning distributed representations of assessments to reduce marking workload, : International Conference on Artificial Intelligence in Education, (2021); 
Transformers: State-of-the-art natural language processing, : Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations, (2020); 
Yang R., Cao J., Wen Z., Wu Y., He X., Enhancing Automated Essay Scoring Performance via Fine-Tuning Pre-Trained Language Models with Combination of Regression and Ranking, (2020); 
Zhang W.E., Sheng Q.Z., Alhazmi A., Li C., Adversarial attacks on deep-learning models in natural language processing: A survey, ACM Transactions on Intelligent Systems and Technology (TIST), 11, 3, pp. 1-41, (2020)#FRF#
