#ITI#Assessing the Accuracy of Parameter Estimates in the Presence of Rapid Guessing Misclassifications#FTI#
#IRE# The presence of rapid guessing (RG) presents a challenge to practitioners in obtaining accurate estimates of measurement properties and examinee ability. In response to this concern, researchers have utilized response times as a proxy of RG and have attempted to improve parameter estimation accuracy by filtering RG responses using popular scoring approaches, such as the effort-moderated item response theory (EM-IRT) model. However, such an approach assumes that RG can be correctly identified based on an indirect proxy of examinee behavior. A failure to meet this assumption leads to the inclusion of distortive and psychometrically uninformative information in parameter estimates. To address this issue, a simulation study was conducted to examine how violations to the assumption of correct RG classification influences EM-IRT item and ability parameter estimation accuracy and compares these results with parameter estimates from the three-parameter logistic (3PL) model, which includes RG responses in scoring. Two RG misclassification factors were manipulated: type (underclassification vs. overclassification) and rate (10%, 30%, and 50%). Results indicated that the EM-IRT model provided improved item parameter estimation over the 3PL model regardless of misclassification type and rate. Furthermore, under most conditions, increased rates of RG underclassification were associated with the greatest bias in ability parameter estimates from the EM-IRT model. In spite of this, the EM-IRT model with RG misclassifications demonstrated more accurate ability parameter estimation than the 3PL model when the mean ability of RG subgroups did not differ. This suggests that in certain situations it may be better for practitioners to (a) imperfectly identify RG than to ignore the presence of such invalid responses and (b) select liberal over conservative response time thresholds to mitigate bias from underclassified RG.#FRE#
#IPC# IRT; item response theory; noneffortful responding; parameter estimation; rapid guessing; response times#FPC#
#IRF# Standards for educational and psychological testing, (2014); 
Benjamini Y., Hochberg Y., Controlling the false discovery rate: A practical and powerful approach to multiple testing, Journal of the Royal Statistical Society: Series B (Methodological), 57, 1, pp. 289-300, (1995); 
Chalmers R.P., Mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Cronbach L.J., Essentials of psychological testing, (1960); 
De Boeck P., Jeon M., An overview of models for response times and processes in cognitive tests, Frontiers in Psychology, 10, (2019); 
Debeer D., Buchholz J., Hartig J., Janssen R., Student, school, and country differences in sustained test-taking effort in the 2009 PISA reading assessment, Journal of Educational and Behavioral Statistics, 39, 6, pp. 502-523, (2014); 
DeMars C.E., Changes in rapid-guessing behavior over a series of assessments, Educational Assessment, 12, 1, pp. 23-45, (2007); 
DeMars C.E., Wise S.L., Can differential rapid-guessing behavior lead to differential item functioning?, International Journal of Testing, 10, 3, pp. 207-229, (2010); 
Goldhammer F., Measuring ability, speed, or both? Challenges, psychometric solutions, and what can be gained from experimental control, Measurement: Interdisciplinary Research and Perspectives, 13, 3-4, pp. 133-164, (2015); 
Goldhammer F., Martens T., Christoph G., Ludtke O., Test-taking engagement in PIAAC, (2016); 
Guo H., Rios J.A., Haberman S., Liu O.L., Wang J., Paek I., A new procedure for detection of students’ rapid guessing responses using response time, Applied Measurement in Education, 29, 3, pp. 173-183, (2016); 
Hulin C.L., Lissak R.I., Drasgow F., Recovery of two- and three-parameter logistic item characteristic curves: A Monte Carlo study, Applied Psychological Measurement, 6, 3, pp. 249-260, (1982); 
Kim S., Moses T., Investigating robustness of item response theory proficiency estimators to atypical response behaviors under two-stage multistage testing, (2016); 
Kuhfeld M., Soland J., Using assessment metadata to quantify the impact of test disengagement on estimates of educational effectiveness, Journal of Research on Educational Effectiveness, (2020); 
Kyllonen P.C., Zu J., Use of response time for measuring cognitive ability, Journal of Intelligence, 4, 14, pp. 1-29, (2016); 
Liu O.L., Rios J.A., Borden V., The effects of motivational instruction on college students’ performance on low-stakes assessment, Educational Assessment, 20, 2, pp. 79-94, (2015); 
Liu Y., Li Z., Liu H., Luo F., Modeling test-taking non-effort in MIRT models, Frontiers in Psychology, 10, (2019); 
Loken E., Beverly T., (2020); 
Meyer J.P., A mixture Rasch model with item response time components, Applied Psychological Measurement, 34, 7, pp. 521-538, (2010); 
Mittelhaeuser M.A., Beguin A.A., Sijtsma K., The effect of differential motivation on IRT linking, Journal of Educational Measurement, 52, 3, pp. 339-358, (2015); 
Osborne J.W., Blanchard M.R., Random responding from participants is a threat to the validity of social science research results, (2011); 
Pastor D.A., Ong T.Q., Strickman S.N., Patterns of solution behavior across items in low-stakes assessments, Educational Assessment, 24, 3, pp. 189-212, (2019); 
Penk C., Schipolowski S., Is it all about value? Bringing back the expectancy component to the assessment of test-taking motivation, Learning and Individual Differences, 42, pp. 27-35, (2015); 
R: A language and environment for statistical computing, (2020); 
Rios J.A., Guo H., Can culture be a salient predictor of test-taking engagement? An analysis of differential noneffortful responding on an international college-level assessment of critical thinking, Applied Measurement in Education, (2020); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of noneffortful responses on aggregated scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, 1, pp. 74-104, (2017); 
Rios J.A., Liu O.L., Bridgeman B., Identifying low-effort examinees on student learning outcomes assessment: A comparison of two approaches, New Directions for Institutional Research, 2014, 161, pp. 69-82, (2014); 
Rios J.A., Soland J., Parameter estimation accuracy of the effort-moderated IRT model under multiple assumption violations, Educational and Psychological Measurement, (2020); 
Schnipke D.L., Scrams D.J., Modeling response times with a two-state mixture model: A new method of measuring speededness, Journal of Educational Measurement, 34, 3, pp. 213-232, (1997); 
Schuster C., Yuan K.H., Robust estimation of latent ability in item response models, Journal of Educational and Behavioral Statistics, 36, 6, pp. 720-735, (2011); 
Silm G., Pedaste M., Taht K., The relationship between performance and test-taking effort when measured with self-report or time-based instruments: A meta-analytic review, Educational Research Review, 31, (2020); 
van Barneveld C., The effect of examinee motivation on test construction within an IRT framework, Applied Psychological Measurement, 31, 1, pp. 31-46, (2007); 
Wang C., Xu G., A mixture hierarchical model for response times and response accuracy, British Journal of Mathematical and Statistical Psychology, 68, 3, pp. 456-477, (2015); 
Wang C., Xu G., Shang Z., Kuncel N., Detecting aberrant behavior and item preknowledge: A comparison of mixture modeling method and residual method, Journal of Educational and Behavioral Statistics, 43, 4, pp. 469-501, (2018); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement: Issues and Practice, 36, 4, pp. 52-61, (2017); 
Wise S.L., The impact of test-taking disengagement on item content representation, Applied Measurement in Education, 33, 2, pp. 83-94, (2020); 
Wise S.L., DeMars C.E., An application of item response time: The effort-moderated IRT model, Journal of Educational Measurement, 43, 1, pp. 19-38, (2006); 
Wise S.L., DeMars C.E., A clarification of the effects of rapid guessing on coefficient α: A note on Attali’s “Reliability of speeded number-right multiple-choice tests., Applied Psychological Measurement, 33, 6, pp. 488-490, (2009); 
Wise S.L., DeMars C.E., Examinee noneffort and the validity of program assessment results, Educational Assessment, 15, 1, pp. 27-41, (2010); 
Wise S.L., Kingsbury G.G., Modeling student test-taking motivation in the context of an adaptive achievement test, Journal of Educational Measurement, 53, 1, pp. 86-105, (2016); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, 2, pp. 163-183, (2005); 
Wise S.L., Kuhfeld M.R., Using retest data to evaluate and improve effort-moderated scoring, Journal of Educational Measurement, (2020); 
Wise S.L., Ma L., (2012); 
Wise S.L., Ma L., Cronin J., Theaker R.A., (2013); 
Wise S.L., Soland J., Bo Y., The (non)impact of differential test taker engagement on aggregated scores, International Journal of Testing, 20, 1, pp. 57-77, (2020)#FRF#
