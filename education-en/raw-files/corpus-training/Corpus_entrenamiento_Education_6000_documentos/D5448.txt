#ITI#Are We Really Falling Behind? Comparing Key Indicators Across International and Local Standardised Tests for Australian High School Science#FTI#
#IRE#There has been a strong narrative in Australia of falling attainment in high school science, with much of the campaign informed by results from international standardised tests such as Programme for International Student Assessment (PISA), which shows a year-on-year decline in scientific literacy of Australian 15-year-old students. These results have been used to justify significant policy and curriculum reform, despite the known limitations of PISA and a lack of additional evidence to support this decline in other tests. In this paper, results from standardised tests administered in Australia will be compared to create a fulsome picture of attainment for high school science students. Reports include both the compilation of data from existing reports and new analyses. With the latest (2018/9) reports from PISA, Trends in International Mathematics and Science Study (TIMSS), and National Assessment Program for Scientific Literacy (NAP-SL) (an Australian test of Science Literacy) and data shared by the NSW Department of Education on ‘The Validation of Assessment for Learning and Individual Development’ (VALID) test for the years 2015, 2016, 2017, and 2018, this offers the most complete picture of student attainment in science to date. Results show that there are disagreements between tests on cohort achievement over time and distribution of attainment at different ‘proficiency levels’. These results suggest caution when using these key results from these tests to inform policy and pedagogy#FRE#
#IPC#Assessment; High school science; PISA; TIMSS#FPC#
#IRF#National assessment program science literacy 2018 public report, (2019); 
Araujo L., Saltelli A., Schnepf S.V., Do PISA data justify PISA-based education policy?, International Journal of Comparative Education and Development, 19, 1, pp. 20-34, (2017); 
Cizek G.J., Setting performance standards: Foundations, methods, and innovations, (2012); 
National STEM school education strategy: A comprehensive plan for science, technology, engineering and mathematics education in Australia, (2015); 
Fischbach A., Keller U., Preckel F., Brunner M., PISA proficiency scores predict educational outcomes, Learning and Individual Differences, 24, pp. 63-72, (2013); 
Froese-Germain B., The OECD, PISA and the impacts on educational policy, Canadian Teachers' Federation, NJ1, (2010); 
Hare J., Why Australia’s students keep falling behind, (2022); 
Harlow A., Jones A., Why students answer TIMSS science test items the way they do, Research in Science Education, 34, pp. 221-238, (2004); 
Haugsbakk G., From Sputnik to PISA shock–New technology and educational reform in Norway and Sweden, Education Inquiry, 4, 4, (2013); 
Hildebrand J., Our system is failing, (2023); 
Hopfenbeck T.N., Lenkeit J., El Masri Y., Cantrell K., Ryan J., Baird J.A., Lessons learned from PISA: A systematic review of peer-reviewed articles on the programme for international student assessment, Scandinavian Journal of Educational Research, 62, 3, pp. 333-353, (2018); 
Jerrim J., Has Peak PISA passed? An investigation of interest in International Large-Scale Assessments across countries and over time, European Educational Research Journal, (2023); 
Khalilzadeh J., Tasci A.D., Large sample size, significance level, and the effect size: Solutions to perils of using big data for academic research, Tourism Management, 62, pp. 89-96, (2017); 
Kane M.T., Explicating validity, Assessment in education: Principles, policy & practice., 23, 2, pp. 198-211, (2016); 
Liou P.Y., Bulut O., The effects of item format and cognitive domain on students’ science performance in TIMSS 2011, Research in science education, 50, 1, pp. 99-121, (2020); 
Nurturing wonder and igniting passion, designs for a new school curriculum: NSW Curriculum Review, NSW Education Standards Authority (NESA). (2020); 
PISA 2018 technical report, (2020); 
PISA 2018 results (volume 1): What students know and can do, (2019); 
Science, technology, engineering and mathematics: Australia’s future, Office of the Chief Scientist, (2014); 
Pons X., Fifteen years of research on PISA effects on education governance: A critical review, European Journal of Education, 52, 2, pp. 131-144, (2017); 
A smart move, (2015); 
Pulkkinen J., Rautopuro J., The correspondence between PISA performance and school achievement in Finland, International Journal of Educational Research, 114, (2022); 
Rindermann H., Baumeister A.E., Parents’ SES vs. parental educational behavior and children’s development: A reanalysis of the Hart and Risley study, Learning and individual differences, 37, pp. 133-138, (2015); 
Thomson S., De Bortoli L., Underwood C., Schmid M., PISA 2018: Reporting Australia’s results. Volume I student performance, Australian Council for Educational Research (ACER), (2019); 
Thomson S., Wernert N., Rodrigues S., O'Grady E., TIMSS 2019 Australia. Volume I: Student performance., (2020); 
Waldow F., What PISA did and did not do: Germany after the ‘PISA-shock’, European Educational Research Journal, 8, 3, pp. 476-483, (2009); 
Wiseman A.W., Policy responses to PISA in comparative perspective, PISA, power, and policy: The emergence of global educational governance, pp. 303-322, (2013); 
Wu M., A comparison of PISA and TIMSS 2003 achievement results in mathematics, Prospects, 39, pp. 33-46, (2009); 
Zhao Y., Two decades of havoc: A synthesis of criticism against PISA, Journal of Educational Change, 21, 2, pp. 245-266, (2020); 
Schools ABS, (2022); 
The 2030 skills scorecard: Bridging business, education, and the future of work, (2019)#FRF#
