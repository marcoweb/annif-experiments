#ITI#The Effect-Size Benchmark That Matters Most: Education Interventions Often Fail#FTI#
#IRE# It is a healthy exercise to debate the merits of using effect-size benchmarks to interpret research findings. However, these debates obscure a more central insight that emerges from empirical distributions of effect-size estimates in the literature: Efforts to improve education often fail to move the needle. I find that 36% of effect sizes from randomized control trials of education interventions with standardized achievement outcomes are less than 0.05 SD. Publication bias surely masks many more failed efforts from our view. Recognizing the frequency of these failures should be at the core of any approach to interpreting the policy relevance of effect sizes. We can aim high without dismissing as trivial those effects sizes that represent more incremental improvement#FRE#
#IPC# achievement; educational policy; effect size; research utilization#FPC#
#IRF# Agodini R., Harris B., An experimental evaluation of four elementary school math curricula, Journal of Research on Educational Effectiveness, 3, 3, pp. 199-253, (2010); 
Andrews I., Kasy M., Identification of and correction for publication bias, American Economic Review, 109, 8, pp. 2766-2794, (2019); 
Chan A.W., Hrobjartsson A., Haahr M.T., Gotzsche P.C., Altman D.G., Empirical evidence for selective reporting of outcomes in randomized trials: Comparison of protocols to published articles, JAMA, 291, 20, pp. 2457-2465, (2004); 
Cohen J., Statistical power analysis for the behavioral sciences, (1988); 
DellaVigna S., Linos E., RCTs to scale: Comprehensive evidence from two nudge units, Econometrica, 90, 1, pp. 81-116, (2022); 
Dickerson K., Publication bias: Recognizing the problem, understandings its origins and scope, and preventing harm, Publication bias in meta analysis: Prevention, assessment, and adjustments, pp. 11-34, (2005); 
Evans D.K., Yuan F., How big are effect sizes in international education studies?, Educational Evaluation and Policy Analysis, 44, 3, pp. 532-540, (2022); 
Fryer R.G., The production of human capital in developed countries: Evidence from 196 randomized field experiments, Handbook of economic field experiments, 2, pp. 95-322, (2017); 
Glass G.V., McGaw B., Smith M.L., Meta-analysis in social research, (1981); 
Jackson C.K., Mackevicius C., The distribution of school spending impacts, (2021); 
Kaplan R.M., Irvin V.L., Likelihood of null effects of large NHLBI clinical trials has increased over time, PloS One, 10, 8, (2015); 
Kelley K., Preacher K.J., On effect size, Psychological Methods, 17, 2, (2012); 
Kraft M.A., Interpreting effect sizes of education interventions, Educational Researcher, 49, 4, pp. 241-253, (2020); 
Lortie-Forgues H., Inglis M., Rigorous large-scale educational RCTs are often uninformative: Should we be concerned?, Educational Researcher, 48, 3, pp. 158-166, (2019); 
Morris C.N., Parametric empirical Bayes inference: Theory and applications, Journal of the American Statistical Association, 78, 381, pp. 47-55, (1983); 
Rothstein H.R., Sutton A.J., Borenstein M., Publication bias in meta-analysis: Prevention, assessment and adjustments, (2005); 
Simpson A., Separating arguments from conclusions: The mistaken role of effect size in educational policy research, Educational Research and Evaluation, 25, 1-2, pp. 99-109, (2019); 
Simpson A., Benchmarking a misnomer: A note on â€œInterpreting effect sizes in education interventions., Educational Researcher, (2021); 
Tyack D.B., Cuban L., Tinkering toward utopia, (1995); 
von Hippel P.T., Bellows L., How much does teacher quality vary across teacher preparation programs? Reanalyses from six states, Economics of Education Review, 64, pp. 298-312, (2018)#FRF#
