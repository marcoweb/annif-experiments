#ITI#Coding energy knowledge in constructed responses with explainable NLP models#FTI#
#IRE# Background: Formative assessments are needed to enable monitoring how student knowledge develops throughout a unit. Constructed response items which require learners to formulate their own free-text responses are well suited for testing their active knowledge. However, assessing such constructed responses in an automated fashion is a complex task and requires the application of natural language processing methodology. In this article, we implement and evaluate multiple machine learning models for coding energy knowledge in free-text responses of German K-12 students to items in formative science assessments which were conducted during synchronous online learning sessions. Dataset: The dataset we collected for this purpose consists of German constructed responses from 38 different items dealing with aspects of energy such as manifestation and transformation. The units and items were implemented with the help of project-based pedagogy and evidence-centered design, and the responses were coded for seven core ideas concerning the manifestation and transformation of energy. The data was collected from students in seventh, eighth and ninth grade. Methodology: We train various transformer- and feature-based models and compare their ability to recognize the respective ideas in students' writing. Moreover, as domain knowledge and its development can be formally modeled through knowledge networks, we evaluate how well the detection of the ideas within responses translated into accurate co-occurrence-based knowledge networks. Finally, in terms of the descriptive accuracy of our models, we inspect what features played a role for which prediction outcome and if the models pick up on undesired shortcuts. In addition to this, we analyze how much the models match human coders in what evidence within responses they consider important for their coding decisions. Results: A model based on a modified GBERT-large can achieve the overall most promising results, although descriptive accuracy varies much more than predictive accuracy for the different ideas assessed. For reasons of comparability, we also evaluate the same machine learning architecture using the SciEntsBank 3-Way benchmark with an English RoBERTa-large model, where it achieves state-of-the-art results in two out of three evaluation categories#FRE#
#IPC# automated coding; constructed response assessment; energy didactics; energy transformation; knowledge networks; short answer scoring#FPC#
#IRF# Anders C.J., Weber L., Neumann D., Samek W., Muller K.-R., Lapuschkin S., Finding and removing Clever Hans: Using explanation methods to debug and improve deep models, Information Fusion, 77, pp. 261-295, (2022); 
Andersen N., Zehner F., shinyReCoR: A Shiny Application for Automatically Coding Text Responses Using R, Psych, 3, 3, pp. 422-446, (2021); 
Anderson J.R., The Architecture of Cognition, (2013); 
Bachman L.F., Carr N., Kamei G., Kim M., Pan M.J., Salvador C., Sawaki Y., A Reliable Approach to Automatic Assessment of Short Answer Free Responses, (2002); 
Bastings J., Filippova K., The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 149-155, (2020); 
Bender E.M., Gebru T., McMillan-Major A., Shmitchell S., On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610-623, (2021); 
Binder A., Bach S., Montavon G., Muller K.-R., Samek W., Layer-Wise Relevance Propagation for Deep Neural Network Architectures, Information Science and Applications (ICISA) 2016, pp. 913-922, (2016); 
Birhane A., Algorithmic injustice: A relational ethics approach, Patterns, 2, 2, (2021); 
Bojanowski P., Grave E., Joulin A., Mikolov T., Enriching Word Vectors with Subword Information, Transactions of the Association for Computational Linguistics, 5, pp. 135-146, (2017); 
Bransford J.D., How People Learn: Brain, Mind, Experience, and School, (2000); 
Burrows S., Gurevych I., Stein B., The Eras and Trends of Automatic Short Answer Grading, International Journal of Artificial Intelligence in Education, 25, 1, pp. 60-117, (2015); 
Burstein J., Wolff S., Lu C., Using lexical semantic techniques to classify free-responses, Breadth and depth of semantic lexicons, pp. 227-244, (1999); 
Callear D.H., Jerrams-Smith J., Soh V., CAA of short non-MCQ answers, (2001); 
Camus L., Filighera A., Investigating Transformers for Automatic Short Answer Grading, Artificial Intelligence in Education, 12164, pp. 43-48, (2020); 
Chan B., Schweter S., Moller T., German's Next Language Model, (2020); 
Chefer H., Gur S., Wolf L., Transformer interpretability beyond attention visualization, (2021); 
Christianson N.H., Sizemore Blevins A., Bassett D.S., Architecture and evolution of semantic networks in mathematics texts, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 476, 2239, (2020); 
Crossley S.A., Kyle K., Davenport J.L., McNamara D.S., Automatic Assessment of Constructed Response Data in a Chemistry Tutor, Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016, Raleigh, North Carolina, USA. International Educational Data Mining Society (IEDMS), pp. 336-340, (2016); 
Cutrone L.A., Chang M., Automarking: Automatic Assessment of Open Questions. 2010 10th IEEE International Conference on Advanced Learning Technologies, pp. 143-147, (2010); 
Devlin J., Chang M.-W., Lee K., Toutanova K., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171-4186, (2019); 
Disessa A.A., Knowledge in pieces, Constructivism in the computer age, pp. 49-70, (1988); 
Dougiamas M., Taylor P., Moodle: Using Learning Communities to Create an Open Source Course Management System, Proceedings of EdMedia + Innovate Learning 2003, pp. 171-178, (2003); 
Drachsler H., Greller W., Privacy and analytics: It's a DELICATE issue a checklist for trusted learning analytics. Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK ‘16, pp. 89-98, (2016); 
Duit R., Teaching and Learning the Physics Energy Concept, Teaching and Learning of Energy in K – 12 Education, pp. 67-85, (2014); 
Dzikovska M., Nielsen R., Brew C., Leacock C., Giampiccolo D., Bentivogli L., Clark P., Dagan I., Dang H.T., SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. Second Joint Conference on Lexical and Computational Semantics (*SEM), Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pp. 263-274, (2013); 
Eckart de Castilho R., Gurevych I., A broad-coverage collection of portable NLP components for building shareable analysis pipelines. Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, 1-11, (2014); 
Firth J.R., A synopsis of linguistic theory 1930-55, Studies in Linguistic Analysis (Special Volume of the Philological Society), 1952-59, pp. 1-32, (1957); 
Gautam D., Rus V., Using Neural Tensor Networks for Open Ended Short Answer Assessment, Artificial Intelligence in Education, pp. 191-203, (2020); 
Geirhos R., Jacobsen J.-H., Michaelis C., Zemel R., Brendel W., Bethge M., Wichmann F.A., Shortcut learning in deep neural networks, Nature Machine Intelligence, 2, 11, pp. 665-673, (2020); 
Greller W., Drachsler H., Translating Learning into Numbers: A Generic Framework for Learning Analytics, Journal of Educational Technology & Society, 15, 3, pp. 42-57, (2012); 
Hahn M., Meurers D., Evaluating the meaning of answers to reading comprehension questions: A Semantics-Based Approach, Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pp. 326-336, (2012); 
Hastie T., Tibshirani R., Generalized Additive Models: Some Applications, Journal of the American Statistical Association, 82, 398, pp. 371-386, (1987); 
Herrmann-Abell C.F., DeBoer G.E., Investigating a learning progression for energy ideas from upper elementary through high school: Learning progression for energy ideas, Journal of Research in Science Teaching, 55, 1, pp. 68-93, (2018); 
Hochreiter S., Schmidhuber J., Long short-term memory, Neural Computation, 9, 8, pp. 1735-1780, (1997); 
Horbach A., Palmer A., Pinkal M., Using the text to evaluate short answers for reading comprehension exercises. Second Joint Conference on Lexical and Computational Semantics (*SEM), Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pp. 286-295, (2013); 
Jimenez S., Becerra C., Gelbukh A., SOftcardinality: Hierarchical Text Overlap for Student Response Analysis. Second Joint Conference on Lexical and Computational Semantics (*SEM), Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pp. 280-284, (2013); 
Klein R., Kyrilov A., Tokman M., Automated assessment of short free-text responses in computer science using latent semantic analysis. Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education - ITiCSE ‘11, 158, (2011); 
Krajcik J.S., Shin N., Project-Based Learning, The Cambridge Handbook of the Learning Sciences, pp. 275-297, (2014); 
Kubsch M., Nordine J., Neumann K., Fortus D., Krajcik J., Probing the Relation between Students' Integrated Knowledge and Knowledge-in-Use about Energy using Network Analysis, EURASIA Journal of Mathematics, Science and Technology Education, 15, 8, (2019); 
1st International Conference Learning Analytics and Knowledge, (2011); 
Landauer T.K., Foltz P.W., Laham D., An introduction to latent semantic analysis, Discourse Processes, 25, 2-3, pp. 259-284, (1998); 
Landis J.R., Koch G.G., The Measurement of Observer Agreement for Categorical Data, Biometrics, 33, 1, (1977); 
Leacock C., Chodorow M., C-rater: Automated Scoring of Short-Answer Questions, Computers and the Humanities, 37, 4, pp. 389-405, (2003); 
Lee H.-S., Liu O.L., Assessing learning progression of energy concepts across middle school grades: The knowledge integration perspective: Knowledge Integration Assessment, Science Education, 94, 4, pp. 665-688, (2010); 
Levy O., Zesch T., Dagan I., Gurevych I., UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis. Second Joint Conference on Lexical and Computational Semantics (*SEM), Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pp. 285-289, (2013); 
Linn M.C., The Knowledge Integration Perspective on Learning and Instruction, The Cambridge handbook of: The learning sciences, pp. 243-264, (2006); 
Liu N.F., Gardner M., Belinkov Y., Peters M.E., Smith N.A., Linguistic knowledge and transferability of contextual representations, (2019); 
Liu O.L., Rios J.A., Heilman M., Gerard L., Linn M.C., Validation of automated scoring of science assessments, Journal of Research in Science Teaching, 53, 2, pp. 215-233, (2016); 
Liu O.L., Ryoo K., Linn M.C., Sato E., Svihla V., Measuring Knowledge Integration Learning of Energy Topics: A two-year longitudinal study, International Journal of Science Education, 37, 7, pp. 1044-1066, (2015); 
Liu X., McKeough A., Developmental growth in students' concept of energy: Analysis of selected items from the TIMSS database, Journal of Research in Science Teaching, 42, 5, pp. 493-517, (2005); 
Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer L., Stoyanov V., RoBERTa: A robustly optimized BERT pretraining approach, CoRR, (2019); 
Livingston S.A., Constructed-Response Test Questions: Why We Use Them; How We Score Them. R&D Connections. Number 11, Educational Testing Service, (2009); 
Loshchilov I., Hutter F., Decoupled Weight Decay Regularization, (2019); 
Lou Y., Caruana R., Gehrke J., Hooker G., Accurate intelligible models with pairwise interactions, Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 623-631, (2013); 
Lukhele R., Thissen D., Wainer H., On the Relative Value of Multiple-Choice, Constructed Response, and Examinee-Selected Items on Two Achievement Tests, Journal of Educational Measurement, 31, 3, pp. 234-250, (1994); 
Maharjan N., Gautam D., Rus V., Assessing Free Student Answers in Tutorial Dialogues Using LSTM Models, Artificial Intelligence in Education, 10948, pp. 193-198, (2018); 
Marvaniya S., Saha S., Dhamecha T.I., Foltz P., Sindhgatta R., Sengupta B., Creating Scoring Rubric from Representative Student Answers for Improved Short Answer Grading, Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pp. 993-1002, (2018); 
Mason L., Baxter J., Bartlett P., Frean M., Boosting Algorithms as Gradient Descent, Advances in Neural Information Processing Systems, 12, (2000); 
McClelland J.L., Cleeremans A., Connectionist Models, The Oxford Companion to Consciousness, (2009); 
Meurers D., Ziai R., Ott N., Kopp J., Evaluating Answers to Reading Comprehension Questions in Context: Results for German and the Role of Information Structure. Proceedings of the TextInfer 2011 Workshop on Textual Entailment, pp. 1-9, (2011); 
Mislevy R.J., Almond R.G., Lukas J.F., A brief introduction to evidence-centered design, ETS Research Report Series, 2003, 1, pp. 1-29, (2003); 
Mislevy R.J., Haertel G.D., Implications of Evidence-Centered Design for Educational Testing, Educational Measurement: Issues and Practice, 25, 4, pp. 6-20, (2007); 
Mitchell T., Russell T., Broomhead P., Aldridge N., Towards robust computerised marking of free-text responses, (2002); 
Murdoch W.J., Singh C., Kumbier K., Abbasi-Asl R., Yu B., Interpretable machine learning: Definitions, methods, and applications, Proceedings of the National Academy of Sciences, 116, 44, pp. 22071-22080, (2019); 
A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
Neumann K., Viering T., Boone W.J., Fischer H.E., Towards a learning progression of energy, Journal of Research in Science Teaching, 50, 2, pp. 162-188, (2013); 
Nivre J., de Marneffe M.-C., Ginter F., Hajic J., Manning C.D., Pyysalo S., Schuster S., Tyers F., Zeman D., Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection. Proceedings of the 12th Language Resources and Evaluation Conference, pp. 4034-4043, (2020); 
Nori H., Jenkins S., Koch P., Caruana R., InterpretML: A Unified Framework for Machine Learning Interpretability, ArXiv preprint, (2019); 
Ott N., Ziai R., Hahn M., Meurers D., CoMeT: Integrating different levels of linguistic modeling for meaning assessment. Second Joint Conference on Lexical and Computational Semantics (*SEM), Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pp. 608-616, (2013); 
Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay E., Scikit-Learn: Machine Learning in Python, Journal of Machine Learning Research, 12, pp. 2825-2830, (2011); 
Pellegrino J.W., Proficiency in science: Assessment challenges and opportunities, Science, 340, 6130, pp. 320-323, (2013); 
Pellegrino J.W., DiBello L.V., Goldman S.R., A framework for conceptualizing and evaluating the validity of instructionally relevant assessments, Educational Psychologist, 51, 1, pp. 59-81, (2016); 
Poulton A., Eliens S., Explaining transformer-based models for automatic short answer grading. 2021 5th International Conference on Digital Technology in Education, pp. 110-116, (2021); 
Qi P., Zhang Y., Zhang Y., Bolton J., Manning C.D., Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, (2020); 
Reimers N., Gurevych I., Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982-3992, (2019); 
Rogers A., Kovaleva O., Rumshisky A., A Primer in BERTology: What We Know About How BERT Works, Transactions of the Association for Computational Linguistics, 8, pp. 842-866, (2020); 
Saha S., Dhamecha T.I., Marvaniya S., Sindhgatta R., Sengupta B., Sentence Level or Token Level Features for Automatic Short Answer Grading?: Use Both, Artificial Intelligence in Education, 10947, pp. 503-517, (2018); 
Bildungsstandards im Fach Physik für die Allgemeine Hochschulreife, (2020); 
Shaffer D.W., Collier W., Ruis A.R., A Tutorial on Epistemic Network Analysis: Analyzing the Structure of Connections in Cognitive, Social, and Interaction Data, Journal of Learning Analytics, 3, 3, pp. 9-45, (2016); 
Siddiqi R., Harrison C., A systematic approach to the automated marking of short-answer questions, IEEE International Multitopic Conference, 2008, pp. 329-332, (2008); 
Slade S., Tait A., Global guidelines: Ethics in Learning Analytics, (2019); 
Sogaard A., Explainable Natural Language Processing, Synthesis Lectures on Human Language Technologies, 14, 3, pp. 1-123, (2021); 
Sultan M.A., Salazar C., Sumner T., Fast and Easy Short Answer Grading with High Accuracy, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1070-1075, (2016); 
Sun X., Yang D., Li X., Zhang T., Meng Y., Qiu H., Wang G., Hovy E., Li J., Interpreting Deep Learning Models in Natural Language Processing: A Review, (2021); 
Sung C., Dhamecha T.I., Mukhi N., Improving Short Answer Grading Using Transformer-Based Pre-training, Artificial Intelligence in Education, 11625, pp. 469-481, (2019); 
Thomas M.S.C., McClelland J.L., Connectionist Models of Cognition, The Cambridge Handbook of Computational Psychology, pp. 23-58, (2001); 
Thomas P., The evaluation of electronic marking of examinations. Proceedings of the 8th Annual Conference on Innovation and Technology in Computer Science Education - ITiCSE ‘03, 50, (2003); 
Uto M., Uchida Y., Automated Short-Answer Grading Using Deep Neural Networks and Item Response Theory, Artificial Intelligence in Education, pp. 334-339, (2020); 
Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need. Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6000-6010, (2017); 
Weinert F.E., Leistungsmessungen in Schulen, (2002); 
Wolf T., Debut L., Sanh V., Chaumond J., Delangue C., Moi A., Cistac P., Rault T., Louf R., Funtowicz M., Davison J., Shleifer S., von Platen P., Ma C., Jernite Y., Plu J., Xu C., Scao T.L., Gugger S., Rush A.M., Transformers: State-of-the-Art Natural Language Processing, (2020); 
Yao J.-X., Guo Y.-Y., Neumann K., Refining a learning progression of energy, International Journal of Science Education, 39, 17, pp. 2361-2381, (2017); 
Zehner F., Automatic Processing of Text Responses in Large-Scale Assessments, Doctoral dissertation. Technische Universität München., (2016); 
Zesch T., Horbach A., ESCRITO - An NLP-Enhanced Educational Scoring Toolkit. Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), (2018)#FRF#
