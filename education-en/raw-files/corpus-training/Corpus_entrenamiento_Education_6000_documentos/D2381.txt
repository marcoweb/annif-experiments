#ITI#Identifying Disengaged Responding in Multiple-Choice Items: Extending a Latent Class Item Response Model With Novel Process Data Indicators#FTI#
#IRE# Disengaged responding poses a severe threat to the validity of educational large-scale assessments, because item responses from unmotivated test-takers do not reflect their actual ability. Existing identification approaches rely primarily on item response times, which bears the risk of misclassifying fast engaged or slow disengaged responses. Process data with its rich pool of additional information on the test-taking process could thus be used to improve existing identification approaches. In this study, three process data variables—text reread, item revisit, and answer change—were introduced as potential indicators of response engagement for multiple-choice items in a reading comprehension test. An extended latent class item response model for disengaged responding was developed by including the three new indicators as additional predictors of response engagement. In a sample of 1,932 German university students, the extended model indicated a better model fit than the baseline model, which included item response time as only indicator of response engagement. In the extended model, both item response time and text reread were significant predictors of response engagement. However, graphical analyses revealed no systematic differences in the item and person parameter estimation or item response classification between the models. These results suggest only a marginal improvement of the identification of disengaged responding by the new indicators. Implications of these results for future research on disengaged responding with process data are discussed.#FRE#
#IPC# computer-based assessments; disengaged responding; item response theory; multiple-choice items; process data; rapid guessing#FPC#
#IRF# Akaike H., A new look at the statistical model identification, IEEE Transactions on Automatic Control, 19, 6, pp. 716-723, (1974); 
Asparouhov T., Muthen B., Technical appendix: Variable-specific entropy contribution, Mplus user's guide, (2018); 
Bezirhan U., Von Davier M., Grabovsky I., Modeling item revisit behavior: The hierarchical speed-accuracy-revisits model, Educational and Psychological Measurement, 81, 2, pp. 363-387, (2021); 
Blossfeld H.-P., Rossbach H.-G., Education as a lifelong process: The German National Educational Panel Study (NEPS), (2019); 
Demars C.E., Wise S.L., Can differential rapid-guessing behavior lead to differential item functioning?, International Journal of Testing, 10, 3, pp. 207-229, (2010); 
Deribo T., Goldhammer F., Kroehne U., Changes in the speed-ability relation through different treatments of rapid guessing, Educational and Psychological Measurement, (2022); 
Erosheva E.A., Grade of membership and latent structure models with application to disability survey data, (2002); 
Gehrer K., Zimmermann S., Artelt C., Weinert S., The assessment of reading competence (including sample items for grade 5 and 9), (2012); 
Goldhammer F., Martens T., Christoph G., Ludtke O., Test-taking engagement in PIAAC, OECD education working papers, (2016); 
Goldhammer F., Martens T., Ludtke O., Conditioning factors of test-taking engagement in PIAAC: An exploratory IRT modelling approach considering person and item characteristics, Large-Scale Assessments in Education, 5, (2017); 
Ivanova M., Michaelides M., Eklof H., How does the number of actions on constructed-response items relate to test-taking effort and performance?, Educational Research and Evaluation, 26, 5-6, pp. 252-274, (2020); 
Jin K.-Y., Siu W.-L., Huang X., Exploring the impact of random guessing in distractor analysis, Journal of Educational Measurement, 59, 1, pp. 43-61, (2022); 
Kroehne U., Deribo T., Goldhammer F., Rapid guessing rates across administration mode and test setting, Psychological Test and Assessment Modeling, 62, 2, pp. 147-177, (2020); 
Lee Y.-H., Jia Y., Using response time to investigate students' test-taking behaviors in a NAEP computer-based study, Large-Scale Assessments in Education, 2, 1, (2014); 
Lindner M.A., Ludtke O., Nagy G., The onset of rapid-guessing behavior over the course of testing time: A matter of motivation and cognitive resources, Frontiers in Psychology, 10, (2019); 
Lundgren E., Eklof H., Within-item response processes as indicators of test-taking effort and motivation, Educational Research and Evaluation, 26, 5-6, pp. 275-301, (2020); 
Marianti S., Fox J.-P., Avetisyan M., Veldkamp B.P., Tijmstra J., Testing for aberrant behavior in response time modeling, Journal of Educational and Behavioral Statistics, 39, 6, pp. 426-451, (2014); 
Muthen L.K., Muthen B.O., Mplus user's guide, (1998); 
Nagy G., Ulitzsch E., A multilevel mixture IRT framework for modeling response times as predictors or indicators of response engagement in IRT models, Educational and Psychological Measurement, 82, 5, pp. 845-879, (2022); 
National educational panel study, scientific use file of starting cohort first-year students, (2022); 
Patel N., Sharma A., Shah T., Lomas D., Modeling NAEP test-taking behavior using educational process analysis, Journal of Educational Data-mining, 13, 2, pp. 16-54, (2021); 
Pokropek A., Grade of membership response time model for detecting guessing behaviors, Journal of Educational and Behavioral Statistics, 41, 3, pp. 300-325, (2016); 
Rasch G., Probabilistic models for some intelligence and attainment tests, (1960); 
R: A language and environment for statistical computing, (2021); 
Rios J.A., Deng J., Ihlenfeldt S.D., To what degree does rapid guessing distort aggregated test scores? A meta-analytic investigation, Educational Assessment, 27, 4, pp. 356-373, (2022); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of careless responding on aggregated-scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, 1, pp. 74-104, (2017); 
Rios J.A., Soland J., An investigation of item, examinee, and country correlates of rapid guessing in PISA, International Journal of Testing, 22, 2, pp. 154-184, (2022); 
Rose N., Von Davier M., Xu X., Modeling nonignorable missing data with item response theory (IRT), ETS Research Report Series, 2010, 1, (2010); 
Sahin F., Colvin K.F., Enhancing response time thresholds with response behaviors for detecting disengaged examinees, Large-Scale Assessments in Education, 8, 1, (2020); 
Schleicher A., PISA 2018 insights and interpretations, (2019); 
Schnipke D.L., Scrams D.J., Modeling item response times with a two-state mixture model: A new method of measuring speededness, Journal of Educational Measurement, 34, 3, pp. 213-232, (1997); 
Schwarz G.E., Estimating the dimension of a model, Annals of Statistics, 6, 2, pp. 461-464, (1978); 
Ulitzsch E., Penk C., 'Von Davier M., Pohl S., Model meets reality: Validating a new behavioral measure for test-taking effort, Educational Assessment, 26, 2, pp. 104-124, (2021); 
Ulitzsch E., Von Davier M., Pohl S., A hierarchical latent response model for inferences about examinee engagement in terms of guessing and item-level non-response, British Journal of Mathematical and Statistical Psychology, 73, pp. 83-112, (2020); 
Wang C., Xu G., A mixture hierarchical model for response times and response accuracy, British Journal of Mathematical and Statistical Psychology, 68, 3, pp. 456-477, (2015); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement: Issues and Practice, 36, 4, pp. 52-61, (2017); 
Wise S.L., Demars C.E., Low examinee effort in low-stakes assessment: Problems and potential solutions, Educational Assessment, 10, 1, pp. 1-17, (2005); 
Wise S.L., Demars C.E., An application of item response time: The effort-moderated IRT model, Journal of Educational Measurement, 43, 1, pp. 19-38, (2006); 
Wise S.L., Soland J., Bo Y., The (non)impact of differential test taker engagement on aggregated scores, International Journal of Testing, 20, 1, pp. 57-77, (2020)#FRF#
