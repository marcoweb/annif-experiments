#ITI#Which factors influence the success in pedigree analysis?#FTI#
#IRE#Pedigree problems are typical tasks in school genetics classes. However, they are perceived as difficult and often are not successfully completed. Therefore, the purpose of this study is to determine relevant factors that might have an impact on success in pedigree analysis. Based on previous research, we investigate the influence of the superficial appearance of the pedigree (relating to a well-known misconception), the mode of inheritance, content knowledge, reasoning abilities, the last biology grade, and mental effort. This means, we are simultaneously examining student and task characteristics. For this purpose, we analyse data from N = 135 students, who solved four pedigree problems each, using Generalised Linear Mixed Models (GLMMs). Specifically, we use multilevel logistic regression to determine the influence of the variables in question. The final model shows mixed results: For example, the represented mode of inheritance has no significant influence. The superficial appearance of the pedigree, in contrast, is one of the important predictors of success in identifying the present mode of inheritance. Therefore, our results imply that students make decisions based on misconceptions when analysing pedigree problems. This and all other results are discussed in order to infer implications for teaching and learning pedigree analysis#FRE#
#IPC#multilevel logistic regression; pedigree analysis; Problem-solving#FPC#
#IRF#Abraham J.K., Perez K.E., Price R.M., The dominance concept inventory: A tool for assessing undergraduate student alternative conceptions about dominance in Mendelian and population genetics, CBE—Life Sciences Education, 13, 2, pp. 349-358, (2014); 
Akaike H., A new look at the statistical model identification, IEEE Transactions on Automatic Control, 19, 6, pp. 716-723, (1974); 
Avena J.S., McIntosh B.B., Whitney O.N., Wiens A., Knight J.K., Successful problem solving in genetics varies based on question content, CBE—Life Sciences Education, 20, 4, (2021); 
Bates D., Machler M., Bolker B.M., Walker S.C., Fitting linear mixed-effects models using lme4, Journal of Statistical Software, 67, 1, pp. 1-48, (2015); 
Beauducel A., Brocke B., Liepmann D., Perspectives on fluid and crystallized intelligence: Facets for verbal, numerical, and figural intelligence, Personality and Individual Differences, 30, 6, pp. 977-994, (2001); 
Bennett R.L., The practical guide to the genetic family history, (2010); 
Bennett R.L., Steinhaus French K., Resta R.G., Lochner Doyle D., Standardized human pedigree nomenclature: Update and assessment of the recommendations of the national society of genetic counselors, Journal of Genetic Counseling, 17, 5, pp. 424-433, (2008); 
Bowerman B.L., O'Connell R.T., Linear statistical models: An applied approach, (1990); 
Chinn C.A., Brewer W.F., An empirical test of a taxonomy of responses to anomalous data in science, Journal of Research in Science Teaching, 35, 6, pp. 623-654, (1998); 
Collins A., Stewart J.H., The knowledge structure of Mendelian genetics, The American Biology Teacher, 51, 3, pp. 143-149, (1989); 
Corbalan G., Kester L., van Merrienboer J.J.G., Selecting learning tasks: Effects of adaptation and shared control on learning efficiency and task involvement, Contemporary Educational Psychology, 33, 4, pp. 733-756, (2008); 
Enders C.K., Tofighi D., Centering predictor variables in cross-sectional multilevel models: A new look at an old issue, Psychological Methods, 12, 2, pp. 121-138, (2007); 
Fox J., Weisberg S., An R companion to applied regression, (2019); 
Hackling M.W., Application of genetics knowledge to the solution of pedigree problems, Research in Science Education, 24, 1, pp. 147-155, (1994); 
Hackling M.W., Lawrence J.A., Expert and novice solutions of genetic pedigree problems, Journal of Research in Science Teaching, 25, 7, pp. 531-546, (1988); 
Hartig F., (2022); 
Heisig J.P., Schaeffer M., Why you should always include a random slope for the lower-level variable involved in a cross-level interaction, European Sociological Review, 35, 2, pp. 258-279, (2019); 
Hosmer D.W., Lemeshow S., Sturdivant R.X., Applied logistic regression, (2013); 
Kalyuga S., Cognitive load theory: How many types of load does it really need?, Educational Psychology Review, 23, 1, pp. 1-19, (2011); 
Koriat A., Nussinson R., Ackerman R., Judgments of learning depend on how learners interpret study effort, Journal of Experimental Psychology: Learning, Memory, and Cognition, 40, 6, pp. 1624-1637, (2014); 
Kostons D., van Gog T., Paas F.G.W.C., Training self-assessment and task-selection skills: A cognitive approach to improving self-regulated learning, Learning and Instruction, 22, 2, pp. 121-132, (2012); 
Krell M., Evaluating an instrument to measure mental load and mental effort considering different sources of validity evidence, Cogent Education, 4, 1, (2017); 
Krell M., Khan S., van Driel J., Analyzing cognitive demands of a scientific reasoning test using the linear logistic test model (LLTM), Education Sciences, 11, 9, (2021); 
Kubinger K.D., Gottschall C.H., Item difficulty of multiple choice tests dependant on different item response formats–an experiment in fundamental research on psychological assessment, Psychology Science, 49, 4, pp. 361-374, (2007); 
Le Hebel F., Montpied P., Tiberghien A., Fontanieu V., Sources of difficulty in assessment: Example of PISA science items, International Journal of Science Education, 39, 4, pp. 468-487, (2017); 
Liepmann D., Beauducel A., Brocke B., Amthauer R., Intelligenz-Struktur-Test 2000 R [Intelligence structure test, (2007); 
Liou P.-Y., Bulut O., The effects of item format and cognitive domain on students’ science performance in TIMSS 2011, Research in Science Education, 50, 1, pp. 99-121, (2020); 
Ludecke D., ggeffects: Tidy data frames of marginal effects from regression models, Journal of Open Source Software, 3, 26, (2018); 
Ludecke D., Ben-Shachar M.S., Patil I., Waggoner P., Makowski D., Performance: An R package for assessment, comparison and testing of statistical models, Journal of Open Source Software, 6, 60, (2021); 
Mosaly P.R., Mazur L.M., Yu F., Guo H., Derek M., Laidlaw D.H., Moore C., Marks L.B., Mostafa J., Relating task demand, mental effort and task difficulty with physicians’ performance during interactions with electronic health records (EHRs), International Journal of Human–Computer Interaction, 34, 5, pp. 467-475, (2018); 
Nakagawa S., Schielzeth H., A general and simple method for obtaining R2 from generalized linear mixed-effects models, Methods in Ecology and Evolution, 4, 2, pp. 133-142, (2013); 
Oberpriller J., de Souza Leite M., Pichler M., Fixed or random? On the reliability of mixed-effects models for a small number of levels in grouping variables, Ecology and Evolution, 12, 7, (2022); 
Opfer J.E., Nehm R.H., Ha M., Cognitive foundations for science assessment design: Knowing what students know about evolution, Journal of Research in Science Teaching, 49, 6, pp. 744-777, (2012); 
Opitz A., Heene M., Fischer F., Using differential item functioning to analyze the domain generality of a common scientific reasoning test, European Journal of Psychological Assessment, 38, 4, pp. 251-260, (2022); 
Paas F.G.W.C., Training strategies for attaining transfer of problem-solving skill in statistics: A cognitive-load approach, Journal of Educational Psychology, 84, 4, pp. 429-434, (1992); 
Paas F.G.W.C., Renkl A., Sweller J., Cognitive load theory and instructional design: Recent developments, Educational Psychologist, 38, 1, pp. 1-4, (2003); 
Paas F.G.W.C., Tuovinen J.E., Tabbers H., van Gerven P.W.M., Cognitive load measurement as a means to advance cognitive load theory, Educational Psychologist, 38, 1, pp. 63-71, (2003); 
Paas F.G.W.C., Tuovinen J.E., van Merrienboer J.J.G., Darabi A.A., A motivational perspective on the relation between mental effort and performance: Optimizing learner involvement in instruction, Educational Technology Research and Development, 53, 3, pp. 25-34, (2005); 
Paas F.G.W.C., van Merrienboer J.J.G., The efficiency of instructional conditions: An approach to combine mental effort and performance measures, Human Factors: The Journal of the Human Factors and Ergonomics Society, 35, 4, pp. 737-743, (1993); 
Paas F.G.W.C., van Merrienboer J.J.G., Instructional control of cognitive load in the training of complex cognitive tasks, Educational Psychology Review, 6, 4, pp. 351-371, (1994); 
Paas F.G.W.C., van Merrienboer J.J.G., Adam J.J., Measurement of cognitive load in instructional research, Perceptual and Motor Skills, 79, 1, pp. 419-430, (1994); 
Powell M.J.D., (2009); 
Renkl A., Atkinson R.K., Structuring the transition from example study to problem solving in cognitive skill acquisition: A cognitive load perspective, Educational Psychologist, 38, 1, pp. 15-22, (2003); 
Schmeck A., Opfermann M., van Gog T., Paas F.G.W.C., Leutner D., Measuring cognitive load with subjective rating scales during problem solving: Differences between immediate and delayed ratings, Instructional Science, 43, 1, pp. 93-114, (2015); 
Schmiemann P., Nehm R.H., Tornabene R.E., Assessment of genetics understanding: Under what conditions do situational features have an impact on measures?, Science & Education, 26, 10, pp. 1161-1191, (2017); 
Shea N.A., Duncan R.G., Stephenson C., A tri-part model for genetics literacy: Exploring undergraduate student reasoning about authentic genetics dilemmas, Research in Science Education, 45, 4, pp. 485-507, (2015); 
Smith M.U., Successful and unsuccessful problem solving in classical genetic pedigrees, Journal of Research in Science Teaching, 25, 6, pp. 411-433, (1988); 
Smith M.U., Good R., Problem solving and classical genetics: Successful versus unsuccessful performance, Journal of Research in Science Teaching, 21, 9, pp. 895-912, (1984); 
Stiller J., Hartmann S., Mathesius S., Straube P., Tiemann R., Nordmeier V., Kruger D., Upmeier zu Belzen A., Assessing scientific reasoning: A comprehensive evaluation of item features that affect item difficulty, Assessment & Evaluation in Higher Education, 41, 5, pp. 721-732, (2016); 
Sweller J., Cognitive load during problem solving: Effects on learning, Cognitive Science, 12, 2, pp. 257-285, (1988); 
Sweller J., Ayres P., Kalyuga S., Cognitive load theory, (2011); 
Taber K.S., The use of Cronbach’s alpha when developing and reporting research instruments in science education, Research in Science Education, 48, 6, pp. 1273-1296, (2018); 
Timm J., Otto B., Schramm T., Striewe M., Schmiemann P., Goedicke M., Technical aspects of automated item generation for blended learning environments in biology, I-Com, 19, 1, pp. 3-15, (2020); 
Timm J., Wools K., Schmiemann P., Secondary students’ reasoning on pedigree problems, CBE—Life Sciences Education, 21, 1, (2022); 
Tsui C.-Y., Treagust D., Evaluating secondary students’ scientific reasoning in genetics using a two–tier diagnostic instrument, International Journal of Science Education, 32, 8, pp. 1073-1098, (2010); 
van Gog T., Hoogerheide V., van Harsel M., The role of mental effort in fostering self-regulated learning with problem-solving tasks, Educational Psychology Review, 32, 4, pp. 1055-1072, (2020); 
van Gog T., Kirschner F., Kester L., Paas F.G.W.C., Timing and frequency of mental effort measurement: Evidence in favour of repeated measures, Applied Cognitive Psychology, 26, 6, pp. 833-839, (2012); 
van Gog T., Paas F.G.W.C., Instructional efficiency: Revisiting the original construct in educational research, Educational Psychologist, 43, 1, pp. 16-26, (2008); 
Wickham H., Ggplot2, (2009); 
Yaremych H.E., Preacher K.J., Hedeker D., Centering categorical predictors in multilevel models: Best practices and interpretation, Psychological Methods, (2021)#FRF#
