#ITI#Polytomous Testlet Response Models for Technology-Enhanced Innovative Items: Implications on Model Fit and Trait Inference#FTI#
#IRE# The development of technology-enhanced innovative items calls for practical models that can describe polytomous testlet items. In this study, we evaluate four measurement models that can characterize polytomous items administered in testlets: (a) generalized partial credit model (GPCM), (b) testlet-as-a-polytomous-item model (TPIM), (c) random-effect testlet model (RTM), and (d) fixed-effect testlet model (FTM). Using data from GPCM, FTM, and RTM, we examine performance of the scoring models in multiple aspects: relative model fit, absolute item fit, significance of testlet effects, parameter recovery, and classification accuracy. The empirical analysis suggests that relative performance of the models varies substantially depending on the testlet-effect type, effect size, and trait estimator. When testlets had no or fixed effects, GPCM and FTM led to most desirable measurement outcomes. When testlets had random interaction effects, RTM demonstrated best model fit and yet showed substantially different performance in the trait recovery depending on the estimator. In particular, the advantage of RTM as a scoring model was discernable only when there existed strong random effects and the trait levels were estimated with Bayes priors. In other settings, the simpler models (i.e., GPCM, FTM) performed better or comparably. The study also revealed that polytomous scoring of testlet items has limited prospect as a functional scoring method. Based on the outcomes of the empirical evaluation, we provide practical guidelines for choosing a measurement model for polytomous innovative items that are administered in testlets.#FRE#
#IPC# innovative items; item response theory; polytomous items; technology-enhanced assessment; testlet#FPC#
#IRF# Aitchison J., Silvey S.D., Maximum likelihood estimation of parameters subject to restraints, Annals of Mathematical Statistics, 29, 3, pp. 813-828, (1958); 
Akaike H., Information theory and an extension of the maximum likelihood principle, pp. 267-281, (1973); 
Akaike H., Factor analysis and AIC, Psychometrika, 52, 3, pp. 317-332, (1987); 
Bentler P.M., EQS structural equations program manual, (1995); 
Betts J., Muntean W., Kim D., Kao S., Evaluating different scoring methods for multiple response items providing partial credit, Educational Psychological Measurement, (2021); 
Bock R.D., Estimating item parameters and latent ability when responses are scored in two or more nominal categories, Psychometrika, 37, 1 Pt. 1, pp. 29-51, (1972); 
Boyd A.M., Dodd B., Fitzpatrick S., A comparison of exposure control procedures in cat systems based on different measurement models for testlets, Applied Measurement in Education, 26, 2, pp. 113-135, (2013); 
Bozdogan H., Model selection and Akaikeâ€™s information criterion (AIC): The general theory and its analytical extensions, Psychometrika, 52, 3, pp. 345-370, (1987); 
Bradlow E.T., Wainer H., Wang X., A Bayesian random effects model for testlets, Psychometrika, 64, 2, pp. 153-168, (1999); 
Chalmers R.P., mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Chen W., Thissen D., Local dependence indexes for item pairs using item response theory, Journal of Educational and Behavioral Statistics, 22, 3, pp. 265-289, (1997); 
Davey T., Godwin J., Mittelholtz D., Developing and scoring an innovative computerized writing assessment, Journal of Educational Measurement, 34, 1, pp. 21-41, (1997); 
De Champlain A., Gessaroli M.E., Assessing the dimensionality of item response matrices with small sample sizes and short test lengths, Applied Measurement in Education, 11, 3, pp. 231-253, (1998); 
DeMars C.E., Application of the bi-factor multidimensional item response theory model to testlet-based tests, Journal of Educational Measurement, 43, 2, pp. 145-168, (2006); 
DeMars C.E., Confirming testlet effects, Applied Psychological Measurement, 36, 2, pp. 104-121, (2012); 
Dempster A.P., Laird N.M., Rubin D.B., Maximum likelihood from incomplete data via the EM algorithm, Journal of the Royal Statistical Society, 39, 1, pp. 1-38, (1977); 
Donoghue J.R., An empirical examination of the IRT information of polytomously scored reading items under the generalized partial credit model, Journal of Educational Measurement, 31, 4, pp. 295-311, (1994); 
Glas C.A.W., Wainer H., Bradlow E.T., MML and EAP estimation in testlet-based adaptive testing, Computerized adaptive testing: Theory and practice, pp. 271-287, (2002); 
Hayashi K., Bentler P.M., Yuan K.-H., On the likelihood ratio test for the number of factors in exploratory factor analysis, Structural Equation Modeling, 14, 3, pp. 505-526, (2007); 
Hernandez-Camacho A., Olea J., Abad F.J., Comparison of uni- and multidimensional models applied in testlet-based tests, Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 13, 4, pp. 135-143, (2017); 
Himelfarb I., Marcoulides K.M., Fang G., Shotts B.L., A two-level alternating direction model for polytomous items with local dependence, Educational and Psychological Measurement, 80, 2, pp. 293-311, (2020); 
Jiao H., Liu J., Hainie K., Woo A., Gorham J., Comparison between dichotomous and polytomous scoring of innovative items in a large-scale computerized adaptive test, Educational and Psychological Measurement, 72, 3, pp. 493-509, (2012); 
Jodoin M.G., Measurement efficiency of innovative item formats in computer-based testing, Journal of Educational Measurement, 40, 1, pp. 1-15, (2003); 
Joreskog K.G., Sorbom D., Lisrel 7. a guide to the program and applications, (1988); 
Lee G., Kolen M.J., Frisbie D.A., Ankenmann R.D., Comparison of dichotomous and polytomous item response models in equating scores from tests composed of testlets, Applied Psychological Measurement, 25, 4, pp. 357-372, (2001); 
Li Y., Bolt D.M., Fu J., A test characteristic curve linking method for the testlet model, Applied Psychological Measurement, 29, 5, pp. 340-356, (2005); 
Li Y., Li S., Wang L., Application of a general polytomous testlet model to the reading section of a large-scale English language assessment, (2010); 
Marais I.D., Andrich D., Effects of varying magnitude and patterns of local dependence in the unidimensional Rasch model, Journal of Applied Measurement, 9, 2, pp. 105-124, (2008); 
Masters G.N., A Rasch model for partial credit scoring, Psychometrika, 47, 2, pp. 147-174, (1982); 
Maydeu-Olivares A., Goodness-of-fit assessment of item response theory models (with discussion), Measurement: Interdisciplinary Research and Perspectives, 11, 3, pp. 71-137, (2013); 
McDonald R.P., Mok M.M.C., Goodness of fit in item response models, Multivariate Behavioral Research, 30, 1, pp. 23-40, (1995); 
Mellenbergh G.J., A unidimensional latent trait model for continuous item responses, Multivariate Behavioral Research, 29, 3, pp. 223-236, (1994); 
Muller H., A Rasch model for continuous rating, Psychometrika, 52, 2, pp. 165-181, (1987); 
Muraki E., A generalized partial credit model: Application of an EM algorithm, Applied Psychological Measurement, 16, 2, pp. 159-176, (1992); 
Muraki E., Information functions of the generalized partial credit model, Applied Psychological Measurement, 17, 4, pp. 351-363, (1993); 
Penfield R.D., Bergeron J.M., Applying a weighted maximum likelihood latent trait estimator to the generalized partial credit model, Applied Psychological Measurement, 29, 3, pp. 218-233, (2005); 
Rao C.R., Large sample tests of statistical hypothesis concerning several parameters with applications to problems of estimation, Proceedings of the Cambridge Philosophical Society, 44, 1, pp. 50-57, (1948); 
Robitzsch A., Kiefer T., Wu M., Tam: Test analysis modules, (2020); 
Rosenbaum P.R., Item bundles, Psychometrika, 53, 3, pp. 349-359, (1988); 
Samejima F., A general model for free-response data, (1972); 
Samejima F., Normal ogive model on the continuous response level in the multidimensional latent space, Psychometrika, 39, 1, pp. 111-121, (1974); 
Schwartz G., Estimating the dimension of a model, Annals of Statistics, 6, 2, pp. 461-464, (1978); 
Sclove S.L., Application of model-selection criteria to some problems in multivariate analysis, Psychometrika, 52, 3, pp. 333-343, (1987); 
Sireci S.G., Thissen D., Wainer H., (1991). On the reliability of testlet-based tests, Journal of Educational Measurement, 28, 3, pp. 237-247; 
Tuerlinckx F., De Boeck P., The effect of ignoring item interactions on the estimated discrimination parameters in item response theory, Psychological Methods, 6, 2, pp. 181-195, (2001); 
Wainer H., Bradlow B., Wang X., Testlet response theory and its application, (2007); 
Wainer H., Kiely G.L., Item clusters and computerized adaptive testing: A case for testlets, Journal of Educational Measurement, 24, 3, pp. 185-201, (1987); 
Wainer H., Thissen D., How is reliability related to the quality of test scores? What is the effect of local dependence on reliability?, Educational Measurement: Issues and Practice, 15, 1, pp. 22-29, (1996); 
Wainer H., Wang C., Using a new statistical model for testlets to score TOEFL, Journal of Educational Measurement, 37, 3, pp. 203-220, (2000); 
Wang W.-C., Wilson M., The Rasch testlet model, Applied Psychological Measurement, 29, 2, pp. 126-149, (2005); 
Wang X., Bradlow E.T., Wainer H., A general Bayesian model for testlets: Theory and applications, Applied Psychological Measurement, 26, 1, pp. 109-128, (2002); 
Yen W.M., Using simulation results to choose a latent trait model, Applied Psychological Measurement, 5, 2, pp. 245-262, (1981); 
Yen W.M., Effects of local item dependence on the fit and equating performance of the three-parameter logistic model, Applied Psychological Measurement, 8, 2, pp. 125-145, (1984)#FRF#
