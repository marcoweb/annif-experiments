#ITI#Open Student Modeling Research and its Connections to Educational Assessment#FTI#
#IRE# Research in the area of Open Student Models (OSMs) has shown that external representations of the student model can be used to facilitate educational processes such as student reflection, knowledge awareness, learning, collaboration, negotiation, and student model diagnosis. OSMs can be integrated into existing learning systems or become a framework for the creation of learning systems. This paper discusses how early work with Jim Greer in the area of open student modeling has inspired and continues to inspire a line of research on innovative assessments and the design and evaluation of score report systems that are used to share assessment/student modeling information with various educational stakeholders to support learning. Several projects are discussed as well as their connections to my work with Jim while working at the ARIES laboratory#FRE#
#IPC# Educational assessment; Evidence-based interaction; Indirectly visible Bayesian student models; Open student models; Privacy and data security; Score reporting systems#FPC#
#IRF# American Educational Research Association, American Psychological Association, & National Council on Measurement in Education: Standards for Educational and Psychological Testing, (2014); 
Uncertainty in open learner models: Visualising inconsistencies in the underlying data, In LAL@ LAK, pp. 23-30, (2016); 
Anwar M., Greer J., Facilitating trust in privacy-preserving e-learning environments, IEEE Transactions on Learning Technologies, 5, 1, pp. 62-73, (2012); 
Anwar M., Greer J., Role- and relationship-based identity Management for Privacy-enhanced E-learning, International Journal of Artificial Intelligence in Education, 21, 3, pp. 191-213, (2012); 
Bennett R.E., Cognitively based assessment of, for, and as learning (Cbal): A preliminary theory of action for summative and formative assessment, Measurement: Interdisciplinary Research & Perspective, 8, 2-3, pp. 70-91, (2010); 
Bennett R.E., Gitomer D.H., Transforming K–12 assessment: Integrating accountability testing, formative assessment and professional support, Assessment issues of the 21st century, pp. 43-61, (2009); 
Biswas G., Schwartz D., Bransford J., Technology support for complex problem solving: From SAD environments to AI, Smart machines in education: The coming revolution in educational technology, pp. 71-97, (2001); 
Bodily R., Kay J., Aleven V., Jivet I., Davis D., Xhakaj F., Verbert K., Open learner models and learning analytics dashboards: A systematic review, Proceedings of the 8th international conference on learning analytics and knowledge, pp. 41-50, (2018); 
Bull S., See yourself write: A simple student model to make students think, User modeling: Proceedings of the sixth international conference, pp. 315-326, (1997); 
Bull S., ‘Do it Yourself’ student models for collaborative student Modelling and peer interaction, Intelligent Tutoring Systems, pp. 176-185, (1998); 
Bull S., Negotiated learner modelling to maintain today’s learner models, Research and Practice in Technology Enhanced Learning, 11, 10, pp. 1-29, (2016); 
Bull S., Kay J., Student models that invite the learner in: The SMILI open learner modelling framework, International Journal of Artificial Intelligence in Education, 17, 2, pp. 89-120, (2007); 
Bull S., Kay J., SMILI☺: A framework for interfaces to learning data in open learner models, learning analytics and related fields, International Journal of Artificial Intelligence in Education, 26, 1, pp. 293-331, (2016); 
Bull S., Pain H., ‘Did I say what I think I said, and do you agree with me?’: Inspecting and questioning the student model, Proceedings of world conference on artificial intelligence and education, pp. 501-508, (1995); 
Chan T.W., Baskin A.B., Learning companion systems. In C. Frasson & G. Gauthier (Eds.), Intelligent Tutoring Systems: At the crossroads of AI and Education (p. 6-33), Ablex Pub, (1990); 
Chrysafiadi K., Virvou M., Student modeling approaches: A literature review for the last decade, Expert Systems with Applications, 40, 11, pp. 4715-4729, (2013); 
Conati C., Porayska-Pomsta K., Mavrikis M., AI in education needs interpretable machine learning: Lessons from open learner Modelling. ArXiv, abs/1807.00154.Corrin, L. (2018). Evaluating students’ interpretation of feedback in interactive dashboards, Score Reporting Research and Applications, pp. 145-159, (2018); 
Corrin L., Evaluating students’ interpretation of feedback in interactive dashboards, Score Reporting Research and Applications, pp. 145-159, (2018); 
Dimitrova V., StyLE-: Interactive open learner Modelling, International Journal of Artificial Intelligence in Education, 13, 1, pp. 35-78, (2003); 
Dimitrova V., Brna P., From interactive open learner Modelling to intelligent mentoring: STyLE-OLM and beyond, International Journal of Artificial Intelligence in Education, 26, 1, pp. 332-349, (2016); 
Ercikan K., Pellegrino J.W., Validation of score meaning using examinee response process for the next generation of assessments, Validation of score meaning for the next generation of assessments: The use of response processes, pp. 1-8, (2017); 
Feng M., Krumm A., Grover S., Applying learning analytics to support instruction, Score reporting research and applications, pp. 145-159, (2018); 
Forsyth C.M., Peters S., Zapata-Rivera D., Lentini J., Graesser A.C., Cai Z., Interactive score reporting: An AutoTutor-based system for teachers, In proceedings of the international conference on artificial intelligence in education, LNCS, pp. 506-509, (2017); 
Ginon B., Boscolo C., Johnson M.D., Bull S., Persuading an open learner model in the context of a university course: An exploratory study, Proceedings of the 13Th International Conference on Intelligent Tutoring Systems, pp. 307-313, (2016); 
Graesser A.C., Person N., Harter D., Teaching tactics and dialog in AutoTutor, International Journal of Artificial Intelligence in Education, 12, pp. 257-279, (2001); 
Greer J., McCalla G., Vassileva J., Deters R., Bull S., Kettel L., Lessons learned in deploying a multi-agent learning support system: The I-help experience, Proceedings International AI and Education Conference AIED’2001, San Antonio, pp. 410-421, (2001); 
Hambleton R., Zenisky A., Reporting test scores in more meaningful ways: A research-based approach to score report design. APA handbook of testing and assessment in psychology, pp. 479-494, (2013); 
Hegarty M., Advances in cognitive science and information visualization, Score reporting research and applications, pp. 19-34, (2018); 
Jivet I., Scheffel M., Specht M., Drachsler H., License to evaluate: Preparing learning analytics dashboards for educational practice, Proceedings of the 8th international conference on learning analytics and knowledge, pp. 31-40, (2018); 
Johnson W.L., Lester J.C., Face-to-face interaction with pedagogical agents, twenty years later, International Journal of Artificial Intelligence in Education., 26, 1, pp. 25-36, (2016); 
Johnson W.L., Rickel J.W., Lester J.C., Animated pedagogical agents: Face-to-face interaction in interactive learning environments, International Journal of Artificial Intelligence in Education, 11, 1, pp. 47-78, (2000); 
Kannan P., Zapata-Rivera D., Leibowitz E.A., Interpretation of score reports by diverse subgroups of parents, Educational Assessment, 23, 3, pp. 173-194, (2018); 
Kerly A., Bull S., Children’s interactions with inspectable and negotiated learner models, Intelligent Tutoring Systems, pp. 132-141, (2008); 
Klerkx J., Verbert K., Duval E., Learning analytics dashboards, The Handbook of Learning Analytics, Pages 143–150. Society for Learning Analytics Research (Solar), (2017); 
Kobsa A., Tailoring Privacy to Users’ Needs (Invited Keynote), User Modeling 2001: 8th international conference UM2001, pp. 303-313, (2001); 
Lopez A.A., Luce C., Zapata-Rivera D., Forsyth C., Using formative conversation- based assessments to support students’ English language development, IEEE Technical Committee on Learning Technology Bulletin, 19, 1, pp. 6-9, (2017); 
Lord F.M., Applications of item response theory to practical testing problems, (1980); 
Maldonado R., Kay J., Yacef K., Schwendimann B., An interacEve teacher’s dashboard for monitoring groups in a multi-tabletop learning environment, Intelligent Tutoring Systems, pp. 482-492, (2012); 
Mislevy R., (2012); 
Pandarova I., Schmidt T., Hartig J., Boubekki A., Jones R.D., Brefeld U., Predicting the difficulty of exercise items for dynamic difficulty adaptation in adaptive language tutoring, International Journal of Artificial Intelligence in Education, 2019, pp. 342-367, (2019); 
Maries A., Kumar A., The effect of student model on learning, Proceedings of the Eighth IEEE International Conference on Advanced Learning Technologies, pp. 877-881, (2008); 
Shahrour G., Bull S., Does ‘notice’ prompt noticing? Raising awareness in language learning with an open learner model, Proceedings of the 5Th International Conference on Adaptive Hypermedia and Adaptive Web-Based Systems, pp. 173-182, (2008); 
Shute V.J., Tensions, trends, tools, and technologies: Time for an Educational Sea change, Research Report. ETS RR-06-16. ETS Research Report Series, (2006); 
Slavuj V., Mestrovic A., Kovacic B., Adaptivity in educational systems for language learning: A review, Computer Assisted Language Learning, 30, 1-2, pp. 64-90, (2017); 
So Y., Zapata-Rivera D., Cho Y., Luce C., Battistini L., Using trialogues to measure English language skills, Journal of educational technology and society, pp. 21-32, (2015); 
Toulmin S.E., The uses of argument, (1958); 
Underwood J.S., Zapata-Rivera D., VanWinkle W., Growing pains: Teachers using and learning to use IDMS (research memorandum 08–07), (2007); 
Underwood J.S., Zapata-Rivera D., VanWinkle W., An evidence-centered approach to using assessment data for policymakers (ETS research rep. No. RR-10-03), (2010); 
Van Labeke N., Brna P., Morales R., Opening up the interpretation process in an open learner model, International Journal of Artificial Intelligence in Education., 17, pp. 305-338, (2007); 
Yang H.C., Zapata-Rivera D., Interlanguage pragmatics with a pedagogical agent: The request game, Computer Assisted Language Learning, 23, 5, pp. 395-412, (2010); 
Zapata-Rivera J.D., Learning Environments based on Inspectable Student Models. Ph.D. Thesis, (2003); 
Zapata-Rivera D., Indirectly visible Bayesian student models, Proceedings of the 5th UAI Bayesian Modelling applications workshop, CEUR Workshop Proceedings, Vol. 268. 9 pp, (2007); 
Zapata-Rivera D., Toward caring assessment systems, Adjunct Publication of the 25Th Conference on User Modeling, Adaptation and Personalization (UMAP ‘17), pp. 97-100, (2017); 
Zapata-Rivera D., Why is score reporting relevant?, Score Reporting Research and Applications, pp. 1-6, (2018); 
Zapata-Rivera D., Score Reporting Research and Applications, (2018); 
Zapata-Rivera D., Supporting human inspection of adaptive instructional systems, Adaptive Instructional Systems. HCII 2019, Lecture Notes in Computer Science, 11597, pp. 482-490, (2019); 
Zapata-Rivera D., Greer J., Exploring various guidance mechanisms to support interaction with Inspectable learner models, Proceedings of Intelligent Tutoring Systems ITS, 2002, pp. 442-452, (2002); 
Zapata-Rivera J.D., Greer J., Interacting with Bayesian student models, International Journal of Artificial Intelligence in Education, 14, 2, pp. 127-163, (2004); 
Zapata-Rivera J.D., Greer J., Inspectable Bayesian student modelling servers in multi-agent tutoring systems, International Journal of Human Computer Studies., 61, 4, pp. 535-563, (2004); 
Zapata-Rivera D., Katz I.R., Keeping your audience in mind: Applying audience analysis to the design of score reports, Assessment in Education: Principles, Policy & Practice, 21, pp. 442-463, (2014); 
Zapata-Rivera D., VanWinkle W., A research-based approach to designing and evaluating score reports for teachers. ETS research memorandum no.RM-10-01, (2010); 
Zapata-Rivera D., Underwood J.S., Bauer M., Advanced reporting Systems in Assessment Environments. In J. Kay, A. Lum, & J.-D. Zapata-Rivera (Eds.), Proceedings of Workshop on Learner Modelling for Reflection, to Support Learner Control, Metacognition and Improved Communication between Teachers and Learners (pp. 23-32), 10th international conference on artificial intelligence in education, Sydney, (2005); 
Zapata-Rivera D., Hansen E.G., Shute V.J., Underwood J.S., Bauer M.I., Evidence-based approach to interacting with open student models, International Journal of Artificial Intelligence in Education., 17, 3, pp. 273-303, (2007); 
Maries A., Kumar A., The effect of student model on learning, Proceedings of the Eighth IEEE International Conference on Advanced Learning Technologies, pp. 877-881, (2008); 
Zapata-Rivera D., VanWinkle W., Zwick R., Applying score design principles in the Design of Score Reports for CBAL™ teachers, ETS research memorandum RM-12-20, (2012); 
Zapata-Rivera D., Vezzubiggers M.K., Supporting teacher communication with parents and students using score reports, Paper Presented at the Annual Meeting of the American Educational Research Association, (2013); 
Zapata-Rivera D., Jackson T., Liu L., Bertling M., Vezzu M., Katz I., (2014); 
Zapata-Rivera D., Jackson T., Katz I.R., Authoring conversation-based assessment scenarios. In R. A. Sottilare, A. C. Graesser, X. Hu, & K. Brawner (Eds.), Design Recommendations for Intelligent Tutoring Systems Volume 3: Authoring Tools and Expert Modeling Techniques. Pp. 169–178. U.S, Army Research Laboratory, (2015); 
Zapata-Rivera D., Liu L., Chen L., Hao J., von Davier A., Assessing science inquiry skills in immersive, conversation-based systems, pp. 237-252, (2016); 
Zapata-Rivera D., Zwick R., Vezzu M., Exploring the Effectiveness of a Measurement Error Tutorial in Helping Teachers Understand Score Report Results, Educational Assessment, 21, 3, pp. 215-229, (2016); 
Zapata-Rivera D., Kannan P., Forsyth C., Peters S., Bryant A.D., Guo E., Long R., Designing and evaluating reporting Systems in the Context of new assessments, Augmented Cognition: Users and Contexts. AC 2018, Lecture Notes in Computer Science, 10916, pp. 143-153, (2018); 
Zapata-Rivera D., Graesser A., Kay J., Hu X., Ososky S.J., Visualization implications for the validity of ITS, In Design Recommendations for Intelligent Tutoring Systems: Volume 8 – Data Visualization, (2020); 
Zwick R., Zapata-Rivera D., Hegarty M., Comparing graphical and verbal representations of measurement error in test score reports, Educational Assessment., 19, 2, pp. 116-138, (2014)#FRF#
