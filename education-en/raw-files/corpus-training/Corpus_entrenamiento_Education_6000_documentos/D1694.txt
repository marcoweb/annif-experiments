#ITI#20 years of interactive tasks in large-scale assessments: Process data as a way towards sustainable change?#FTI#
#IRE# Background: Over the last 20 years, educational large-scale assessments have undergone dramatic changes moving away from simple paper-pencil assessments to innovative, technology-based assessments. This comprehensive switch has led to some rather technical improvements such as identifying early guessing or improving standardization. Objectives: At the same time, process data on student interaction with items has been shown to carry value for obtaining, reporting, and interpreting additional results on student skills in international comparisons. In fact, on the basis of innovative simulated assessment environments, news about student rankings, under- and overperforming countries, and novel ideas on how to improve educational systems are prominently featured in the media. At the same time, few of these efforts have been used in a sustainable way to create new knowledge (i.e., on a scientific level), to improve learning and instruction (i.e., on a practical level), and to provide actionable advice to political stakeholders (i.e., on a policy level). Methods: This paper will adopt a meta-perspective and discuss recent and current developments with a focus on these three perspectives. There will be a particular emphasis on new assessment environments that have been recently employed in large-scale assessments. Results and Conclusions: Most findings remain very task specific. We propose a necessary steps that need to be taken in order to yield sustainable change from analysing process data on all three levels. Implications: New technologies might be capable of contributing to the research-policy-practitioner gap when it comes to utilizing the results from large-scale assessments to increase the quality of education around the globe but this will require a more systematic approach towards researching them#FRE#
#IPC# large-scale assessments; process data; replication; research-policy-practitioner gap; technology-based education#FPC#
#IRF# Azevedo R., Cromley J.G., Seibert D., Does adaptive scaffolding facilitate students' ability to regulate their learning with hypermedia?, Contemporary Educational Psychology, 29, 3, pp. 344-370, (2004); 
Bakharia A., Corrin L., de Barba P., Kennedy G., Gasevic D., Mulder R., Williams D., Dawson S., Lockyer L., A conceptual framework linking learning design with learning analytics, Proceedings of the sixth international conference on Learning Analytics & Knowledge - LAK '16, pp. 329-338, (2016); 
Bennett R.E., The changing nature of educational assessment, Review of Research in Education, 39, 1, pp. 370-407, (2015); 
Brandl L., Richters C., Radkowitsch A., Obersteiner A., Fischer M.R., Schmidmaier R., Fischer F., Stadler M., Simulation-based learning of complex skills: Predicting performance with theoretically derived process features, Psychological Test and Assessment Modeling, 63, 4, pp. 542-560, (2021); 
Breakspear S., The policy impact of PISA: An exploration of the normative effects of international benchmarking in school system performance (OECD Education Working Papers No. 71), (2012); 
Brooks C., Baker R., Andres J.M.L., Infrastructure for replication in learning analytics, Nature, (2015); 
Care E., Griffin P., McGaw B., Assessment and teaching of 21st century skills, (2012); 
Chudowsky N., Pellegrino J.W., Large-scale assessments that support learning: What will it take?, Theory Into Practice, 42, 1, pp. 75-83, (2003); 
Dawson S., Joksimovic S., Poquet O., Siemens G., Increasing the impact of learning analytics, Proceedings of the 9th international conference on Learning Analytics & Knowledge, pp. 446-455, (2019); 
Fischer F., Kollar I., Ufer S., Sodian B., Hussmann H., Pekrun R., Neuhaus B., Dorner B., Pankofer S., Fischer M., Strijbos J.-W., Heene M., Eberle J., Scientific reasoning and argumentation: Advancing an interdisciplinary research agenda in education, Frontline Learning Research, 2, 3, pp. 28-45, (2014); 
Gasevic D., Dawson S., Rogers T., Gasevic D., Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success, The Internet and Higher Education, 28, pp. 68-84, (2016); 
Goldhammer F., Hahnel C., Kroehne U., Analysing log file data from PIAAC, Methodology of educational measurement and assessment. Large-scale cognitive assessment, pp. 239-269, (2020); 
Goldhammer F., Hahnel C., Kroehne U., Zehner F., From byproduct to design factor: On validating the interpretation of process indicators based on log data, Large-scale Assessments in Education, 9, 1, pp. 1-25, (2021); 
Goldhammer F., Naumann J., Rolke H., Stelter A., Toth K., Relating product data to process data from computer-based competency assessment, Methodology of educational measurement and assessment. Competence assessment in education, pp. 407-425, (2017); 
Goldhammer F., Zehner F., What to make of and how to interpret process data, Measurement: Interdisciplinary Research and Perspectives, 15, 3-4, pp. 128-132, (2017); 
Greiff S., Molnar G., Martin R., Zimmermann J., Csapo B., Students' exploration strategies in computer-simulated complex problem environments: A latent class approach, Computers & Education, 126, pp. 248-263, (2018); 
Greiff S., Niepel C., Scherer R., Martin R., Understanding students' performance in a computer-based assessment of complex problem solving: An analysis of behavioral data from computer-generated log files, Computers in Human Behavior, 61, pp. 36-46, (2016); 
Greiff S., Wustenberg S., Avvisati F., Computer-generated log-file analyses as a window into students' minds? A showcase study based on the PISA 2012 assessment of problem solving, Computers & Education, 91, pp. 92-105, (2015); 
Gur B.S., Celik Z., Ozoglu M., Policy options for Turkey: A critique of the interpretation and utilization of PISA results in Turkey, Journal of Education Policy, 27, 1, pp. 1-21, (2012); 
Hahnel C., Ramalingam D., Kroehne U., Goldhammer F., Patterns of reading behaviour in digital hypertext environments, Journal of Computer Assisted Learning, 39, 3, (2023); 
Havnes A., Proitz T.S., Why use learning outcomes in higher education? Exploring the grounds for academic resistance and reclaiming the value of unexpected learning, Educational Assessment, Evaluation and Accountability, 28, 3, pp. 205-223, (2016); 
He Q., Borgonovi F., Paccagnella M., Leveraging process data to assess adults' problem-solving skills: Using sequence mining to identify behavioral patterns across digital tasks, Computers & Education, 166, (2021); 
He Q., von Davier M., Analyzing process data from problem-solving items with N-grams, Advances in higher education and professional development (AHEPD) book series. Handbook of research on technology tools for real-world skill development, pp. 750-777, (2016); 
Huang X., Wilson M., Wang L., Exploring plausible causes of differential item functioning in the PISA science assessment: Language, curriculum or culture, Educational Psychology, 36, 2, pp. 378-390, (2016); 
Ihantola P., Vihavainen A., Ahadi A., Butler M., Borstler J., Edwards S.H., Isohanni E., Korhonen A., Petersen A., Rivers K., Rubio M.A., Sheard J., Skupas B., Spacco J., Szabo C., Toll D., Educational data mining and learning analytics in programming, Proceedings of the 2015 ITiCSE on working group reports, pp. 41-63, (2015); 
Jiang Y., Gong T., Saldivia L.E., Cayton-Hodges G., Agard C., Using process data to understand problem-solving strategies and processes for drag-and-drop items in a large-scale mathematics assessment, Large-scale Assessments in Education, 9, 1, pp. 1-31, (2021); 
Kane M., Mislevy R., Validating score interpretations based on response processes, The NCME applications of educational measurement and assessment book series. Validation of score meaning for the next generation of assessments: The use of response processes, pp. 11-24, (2017); 
Kong X.J., Wise S.L., Bhola D.S., Setting the response time threshold parameter to differentiate solution behavior from rapid-guessing behavior, Educational and Psychological Measurement, 67, 4, pp. 606-619, (2007); 
Kroehne U., Goldhammer F., How to conceptualize, represent, and analyze log data from technology-based assessments? A generic framework and an application to questionnaire items, Behaviormetrika, 45, 2, pp. 527-563, (2018); 
Kuhlmann C., Tillmann K.-J., Mehr Ganztagsschulen als Konsequenz aus PISA? Bildungspolitische Diskurse und Entwicklungen in den Jahren 2000 bis 2003, Ganztagsschule als symbolische Konstruktion, pp. 23-45, (2009); 
Li H., Kim M.K., Xiong Y., Individual learning vs. interactive learning: A cognitive diagnostic analysis of MOOC Students' learning behaviors, American Journal of Distance Education, 34, 2, pp. 121-136, (2020); 
Lockyer L., Heathcote E., Dawson S., Informing pedagogical action: Aligning learning analytics with learning design, American Behavioral Scientist, 57, 10, pp. 1439-1459, (2013); 
Lotz C., Scherer R., Greiff S., Sparfeldt J.R., Intelligence in action â€“ Effective strategic behaviors while solving complex problems, Intelligence, 64, pp. 98-112, (2017); 
Makel M.C., Plucker J.A., Facts are more important than novelty, Educational Researcher, 43, 6, pp. 304-316, (2014); 
Mislevy R.J., Advances in measurement and cognition, The Annals of the American Academy of Political and Social Science, 683, 1, pp. 164-182, (2019); 
PISA computer-based assessment of student skills in science, (2010); 
PISA 2009 technical report, (2012); 
PISA 2012 assessment and analytical framework, (2013); 
PISA in focus, 67, (2016); 
PISA 2015 assessment and analytical framework, (2017); 
Oliveri M.E., von Davier M., Toward increasing fairness in score scale calibrations employed in international large-scale assessments, International Journal of Testing, 14, 1, pp. 1-21, (2014); 
Pohl S., Ulitzsch E., von Davier M., Reframing rankings in educational assessments, Science (New York, N.Y.), 372, 6540, pp. 338-340, (2021); 
Reis Costa D., Bolsinova M., Tijmstra J., Andersson B., Improving the precision of ability estimates using time-on-task variables: Insights from the PISA 2012 computer-based assessment of mathematics, Frontiers in Psychology, 12, (2021); 
Rutkowski L., Rutkowski D., A call for a more measured approach to reporting and interpreting PISA results, Educational Researcher, 45, 4, pp. 252-257, (2016); 
Rutkowski L., von Davier M., Rutkowski D., Handbook of international large-scale assessment, (2013); 
Skedsmo G., Huber S.G., Policies and practices related to student assessment and learning outcomesâ€”Combining different purposes and ideals, Educational Assessment, Evaluation and Accountability, 29, 3, pp. 225-228, (2017); 
Stadler M., Becker N., Godker M., Leutner D., Greiff S., Complex problem solving and intelligence: A meta-analysis, Intelligence, 53, pp. 92-101, (2015); 
Stadler M., Fischer F., Greiff S., Taking a closer look: An exploratory analysis of successful and unsuccessful strategy use in complex problems, Frontiers in Psychology, 10, (2019); 
Stadler M., Herborn K., Mustafic M., Greiff S., The assessment of collaborative problem solving in PISA 2015: An investigation of the validity of the PISA 2015 CPS tasks, Computers & Education, 157, (2020); 
Stadler M., Hofer S., Greiff S., First among equals: Log data indicates ability differences despite equal scores, Computers in Human Behavior, 111, (2020); 
Stadler M., Radkowitsch A., Schmidmaier R., Fischer M.R., Fischer F., Take your time: Invariance of time- on-task in problem solving tasks across expertise levels, Psychological Test and Assessment Modeling, 65, 4, pp. 517-525, (2020); 
Thille C., Kizilee R.F., Piech C., Halawa S.A., Greene D.K., The future of dataâ€“enriched assessment, Research & Practice in Assessment, 9, pp. 5-16, (2014); 
Ulitzsch E., He Q., Pohl S., Using sequence mining techniques for understanding incorrect behavioral patterns on interactive tasks, Journal of Educational and Behavioral Statistics, 47, 1, pp. 3-35, (2021); 
Education 2030 Incheon declaration and framework for action: Towards inclusive and equitable quality education and lifelong learning for all, (2015); 
van der Kleij F.M., Vermeulen J.A., Schildkamp K., Eggen T.J., Integrating data-based decision making, assessment for learning and diagnostic testing in formative assessment, Assessment in Education: Principles, Policy & Practice, 22, 3, pp. 324-343, (2015); 
von Davier M., Khorramdel L., He Q., Shin H.J., Chen H., Developments in psychometric population models for technology-based large-scale assessments: An overview of challenges and opportunities, Journal of Educational and Behavioral Statistics, 44, 6, pp. 671-705, (2019); 
Wagemaker H., International large-scale assessments: From research to policy. Handbook of international large-scale assessment, pp. 11-36, (2014); 
Winthrop R., Simons K.A., Can international large-scale assessments inform a global learning goal? Insights from the learning metrics task force, Research in Comparative and International Education, 8, 3, pp. 279-295, (2013); 
Wolff A., Zdrahal Z., Nikolov A., Pantucek M., Improving retention, Proceedings of the third international conference on learning analytics and knowledge - LAK '13, (2013); 
Xiao Y., He Q., Veldkamp B., Liu H., Exploring latent states of problem-solving competence using hidden Markov model on process data, Journal of Computer Assisted Learning, 37, 5, pp. 1232-1247, (2021)#FRF#
