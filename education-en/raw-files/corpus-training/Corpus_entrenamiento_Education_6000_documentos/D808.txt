#ITI#A suggestive approach for assessing item quality, usability and validity of Automatic Item Generation#FTI#
#IRE# Automatic Item Generation (AIG) refers to the process of using cognitive models to generate test items using computer modules. It is a new but rapidly evolving research area where cognitive and psychometric theory are combined into digital framework. However, assessment of the item quality, usability and validity of AIG relative to traditional item development methods lacks clarification. This paper takes a top-down strong theory approach to evaluate AIG in medical education. Two studies were conducted: Study I—participants with different levels of clinical knowledge and item writing experience developed medical test items both manually and through AIG. Both item types were compared in terms of quality and usability (efficiency and learnability); Study II—Automatically generated items were included in a summative exam in the content area of surgery. A psychometric analysis based on Item Response Theory inspected the validity and quality of the AIG-items. Items generated by AIG presented quality, evidences of validity and were adequate for testing student’s knowledge. The time spent developing the contents for item generation (cognitive models) and the number of items generated did not vary considering the participants' item writing experience or clinical knowledge. AIG produces numerous high-quality items in a fast, economical and easy to learn process, even for inexperienced and without clinical training item writers. Medical schools may benefit from a substantial improvement in cost-efficiency in developing test items by using AIG. Item writing flaws can be significantly reduced thanks to the application of AIG's models, thus generating test items capable of accurately gauging students' knowledge#FRE#
#IPC# Automatic Item Generation; Item quality; Item writing; Usability; Validity#FPC#
#IRF# Albano A.D., Rodriguez M.C., Item development research and practice, Handbook of Accessible Instruction and Testing Practices, pp. 181-198, (2018); 
Standards for educational and psychological testing, American Educational Research Association, (2018); 
Arendasy M., Sommer M., Using psychometric technology in educational assessment: The case of a schema-based isomorphic approach to the automatic generation of quantitative reasoning items, Learning and Individual Differences, 17, 4, pp. 366-383, (2007); 
Bejar I.I., Item generation: Implications for a validity argument, Automatic Item Generation: Theory and Practice, pp. 50-66, (2012); 
Bejar I.I., Lawless R., Morley M.E., Wagner M.E., Bennett R.E., Revuelta J., A feasibility study of on-the-fly item generation in adaptive testing, Journal of Technology, Learning, and Assessment, 2, 3, pp. 1-30, (2003); 
Billings M.S., Deruchie K., Hussie K., Services M., Kulesher A., Editor M., Merrell J., Swygert K.A., Tyson J., Case S.M., Haist S., Swanson D.B., Constructing written test questions for the health sciences contributors from NBME (6th ed., Issue November), National Board of Medical Examiners, (2020); 
Bonifay W.E., Reise S.P., Scheines R., Meijer R.R., When are multidimensional data unidimensional enough for structural equation modeling? An evaluation of the DETECT multidimensionality index, Structural Equation Modeling, 22, 4, pp. 504-516, (2015); 
Chalmers R.P., Mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, (2012); 
Choi J., Automatic item generation with machine learning techniques, Application of Artificial Intelligence to Assessment (Vol., 1, pp. 189-210, (2020); 
De Champlain A.F., A primer on classical test theory and item response theory for assessments in medical education, Medical Education, 44, 1, pp. 109-117, (2010); 
de Chiusole D., Stefanutti L., Anselmi P., Robusto E., Testing the actual equivalence of automatically generated items, Behavior Research Methods, 50, 1, pp. 39-56, (2018); 
Douthit N.T., Norcini J., Mazuz K., Alkan M., Feuerstein M.T., Clarfield A.M., Dwolatzky T., Solomonov E., Waksman I., Biswas S., Assessment of global health education: The role of multiple-choice questions, Frontiers in Public Health, (2021); 
Edelen M.O., Reeve B.B., Applying item response theory (IRT) modeling to questionnaire development, evaluation, and refinement, Quality of Life Research, 16, pp. 5-18, (2007); 
Embretson S.E., Kingston N.M., Automatic Item Generation: A more efficient process for developing mathematics achievement items?, Journal of Educational Measurement, 55, 1, pp. 112-131, (2018); 
Falcao F., Costa P., Pego J.M., Feasibility assurance: a review of automatic item generation in medical assessment, Advances in Health Sciences Education, pp. 1-21, (2022); 
Ferrara S., Demauro G.E., Standardized assessment of individual achievement in K-12, Educational Measurement, pp. 579-622, (2006); 
Gierl M., Haladyna T.M., Automatic Item Generation: An introduction, Automatic Item Generation: Theory and Practice, pp. 3-12, (2012); 
Gierl M., Latifi S.F., Lai H., Matovinovic D., Boughton K.A., Using automated procedures to generate test items that measure junior high science achievement, In Handbook of Research on Technology Tools for Real-World Skill Development, pp. 590-610, (2016); 
Gierl M., Lai H., The Role of Cognitive Models in Automatic Item Generation, The Handbook of Cognition and Assessment, pp. 124-145, (2016); 
Gierl M., Shin J., Firoozi T., Lai H., Using content coding and automatic item generation to improve test security, Frontiers in Education, (2022); 
Gierl M., Swygert K., Matovinovic D., Kulesher A., Lai H., Three sources of validation evidence needed to evaluate the quality of generated test items for medical licensure, Teaching and Learning in Medicine, pp. 1-11, (2022); 
Gierl M., Lai H., The role of item models in automatic item generation, International Journal of Testing, 12, 3, pp. 273-298, (2012); 
Gierl M., Lai H., Evaluating the quality of medical multiple-choice items created with automated processes, Medical Education, 47, 7, pp. 726-733, (2013); 
Gierl M., Lai H., Instructional topics in educational measurement (ITEMS) module: Using automated processes to generate test items, Educational Measurement: Issues and Practice, 32, 3, pp. 36-50, (2013); 
Gierl M., Lai H., Using automatic item generation to create solutions and rationales for computerized formative testing, Applied Psychological Measurement, 42, 1, pp. 42-57, (2018); 
Gierl M., Lai H., Pugh D., Touchie C., Boulais A.-P., de Champlain A., Evaluating the psychometric characteristics of generated multiple-choice test items, Applied Measurement in Education, 29, 3, pp. 196-210, (2016); 
Gierl M., Lai H., Tanygin V., Advanced methods in automatic item generation, (2021); 
Gierl M., Lai H., Turner S., Using automatic item generation to create multiple-choice test items, Medical Education, 46, 8, pp. 757-765, (2012); 
Grainger R., Dai W., Osborne E., Kenwright D., Medical students create multiple-choice questions for learning in pathology education: A pilot study, BMC Medical Education, 18, 1, pp. 1-8, (2018); 
Grimm K.J., Widaman K.F., Construct validity, APA handbook of research methods in psychology, Vol 1: Foundations, planning, measures, and psychometrics., pp. 621-642, (2012); 
Harrison P.M.C., Collins T., Mullensiefen D., Applying modern psychometric techniques to melodic discrimination testing: Item response theory, computerised adaptive testing, and automatic item generation, Scientific Reports, 7, 1, pp. 1-18, (2017); 
Hohensinn C., Kubinger K.D., Applying item response theory methods to examine the impact of different response formats, Educational and Psychological Measurement, 71, 4, pp. 732-746, (2011); 
Jendryczko D., Berkemeyer L., Holling H., Introducing a computerized figural memory test based on automatic item generation: An analysis with the rasch poisson counts model, Frontiers in Psychology, 11, June, pp. 1-16, (2020); 
Jeng J., Usability assessment of academic digital libraries: Effectiveness, efficiency, satisfaction, and learnability, Libri, 55, 2-3, pp. 96-121, (2005); 
Johnes J., Portela M., Thanassoulis E., Efficiency in education, Journal of the Operational Research Society, 68, 4, pp. 331-338, (2017); 
Jozefowicz R.F., Koeppen B.M., Case S., Galbraith R., Swanson D., Glew R.H., The quality of in-house medical school examinations, Academic Medicine, 77, 2, pp. 156-161, (2002); 
Kosh A.E., Distractor suites: A method for developing answer choices in automatically generated multiple-choice items, In Journal of Applied Testing Technology, 22, 1, (2021); 
Kosh A.E., Simpson M.A., Bickel L., Kellogg M., Sanford-Moore E., A cost-benefit analysis of automatic item generation, Educational Measurement: Issues and Practice, 38, 1, pp. 48-53, (2019); 
Kurdi G., Leo J., Parsia B., Sattler U., Al-Emari S., A systematic review of automatic question generation for educational purposes, International Journal of Artificial Intelligence in Education, 30, 1, pp. 121-204, (2020); 
Lai H., Alves C., Gierl M.J., Using Automatic Item Generation to address item demands for CAT, GMAC Conference on Computerized Adaptive Testing, pp. 1-16, (2009); 
Lai H., Gierl M., Touchie C., Pugh D., Boulais A.P., de Champlain A., Using Automatic Item Generation to improve the quality of MCQ distractors, Teaching and Learning in Medicine, 28, 2, pp. 166-173, (2016); 
Leighton J.P., Gierl M.J., The learning sciences in educational assessment: The role of cognitive models, (2011); 
Lewis J., Usability testing, Handbook of Human Factors and Ergonomics, 12, 1, pp. 1267-1305, (2016); 
Mair P., Modern psychometrics with R, Technometrics, 62, 1, (2018); 
Mindyarto B.N., Nugroho S.E., Linuwih S., Applying automatic item generation to create cohesive physics testlets, Journal of Physics: Conference Series, (2018); 
Patel S., Exploring the Effect of Occlusion on a Computerized Mental-Rotation Test: Implications for Automatic Item Generation, (2021); 
Patrick A., Hatzinger R., Maier M.J., Rusch T., Mair M.P., Package ‘ eRm.’, (2018); 
Pugh D., de Champlain A., Gierl M., Lai H., Touchie C., Can automated item generation be used to develop high quality MCQs that assess application of knowledge?, Research and Practice in Technology Enhanced Learning, (2020); 
Pugh D., de Champlain A., Gierl M., Lai H., Touchie C., Using cognitive models to develop quality multiple-choice questions, Medical Teacher, 38, 8, pp. 838-843, (2016); 
Rafatbakhsh E., Ahmadi A., Moloodi A., Mehrpour S., Development and validation of an Automatic Item Generation system for english idioms, Educational Measurement: Issues and Practice, 40, 2, pp. 1-11, (2020); 
Rasch G., Probabilistic models for some intelligence and attainment test, Studies in Mathematical Psychology, 1, (1960); 
Raykov T., Pohl S., Essential unidimensionality examination for multicomponent scales: An interrelationship decomposition approach, Educational and Psychological Measurement, 73, 4, pp. 581-600, (2013); 
Revelle W., Revelle M.W., Package ‘ psych.’, pp. 337-338, (2015); 
Robitzsch A., Robitzsch M.A., Package ‘sirt.’, (2021); 
Royal K.D., Hedgpeth M.-W., Jeon T., Colford C.M., Automated item generation: The future of medical education assessment?, EMJ Innovation, 2, 1, pp. 88-93, (2018); 
Rudner L., Elements of adaptive testing, Elements of Adaptive Testing, May., (2010); 
Rust J., Kosinski M., Stillwell D., Modern Psychometrics, Modern Psychometrics, (2020); 
Schmeiser C., Welch C., Test Development, Educational Measurement, pp. 307-353, (2006); 
Schmidt H.G., Mamede S., How to improve the teaching of clinical reasoning: A narrative review and a proposal, Medical Education, 49, 10, pp. 961-973, (2015); 
Shappell E., Podolej G., Ahn J., Tekian A., Park Y.S., Notes from the field: Automatic item generation, standard setting, and learner performance in mastery multiple-choice tests, Evaluation & the Health Professions, 44, 3, pp. 315-318, (2021); 
Shin E., Automated Item Generation by Combining the Non-Template and Template-Based Approaches to Generate Reading Inference Test Items, (2021); 
Shono Y., Ames S.L., Stacy A.W., Evaluation of internal validity using modern test theory: Application to word association, Psychological Assessment, 28, 2, (2016); 
Sinharay S., Johnson M.S., Williamson D.M., Calibrating item families and summarizing the results using family expected response functions, Journal of Educational and Behavioral Statistics, 28, 4, pp. 295-313, (2003); 
Stout W., Habing B., Douglas J., Kim H.R., Roussos L., Zhang J., Conditional covariance-based nonparametric multidimensionality assessment, Applied Psychological Measurement, 20, 4, pp. 331-354, (1996); 
von Davier M., Automated Item Generation with recurrent neural networks, Psychometrika, 83, 4, pp. 847-857, (2018); 
Wickham H., Chang W., Wickham M.H., Package ‘ggplot2’, Create Elegant Data Visualisations Using the Grammar of Graphics, 2, 1, pp. 1-189, (2016); 
Yaneva V., Ha L.A., Baldwin P., Mee J., Predicting item survival for multiple choice questions in a high-stakes medical exam, LREC 2020 - 12Th International Conference on Language Resources and Evaluation, Conference Proceedings, May, pp. 6812-6818, (2020); 
Yang Y., Sanyal D., Michelson J., Ainooson J., Kunda M., Automatic Item Generation of figural analogy problems: A review and outlook., (2022); 
Zegota S., Becker T., Hagmayer Y., Raupach T., Using item response theory to appraise key feature examinations for clinical reasoning, Medical Teacher, 44, pp. 1-7, (2022); 
Zhang J., Conditional covariance theory and detect for polytomous items, Psychometrika, 72, 1, pp. 69-91, (2007)#FRF#
