#ITI#Development and validation of a computational thinking test for lower primary school students#FTI#
#IRE# Computational thinking (CT) has permeated primary and early childhood education in recent years. Despite the extensive effort in CT learning initiatives, few age-appropriate assessment tools targeting young children have been developed. In this study, we proposed Computational Thinking Test for Lower Primary (CTtLP), which was designed for lower primary school students (aged 6–10). Based on the evidence-centred design approach, a set of constructed-response items that are independent of programming platforms was developed. To validate the test, content validation was first performed via expert review and cognitive interviews, and refinements were made based on the comments. Then, a large-scale field test was administered with a sample of 1st–3rd graders (N = 1225), and the data was used for psychometric analysis based on both classical test theory (CTT) and item response theory (IRT). The CTT results provided robust criterion validity, internal consistency, and test–retest reliability values. Regarding IRT results, a three-parameter logistic model was selected according to the item fit indices, based on which fair item parameters and test information reliability were generated. Overall, the test items and the whole scale showed proper fit, suggesting that CTtLP was a suitable test for the target group. Analyses of the test performance were then put forward. Results reported that students’ performance improved with grade level, and no gender difference was detected. Based on the test responses, we also identified children’s challenges in understanding CT constructs, indicating that students tended to have difficulty in understanding loop control and executing multiple directions. The study provides a rigorously validated diagnostic test for measuring CT acquisition in lower primary school students and demonstrates a replicable design and validation process for future assessment practices, and findings on the difficulties children faced in CT conceptual understanding could shed light on CT primary and early childhood education#FRE#
#IPC# Assessment; Computational thinking; Early childhood education; Evidence-centred design; Primary school#FPC#
#IRF# Aesaert K., Van Nijlen D., Vanderlinde R., van Braak J., Direct measures of digital information processing and communication skills in primary education: Using item response theory for the development and validation of an ICT competence scale, Computers & Education, 76, pp. 168-181, (2014); 
Anastasi A., Urbina S., Psychological testing, (1997); 
Atmatzidou S., Demetriadis S., Advancing students’ computational thinking skills through educational robotics: A study on age and gender relevant differences, Robotics and Autonomous Systems, 75, pp. 661-670, (2016); 
Baker F.B., The basics of item response theory, (2001); 
Basu S., Rutstein D.W., Xu Y., Wang H., Shear L., A principled approach to designing computational thinking concepts and practices assessments for upper elementary grades, Computer Science Education, (2021); 
Bell T., Curzon P., Cutts Q., Dagiene V., Haberman B., Overcoming obstacles to CS education by using non-programming outreach programmes, International Conference on Informatics in Schools: Situation, Evolution, and Perspectives, pp. 71-81, (2011); 
Bers M.U., Coding and computational thinking in early childhood: The impact of ScratchJr in Europe, European Journal of STEM Education, 3, 3, (2018); 
Bers M.U., Coding, playgrounds and literacy in early childhood education: The development of KIBO robotics and ScratchJr, 2018 IEEE Global Engineering Education Conference (EDUCON, (2018); 
Bilbao J., Bravo E., Garcia O., Varela C., Rodriguez M., Contests as a way for changing methodologies in the curriculum, The European Conference on Education, (2014); 
Bocconi S., Chioccariello A., Dettori G., Ferrari A., Engelhardt K., Kampylis P., Punie Y., Developing computational thinking in compulsory education, European Commission, JRC Science for Policy Report, (2016); 
Brennan K., Resnick M., New frameworks for studying and assessing the development of computational thinking, Proceedings of the 2012 Annual Meeting of the American Educational Research Association, (2012); 
Bubica N., Boljat I., Assessment of computational thinking: A Croatian evidence-centered design model, Informatics in Education, (2021); 
Computing Progression Pathways, (2015); 
Chen G., Shen J., Barth-Cohen L., Jiang S., Huang X., Eltoukhy M., Assessing elementary students’ computational thinking in everyday reasoning and robotics programming, Computers & Education, 109, pp. 162-175, (2017); 
Chiazzese G., Arrigo M., Chifari A., Lonati V., Tosto C., Educational robotics in primary school: Measuring the development of computational thinking skills with the bebras tasks, Informatics, 6, 4, (2019); 
K-12 Computer Science Standards, Revised 2017A, (2017); 
Progression of Computer Science Teachers Association (CSTA) K-12 Computer Science Standards, Revised, 2017b, (2017); 
Cutumisu M., Adams C., Lu C., A scoping review of empirical research on recent computational thinking assessments, Journal of Science Education and Technology, 28, 6, pp. 651-676, (2019); 
Dagiene V., Stupuriene G., Bebras: A sustainable community building model for the concept-based learning of informatics and computational thinking, Informatics in Education, 15, 1, pp. 25-44, (2016); 
de Ruiter L.E., Bers M.U., The Coding Stages Assessment: Development and validation of an instrument for assessing young children’s proficiency in the ScratchJr programming language, Computer Science Education, (2021); 
Dolgopolovas V., Jevsikova T., Dagiene V., Savulioniene L., Exploration of computational thinking of software engineering novice students based on solving computer science tasks, The International Journal of Engineering Education, 32, 3, pp. 1107-1116, (2016); 
El-Hamamsy L., Zapata-Caceres M., Barroso E.M., Mondada F., Zufferey J.D., Bruno B., The competent computational thinking test: development and validation of an unplugged computational thinking test for upper primary school, Journal of Educational Computing Research, 60, 7, (2022); 
Embretson S.E., Reise S.P., Item response theory for psychologists, (2000); 
Flannery L.P., Bers M.U., Let’s dance the “robot hokey-pokey!” children’s programming approaches and achievement throughout early cognitive development, Journal of Research on Technology in Education, 46, 1, pp. 81-101, (2013); 
George D., Mallery P., IBM SPSS statistics 26 step by step: A simple guide and reference, Routledge, (2019); 
Grover S., Pea R., Computational thinking in K–12: A review of the state of the field, Educational Researcher, 42, 1, pp. 38-43, (2013); 
Hambleton R.K., Swaminathan H., Rogers H.J., Fundamentals of item response theory (Vol, Sage, 2, (1991); 
Hu L.T., Bentler P.M., Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling: A Multidisciplinary Journal, 6, 1, pp. 1-55, (1999); 
Klinkenberg S., Straatemeier M., van der Maas H.L., Computer adaptive practice of maths ability using a new item response model for on the fly ability and difficulty estimation, Computers & Education, 57, 2, pp. 1813-1824, (2011); 
Kong S.-C., Lai M., Validating a computational thinking concepts test for primary education using item response theory: An analysis of students’ responses, Computers & Education, (2022); 
Kong S.C., Wang Y.Q., Item response analysis of computational thinking practices: Test characteristics and students’ learning abilities in visual programming contexts, Computers in Human Behavior, 122, (2021); 
Korkmaz O., Cakir R., Ozden M.Y., A validity and reliability study of the computational thinking scales (CTS), Computers in Human Behavior, 72, pp. 558-569, (2017); 
Luo F., Antonenko P.D., Davis E.C., Exploring the evolution of two girls’ conceptions and practices in computational thinking in science, Computers & Education, 146, (2020); 
Magno C., Demonstrating the difference between classical test theory and item response theory using derived test data, The International Journal of Educational and Psychological Assessment, 1, 1, pp. 1-11, (2009); 
Manske S., Werneburg S., Hoppe H.U., Learner modeling and learning analytics in computational thinking games for education, Data Analytics Approaches in Educational Games and Gamification Systems, pp. 187-212, (2019); 
Mislevy R.J., Validity by design, Educational Researcher, 36, 8, pp. 463-469, (2007); 
Mislevy R.J., Almond R.G., Lukas J.F., A brief introduction to evidence-centered design, ETS Research Report Series, 2003, 1, (2003); 
Mislevy R.J., Haertel G.D., Implications of evidence-centered design for educational testing, Educational Measurement: Issues and Practice, 25, 4, pp. 6-20, (2006); 
Moreno-Leon J., On the development of computational thinking skills in schools through computer programming with Scratch [Doctoral dissertation]., (2018); 
Moreno-Leon J., Robles G., Roman-Gonzalez M., Dr. Scratch: Automatic analysis of Scratch projects to assess and foster computational thinking, Dr. Scratch: Análisis Automático de Proyectos Scratch para Evaluar y Fomentar el Pensamiento Computacional, 46, pp. 1-23, (2015); 
Muthen B., Muthen B.O., Statistical analysis with latent variables, 123, (2009); 
Nunnally J.C., Psychometric Theory 3E, (1994); 
Paek I., Cole K., Using R for item response theory model applications, Routledge, (2019); 
Papert S., Mindstorms: Children, computers, and powerful ideas, Harvester Press, (1980); 
Perez J.E., Padrones W., Implementation of a test constructor utilizing a calibrated item bank using 3PL-IRT model, Procedia Computer Science, 197, pp. 495-502, (2022); 
Rasch G., Probabilistic models for some intelligence and attainment tests, ERIC, (1993); 
Reckase M.D., A linear logistic multidimensional model for dichotomous item response data, Handbook of Modern Item Response Theory, pp. 271-286, (1997); 
Reeve B.B., Fayers P., Applying item response theory modeling for evaluating questionnaire item and scale properties, Assessing Quality of Life in Clinical Trials: Methods of Practice, 2, pp. 55-73, (2005); 
Relkin E., de Ruiter L.E., Bers M.U., TechCheck: Development and validation of an unplugged assessment of computational thinking in early childhood education, Journal of Science Education and Technology, 29, pp. 482-498, (2020); 
Relkin E., de Ruiter L.E., Bers M.U., Learning to code and the acquisition of computational thinking by young children, Computers & Education, 169, (2021); 
Roman-Gonzalez M., Computational thinking test: Design guidelines and content validation, Proceedings of EDULEARN15 Conference, (2015); 
Roman-Gonzalez M., Moreno-Leon J., Robles G., Complementary tools for computational thinking assessment, Proceedings of International Conference on Computational Thinking Education (CTE, 2017a, (2017); 
Roman-Gonzalez M., Perez-Gonzalez J.-C., Jimenez-Fernandez C., Which cognitive abilities underlie computational thinking? Criterion validity of the Computational Thinking Test, Computers in Human Behavior, 72, pp. 678-691, (2017); 
Rowe E., Asbell-Clarke J., Almeda M.V., Gasca S., Edwards T., Bardar E., Shute V., Ventura M., Interactive Assessments of CT (IACT): Digital interactive logic puzzles to assess computational thinking in Grades 3–8, International Journal of Computer Science Education in Schools, 5, 2, pp. 28-73, (2021); 
Sahin A., Anil D., The effects of test length and sample size on item parameters in item response theory, Educational Sciences: Theory & Practice, 17, 1, pp. 321-335, (2017); 
Smith T.I., Louis K.J., Ricci B.J., Bendjilali N., Quantitatively ranking incorrect responses to multiple-choice questions using item response theory, Physical Review Physics Education Research, 16, 1, (2020); 
Snow E., Rutstein D., Basu S., Bienkowski M., Everson H.T., Leveraging evidence-centered design to develop assessments of computational thinking practices, International Journal of Testing, 19, 2, pp. 103-127, (2019); 
Strawhacker A., Lee M., Bers M.U., Teaching tools, teachers’ rules: Exploring the impact of teaching styles on young children’s programming knowledge in ScratchJr, International Journal of Technology and Design Education, 28, 2, pp. 347-376, (2018); 
Tang X., Yin Y., Lin Q., Hadad R., Zhai X., Assessing computational thinking: A systematic review of empirical studies, Computers & Education, (2020); 
Wang D., Wang T., Liu Z., A tangible programming tool for children to cultivate computational thinking, The Scientific World Journal, 2014, (2014); 
Wing J.M., Computational thinking, Communications of the ACM, 49, 3, pp. 33-35, (2006); 
Xu F., Zhang S., Understanding the source of confusion with computational thinking: A systematic review of definitions, 2021 IEEE Integrated STEM Education Conference (ISEC, (2021); 
Yen W.M., Scaling performance assessments: Strategies for managing local item dependence, Journal of Educational Measurement, 30, 3, pp. 187-213, (1993); 
Zapata-Caceres M., Martin-Barroso E., Roman-Gonzalez M., Computational thinking test for beginners: Design and content validation, 2020 IEEE Global Engineering Education Conference, (2020); 
Zhang S., Wong G.K.W., Pan G., Computational thinking test for lower primary students: Design principles, content validation, and pilot testing. 2021 IEEE International Conference on Engineering, Technology, And Education (IEEE-TALE, (2021); 
Zhang S., Wong G.K.W., Chan P.C.F., Playing coding games to learn computational thinking: What motivates students to use this tool at home?, Education and Information Technologies, 28, 1, pp. 193-216, (2023)#FRF#
