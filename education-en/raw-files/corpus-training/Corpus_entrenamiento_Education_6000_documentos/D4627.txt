#ITI#A study of meta-analyses reporting quality in the large and expanding literature of educational technology#FTI#
#IRE#As the empirical literature in educational technology continues to grow, meta-analyses are increasingly being used to synthesise research to inform practice. However, not all meta-analyses are equal. To examine their evolution over the past 30 years, this study systematically analysed the quality of 52 meta-analyses (1988–2017) on educational technology. Methodological and reporting quality is defined here as the completeness of the descriptive and methodological reporting features of meta-analyses. The study employed the Meta-Analysis Methodological Reporting Quality Guide (MMRQG), an instrument designed to assess 22 areas of reporting quality in meta-analyses. Overall, MMRQG scores were negatively related to average effect size (i.e., the higher the quality, the lower the effect size). Owing to the presence of poor-quality syntheses, the contribution of educational technologies to learning has been overestimated, potentially misleading researchers and practitioners. Nine MMRQG items discriminated between higher and lower average effect sizes. A publication date analysis revealed that older reviews (1988–2009) scored significantly lower on the MMRQG than more recent reviews (2010–2017). Although the increase in quality bodes well for the educational technology literature, many recent meta-analyses still show only moderate levels of quality. Identifying and using only best evidence-based research is thus imperative to avoid bias. Implications for practice or policy: • Educational technology practitioners should make use of meta-analytical findings that systematically synthesise primary research. • Academics, policymakers and practitioners should consider the methodological quality of meta-analyses as they vary in reliability. • Academics, policymakers and practitioners could avoid misleading bias in research evidence by using the MMRQG to evaluate the quality of meta-analyses. • Meta-analyses with lower MMRQG scores should be considered with caution as they seem to overestimate the effect of educational technology on learning#FRE#
#IPC#bias; educational technology; meta-analysis; reporting quality; research methodology; systematic review#FPC#
#IRF#Ahn S., Ames A. J., Myers N. D., A review of meta-analyses in education: Methodological strengths and weaknesses, Review of Educational Research, 82, 4, pp. 436-476, (2012); 
Amiel T., Reeves T. C., Design-based research and educational technology: Rethinking technology and the research agenda, Journal of Educational Technology & Society, 11, 4, pp. 29-40, (2008); 
Bernard R. M., Things I have learned about meta-analysis since 1990: Reducing bias in search of “The Big Picture” / Ce que j’ai appris sur la méta-analyse depuis 1990: réduire les partis pris en quête d’une vue d’ensemble, CJLT: Canadian Journal of Educational Technology, 40, 3, pp. 1-17, (2014); 
Bernard R. M., Abrami P. C., Borokhovski E., Wade C. A., Tamim R. M., Surkes M. A., Bethel E. C., A meta-analysis of three types of interaction treatments in distance education, Review of Educational Research, 79, 3, pp. 1243-1289, (2009); 
Bethel E. C., Bernard R. M., Developments and trends in synthesizing diverse forms of evidence: Beyond comparisons between distance education and classroom instruction, Distance Education, 31, 3, pp. 231-256, (2010); 
Borenstein M., Hedges L. V., Higgins J. P. T., Rothstein H. R., Introduction to meta-analysis, (2009); 
Carpenter C. R., Greenhill L. P., An investigation of closed-circuit television for teaching university courses (Report 1), (1955); 
Carpenter C. R., Greenhill L. P., An investigation of closed-circuit television for teaching university courses (Report 2), (1958); 
Cheung A. C., Slavin R. E., How features of educational technology applications affect student reading outcomes: A meta-analysis, Educational Research Review, 7, 3, pp. 198-215, (2012); 
Cheung A. C. K., Slavin R. E., The effectiveness of educational technology applications for enhancing mathematics achievement in K-12 classrooms: A meta-analysis, Educational Research Review, 9, pp. 88-113, (2013); 
Cheung A. C. K., Slavin R. E., How methodological features affect effect sizes in education, Educational Researcher, 45, 5, pp. 283-292, (2016); 
Clark R. E., Confounding in educational technology research, Journal of Educational Computing Research, 1, 2, pp. 137-148, (1985); 
Cohen J., Statistical power analysis for the behavioral sciences, (1988); 
Cook D. A., The failure of e-learning research to inform educational practice, and what we can do about it, Medical Teacher, 31, 2, pp. 158-162, (2009); 
Cooper H. M., Research synthesis and meta-analysis: A step-by-step approach (6th ed.), (2017); 
Cuban L., Jandric P., The dubious promise of educational technologies: Historical patterns and future challenges, E-Learning and Digital Media, 12, 3-4, pp. 425-439, (2015); 
D'Angelo C., Rutstein D., Harris C., Bernard R. M., Borokhovski E., Haertel G., Simulations for STEM learning: Systematic review and meta-analysis, (2014); 
Glass G. V., Primary, secondary and meta-analysis of research, Educational Researcher, 5, 10, pp. 3-8, (1976); 
Hedges L. V., Olkin I., Statistical methods for meta-analysis, (1985); 
What Works Clearinghouse; 
Jackson G. B., Methods for integrated review, Review of Educational Research, 50, 3, pp. 438-460, (1980); 
Kugley S., Wade A., Thomas J., Mahood Q., Jorgensen A-M. K., Hammerstrom K. T., Sathe N., Searching for studies: A guide to information retrieval for Campbell Systematic Reviews, (2017); 
Kulik C. L. C., Kulik J. A., Effectiveness of computer-based instruction: An updated analysis, Computers in Human Behavior, 7, 1, pp. 75-94, (1991); 
Kulik J. E., Kulik C.-L. C., Computer-based instruction: What 200 evaluations say [Paper presentation], Association for Educational Communications and Technology Annual Convention, (1987); 
Lipsey M. W., Those confounded moderators in meta-analysis: Good, bad and ugly, The Annals of the Academy of Political and Socal Science, 587, 1, pp. 69-81, (2003); 
Lipsey M. W., Wilson D. B., Practical meta-analysis, (2000); 
Littell J. H., White H., The Campbell Collaboration: Providing better evidence for a better world, Research on Social Practice, 28, 1, pp. 6-12, (2018); 
Maki W. S., Maki R. H., Multimedia comprehension skill predicts differential outcomes of Web-based and lecture courses, Journal of Experimental Psychology: Applied, 8, pp. 85-98, (2002); 
McGreal R., Comparison of the attitudes of learners taking audiographic teleconferencing courses in secondary schools in northern Ontario, Interpersonal Computing and Technology Journal, 2, 4, pp. 11-23, (1994); 
Pickup D. I., Bernard R. M., Borokhovski E., Wade A. C., Tamim R. M., Systematically searching empirical literature in the social sciences: Results from two meta-analyses within the domain of education, Russian Psychological Journal, 15, 4, pp. 245-265, (2018); 
Polanin J. R., Tanner-Smith E. E., Hennessy E. A., Estimating the difference between published and unpublished effect sizes: A meta-review, Review of Educational Research, 86, 1, pp. 207-236, (2016); 
Rosenthal R., Meta-analytic procedures for social research, (1984); 
Scammacca N., Roberts G., Stuebing K. K., Meta-analysis with complex research designs: Dealing with dependence from multiple measures and multiple group comparisons, Review of Educational Research, 84, 3, pp. 328-364, (2014); 
Schmid R. F., Bernard R. M., Borokhovski E., Tamim R. M., Abrami P. C., Surkes M. A., Wade C. A., Woods J., The effects of technology use in postsecondary education: A meta-analysis of classroom applications, Computers & Education, 72, pp. 271-291, (2014); 
Schmidt F. L., Hunter J. E., Methods of meta-analysis: Correcting error and bias in research findings, (2015); 
Slavin R. E., Best evidence synthesis: An alternative to meta-analytic and traditional reviews, Educational Researcher, 15, 9, pp. 5-11, (1996); 
Tamim R. M., Bernard R. M., Borokhovski E., Abrami P. C., Schmid R. F., What forty years of research says about the impact of technology on learning: A second-order meta-analysis and validation study, Review of Educational Research, 81, 1, pp. 4-28, (2011); 
Valentine J. C., Cooper H. M., A systematic and transparent approach for assessing the methodological quality of intervention effectiveness research: The study design and implementation assessment device (Study DIAD), Psychological Methods, 13, 2, pp. 130-149, (2008); 
Valentine J. C., Thompson S., Issues relating to confounding and meta-analysis when including non-randomized studies in systematic reviews on the effects of interventions, Research Synthesis Methods, 4, 1, pp. 26-35, (2013); 
Valentine J. C., Cooper H. M., Patall E. A., Tyson D., Robinson J. C., A method for evaluating research syntheses: The quality, conclusions, and consensus of 12 syntheses of the effects of after‐school programs, Research Synthesis Methods, 1, 1, pp. 20-38, (2010); 
Viechtbauer W., Cheung M. W.-L., Outlier and influence diagnostics for meta-analysis, Research Synthesis Methods, 1, 2, pp. 112-125, (2010)#FRF#
