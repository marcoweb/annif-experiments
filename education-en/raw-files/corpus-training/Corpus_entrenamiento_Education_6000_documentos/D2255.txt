#ITI#Parameter Estimation Accuracy of the Effort-Moderated Item Response Theory Model Under Multiple Assumption Violations#FTI#
#IRE# As low-stakes testing contexts increase, low test-taking effort may serve as a serious validity threat. One common solution to this problem is to identify noneffortful responses and treat them as missing during parameter estimation via the effort-moderated item response theory (EM-IRT) model. Although this model has been shown to outperform traditional IRT models (e.g., two-parameter logistic [2PL]) in parameter estimation under simulated conditions, prior research has failed to examine its performance under violations to the model’s assumptions. Therefore, the objective of this simulation study was to examine item and mean ability parameter recovery when violating the assumptions that noneffortful responding occurs randomly (Assumption 1) and is unrelated to the underlying ability of examinees (Assumption 2). Results demonstrated that, across conditions, the EM-IRT model provided robust item parameter estimates to violations of Assumption 1. However, bias values greater than 0.20 SDs were observed for the EM-IRT model when violating Assumption 2; nonetheless, these values were still lower than the 2PL model. In terms of mean ability estimates, model results indicated equal performance between the EM-IRT and 2PL models across conditions. Across both models, mean ability estimates were found to be biased by more than 0.25 SDs when violating Assumption 2. However, our accompanying empirical study suggested that this biasing occurred under extreme conditions that may not be present in some operational settings. Overall, these results suggest that the EM-IRT model provides superior item and equal mean ability parameter estimates in the presence of model violations under realistic conditions when compared with the 2PL model.#FRE#
#IPC# item response theory; noneffortful responding; parameter estimation; test-taking effort#FPC#
#IRF# Attali Y., Bar-Hillel M., Guess where: The position of correct answers in multiple-choice test items as a psychometric variable, Journal of Educational Measurement, 40, 2, pp. 109-128, (2003); 
Benjamini Y., Hochberg Y., Controlling the false discovery rate: A practical and powerful approach to multiple testing, Journal of the Royal Statistical Society Series B (Methodological), 57, 1, pp. 289-300, (1995); 
Chalmers R.P., mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Debeer D., Buchholz J., Hartig J., Janssen R., Student, school, and country differences in sustained test-taking effort in the 2009 PISA reading assessment, Journal of Educational and Behavioral Statistics, 39, 6, pp. 502-523, (2014); 
DeMars C.E., Wise S.L., Can differential rapid-guessing behavior lead to differential item functioning?, International Journal of Testing, 10, 3, pp. 207-229, (2010); 
Erwin T.D., Wise S.L., A scholar-practitioner model for assessment, Building a scholarship of assessment, pp. 67-81, (2002); 
Goldhammer F., Martens T., Christoph G., Ludtke O., Test-taking engagement in PIAAC, (2016); 
Guo H., Rios J.A., Haberman S., Liu O.L., Wang J., Paek I., A new procedure for detection of students’ rapid guessing responses using response time, Applied Measurement in Education, 29, 3, pp. 173-183, (2016); 
Hong M., Steedle J.T., Cheng Y., Methods of detecting insufficient effort responding: Comparisons and practical recommendations, Educational and Psychological Measurement, 80, 2, pp. 312-345, (2020); 
Hulin C.L., Lissak R.I., Drasgow F., Recovery of two- and three-parameter logistic item characteristic curves: A Monte Carlo study, Applied Psychological Measurement, 6, 3, pp. 249-260, (1982); 
Jagacinski C.M., Nicholls J.G., Reducing effort to protect perceived ability: “They’d do it, but I wouldn’t., Journal of Educational Psychology, 82, 1, pp. 15-21, (1990); 
Kim S., Moses T., Investigating robustness of item response theory proficiency estimators to atypical response behaviors under two-stage multistage testing, (2016); 
Kong X., Using response time and the effort-moderated model to investigate the effects of rapid guessing on estimation of item and person parameters, (2007); 
Kuhfeld M., Soland J., Using assessment metadata to quantify the impact of test disengagement on estimates of educational effectiveness, Journal of Research on Educational Effectiveness, 13, 1, pp. 147-175, (2020); 
Lu J., Wang C., Zhang J., Tao J., A mixture model for responses and response times with a higher-order ability structure to detect rapid guessing behaviour, British Journal of Mathematical and Statistical Psychology, 73, 2, pp. 261-288, (2020); 
Meade A.W., Craig S.B., Identifying careless responses in survey data, Psychological methods, 17, 3, pp. 437-455, (2012); 
Pastor D.A., Ong T.Q., Strickman S.N., Patterns of solution behavior across items in low-stakes assessments, Educational Assessment, 24, 3, pp. 189-212, (2019); 
Penk C., Richter D., Change in test-taking motivation and its relationship to test performance in low-stakes assessments, Educational Assessment, Evaluation and Accountability, 29, 1, pp. 55-79, (2017); 
R: A language and environment for statistical computing, (2018); 
Rios J.A., Guo H., Can culture be a salient predictor of test-taking engagement? An analysis of differential noneffortful responding on an international college-level assessment of critical thinking, Applied Measurement in Education, (2020); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of noneffortful responses on aggregated scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, 1, pp. 74-104, (2017); 
Rios J.A., Liu O.L., Bridgeman B., Identifying unmotivated examinees on student learning outcomes assessment: A comparison of two approaches, New Directions for Institutional Research, 2014, 161, pp. 69-82, (2014); 
Soland J., Are achievement gap estimates biased by differential student test effort? Putting an important policy metric to the test, Teachers College Record, 120, 12, pp. 1-26, (2018); 
Soland J., Jensen N., Keys T.D., Bi S.Z., Wolk E., Are test and academic disengagement related? Implications for measurement and practice, Educational Assessment, 24, 2, pp. 119-134, (2019); 
Thompson T., Davidson J.A., Garber J.G., Self-worth protection in achievement motivation: Performance effects and attributional behavior, Journal of Educational Psychology, 87, 4, pp. 598-610, (1995); 
Wainer H., Measurement problems, Journal of Educational Measurement, 30, 1, pp. 1-21, (1993); 
Wang C., Xu G., A mixture hierarchical model for response times and response accuracy, British Journal of Mathematical and Statistical Psychology, 68, 3, pp. 456-477, (2015); 
Wang C., Xu G., Shang Z., A two-stage approach to differentiating normal and aberrant behavior in computer based testing, Psychometrika, 83, 1, pp. 223-254, (2018); 
Wise S.L., The utility of adaptive testing in addressing the problem of unmotivated examinees, Journal of Computerized Adaptive Testing, 2, 1, pp. 1-17, (2014); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, 3, pp. 237-252, (2015); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement: Issues and Practice, 36, 4, pp. 52-61, (2017); 
Wise S.L., DeMars C.E., An application of item response time: The effort-moderated IRT model, Journal of Educational Measurement, 43, 1, pp. 19-38, (2006); 
Wise S.L., Kingsbury G.G., Modeling student test-taking motivation in the context of an adaptive achievement test, Journal of Educational Measurement, 53, 1, pp. 86-105, (2016); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, 2, pp. 163-183, (2005); 
Wise S.L., Ma L., Setting response time thresholds for a CAT item pool: The normative threshold method, (2012)#FRF#
