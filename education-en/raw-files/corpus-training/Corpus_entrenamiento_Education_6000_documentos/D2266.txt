#ITI#Is Differential Noneffortful Responding Associated With Type I Error in Measurement Invariance Testing?#FTI#
#IRE# Low test-taking effort as a validity threat is common when examinees perceive an assessment context to have minimal personal value. Prior research has shown that in such contexts, subgroups may differ in their effort, which raises two concerns when making subgroup mean comparisons. First, it is unclear how differential effort could influence evaluations of scale property equivalence. Second, if attaining full scalar invariance, the degree to which differential effort can bias subgroup mean comparisons is unknown. To address these issues, a simulation study was conducted to examine the influence of differential noneffortful responding (NER) on evaluations of measurement invariance and latent mean comparisons. Results showed that as differential rates of NER grew, increased Type I errors of measurement invariance were observed only at the metric invariance level, while no negative effects were apparent for configural or scalar invariance. When full scalar invariance was correctly attained, differential NER led to bias of mean score comparisons as large as 0.18 standard deviations with a differential NER rate of 7%. These findings suggest that test users should evaluate and document potential differential NER prior to both conducting measurement quality analyses and reporting disaggregated subgroup mean performance.#FRE#
#IPC# measurement invariance; noneffortful responding; subgroup comparisons; test-taking effort; validity#FPC#
#IRF# Standards for educational and psychological testing, (2014); 
Bandalos D.L., Relative performance of categorical diagonally weighted least squares and robust maximum likelihood estimation, Structural Equation Modeling: A Multidisciplinary Journal, 21, 1, pp. 102-116, (2014); 
Bloom H.S., Hill C.J., Black A.B., Lipsey M.W., Performance trajectories and performance gaps as achievement effect-size benchmarks for educational interventions, Journal of Research on Educational Effectiveness, 1, 4, pp. 289-328, (2008); 
Boe E.E., May H., Boruch R.F., Student task persistence in the Third International Mathematics and Science Study: A major source of achievement differences at the national, classroom, and student levels, (2002); 
Borghans L., Schils T., The leaning tower of PISA: Decomposing achievement test scores into cognitive and noncognitive components, (2012); 
Chen F.F., Sensitivity of goodness of fit indexes to lack of measurement invariance, Structural Equation Modeling: A Multidisciplinary Journal, 14, 3, pp. 464-504, (2007); 
Cheung G.W., Rensvold R.B., Evaluating goodness-of-fit indexes for testing measurement invariance, Structural Equation Modeling, 9, 2, pp. 233-255, (2002); 
Debeer D., Buchholz J., Hartig J., Janssen R., Student, school, and country differences in sustained test-taking effort in the 2009 PISA reading assessment, Journal of Educational and Behavioral Statistics, 39, 6, pp. 502-523, (2014); 
DeMars C.E., Changes in rapid-guessing behavior over a series of assessments, Educational Assessment, 12, 1, pp. 23-45, (2007); 
DeMars C.E., Type I error inflation for detecting DIF in the presence of impact, Educational and Psychological Measurement, 70, 6, pp. 961-972, (2010); 
DeMars C.E., Bashkov B.M., Socha A.B., The role of gender in test-taking motivation under low-stakes conditions, Research & Practice in Assessment, 8, pp. 69-82, (2013); 
DeMars C.E., Wise S.L., Can differential rapid-guessing behavior lead to differential item functioning?, International Journal of Testing, 10, 3, pp. 207-229, (2010); 
Dimitrov D.M., Testing for factorial invariance in the context of construct validation, Measurement and Evaluation in Counseling and Development, 43, 2, pp. 121-149, (2010); 
Finch H.W., French B.F., Hernandez Finch M.E., Comparison of methods for factor invariance testing of a 1-factor model with small samples and skewed latent traits, Frontiers in Psychology, 9, 332, pp. 1-12, (2018); 
Fischer R., Karl J.A., A primer to (cross-cultural) multi-group invariance testing possibilities in R, Frontiers in Psychology, 10, (2019); 
Goldhammer F., Martens T., Christoph G., Ludtke O., Test-taking engagement in PIAAC, (2016); 
Hu L.T., Bentler P.M., Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling: A Multidisciplinary Journal, 6, 1, pp. 1-55, (1999); 
Joo S.H., Kim E.S., Impact of error structure misspecification when testing measurement invariance and latent-factor mean difference using MIMIC and multiple-group confirmatory factor analysis, Behavior Research Methods, 51, 6, pp. 2688-2699, (2019); 
Kam C.C.S., Meyer J.P., How careless responding and acquiescence response bias can influence construct dimensionality: The case of job satisfaction, Organizational Research Methods, 18, 3, pp. 512-541, (2015); 
Kamata A., Bauer D.J., A note on the relation between factor analytic and item response theory models, Structural Equation Modeling: A Multidisciplinary Journal, 15, 1, pp. 136-153, (2008); 
Kim K.H., The relation among fit indexes, power, and sample size in structural equation modeling, Structural Equation Modeling, 12, 3, pp. 368-390, (2005); 
Kline R.B., Principles and practice of structural equation modeling, (2005); 
Kuhfeld M., Soland J., Using assessment metadata to quantify the impact of test disengagement on estimates of educational effectiveness, Journal of Research on Educational Effectiveness, 13, 1, pp. 147-175, (2020); 
Liu Y., Li Z., Liu H., Luo F., Modeling test-taking non-effort in MIRT models, Frontiers in Psychology, 10, (2019); 
Millsap R.E., Statistical approaches to measurement invariance, (2011); 
Mittelhaeuser M.A., Beguin A.A., Sijtsma K., The effect of differential motivation on IRT linking, Journal of Educational Measurement, 52, 3, pp. 339-358, (2015); 
PISA 2018 Results (Volume 1): What students know and can do, (2019); 
Osborne J.W., Blanchard M.R., Random responding from participants is a threat to the validity of social science research results, Frontiers in Psychology, 1, (2011); 
Penk C., Pohlmann C., Roppelt A., The role of test-taking motivation for students’ performance in low-stakes assessments: An investigation of school-track-specific differences, Large-Scale Assessments in Education, 2, 1, pp. 1-17, (2014); 
Penk C., Schipolowski S., Is it all about value? Bringing back the expectancy component to the assessment of test-taking motivation, Learning and Individual Differences, 42, pp. 27-35, (2015); 
Putnick D.L., Bornstein M.H., Measurement invariance conventions and reporting: The state of the art and future directions for psychological research, Developmental Review, 41, pp. 71-90, (2016); 
R: A language and environment for statistical computing, (2018); 
Rios J.A., Improving test-taking motivation on low-stakes educational assessments: A meta-analysis of interventions, Applied Measurement in Education; 
Rios J.A., Guo H., Can culture be a salient predictor of test-taking engagement? An analysis of differential NER on an international college-level assessment of critical thinking, Applied Measurement in Education, 33, 4, pp. 263-279, (2020); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of careless responding on aggregated-scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, 1, pp. 74-104, (2017); 
Rios J.A., Soland J., Parameter estimation accuracy of the Effort-Moderated Item Response Theory Model under multiple assumption violations, Educational and Psychological Measurement, (2020); 
Rosseel Y., lavaan: An R package for structural equation modeling, Journal of Statistical Software, 48, 2, pp. 1-36, (2012); 
Rutkowski L., Svetina D., Assessing the hypothesis of measurement invariance in the context of large-scale international surveys, Educational and Psychological Measurement, 74, 1, pp. 31-57, (2014); 
Schnipke D.L., (1995); 
Setzer J.C., Wise S.L., van den Heuvel J.R., Ling G., An investigation of examinee test-taking effort on a large-scale assessment, Applied Measurement in Education, 26, 1, pp. 34-49, (2013); 
Smith J.K., Given L.M., Julien H., Ouellette D., DeLong K., Information literacy proficiency: Assessing the gap in high school students’ readiness for undergraduate academic work, Library & Information Science Research, 35, 2, pp. 88-96, (2013); 
Soland J., Are achievement gap estimates biased by differential student test effort? Putting an important policy metric to the test, Teachers College Record, 120, 12, pp. 1-26, (2018); 
Soland J., Kuhfeld M., Do students rapidly guess repeatedly over time? A longitudinal analysis of student test disengagement, background, and attitudes, Educational Assessment, 24, 4, pp. 327-342, (2019); 
Stark S., Chernyshenko O.S., Drasgow F., Detecting differential item functioning with confirmatory factor analysis and item response theory: Toward a unified strategy, Journal of Applied Psychology, 91, 6, pp. 1292-1306, (2006); 
van Barneveld C., The effect of examinee motivation on test construction within an IRT framework, Applied Psychological Measurement, 31, 1, pp. 31-46, (2007); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, 3, pp. 237-252, (2015); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement: Issues and Practice, 36, 4, pp. 52-61, (2017); 
Wise S.L., Cotton M.R., Test-taking effort and score validity: The influence of student conceptions of assessment, Student perspectives on assessment: What students can tell us about assessment for learning, pp. 187-206, (2009); 
Wise S.L., DeMars C.E., Low examinee effort in low-stakes assessment: Problems and potential solutions, Educational Assessment, 10, 1, pp. 1-18, (2005); 
Wise S.L., DeMars C.E., An application of item response time: The effort-moderated IRT model, Journal of Educational Measurement, 43, 1, pp. 19-38, (2006); 
Wise S.L., DeMars C.E., A clarification of the effects of rapid guessing on coefficient alpha: A note on Attali’s reliability of speeded number-right multiple-choice tests, Applied Psychological Measurement, 33, 6, pp. 488-490, (2009); 
Wise S.L., DeMars C.E., Examinee noneffort and the validity of program assessment results, Educational Assessment, 15, 1, pp. 27-41, (2010); 
Wise S.L., Kingsbury G.G., Modeling student test-taking motivation in the context of an adaptive achievement test, Journal of Educational Measurement, 53, 1, pp. 86-105, (2016); 
Wise S.L., Kingsbury G.G., Thomason J., Kong X., (2004); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, 2, pp. 163-183, (2005); 
Wise S.L., Kuhfeld M.R., Using retest data to evaluate and improve effort? moderated scoring, Journal of Educational Measurement, (2020); 
Wolf E.J., Harrington K.M., Clark S.L., Miller M.W., Sample size requirements for structural equation models: An evaluation of power, bias, and solution propriety, Educational and Psychological Measurement, 73, 6, pp. 913-934, (2013); 
Yoon M., Lai H.C., Testing factorial invariance with unbalanced samples, Structural Equation Modeling: A Multidisciplinary Journal, 25, 2, pp. 201-213, (2018); 
Zamarro G., Hitt C., Mendez I., When students don’t care: Reexamining international differences in achievement and student effort, Journal of Human Capital, 13, 4, pp. 519-552, (2019)#FRF#
