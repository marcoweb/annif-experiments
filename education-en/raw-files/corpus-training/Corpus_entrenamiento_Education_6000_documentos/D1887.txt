#ITI#Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric Scores Determined?#FTI#
#IRE# This article investigates the feasibility of using automated scoring methods to evaluate the quality of student-written essays. In 2012, Kaggle hosted an Automated Student Assessment Prize contest to find effective solutions to automated testing and grading. This article: a) analyzes the datasets from the contest – which contained hand-graded essays – to measure their suitability for developing competent automated grading tools; b) evaluates the potential for deep learning in automated essay scoring (AES) to produce sophisticated testing and grading algorithms; c) advocates for thorough and transparent performance reports on AES research, which will facilitate fairer comparisons among various AES systems and permit study replication; d) uses both deep neural networks and state-of-the-art NLP tools to predict finer-grained rubric scores, to illustrate how rubric scores are determined from a linguistic perspective, and to uncover important features of an effective rubric scoring model. This study’s findings first highlight the level of agreement that exists between two human raters for each rubric as captured in the investigated essay dataset, that is, 0.60 on average as measured by the quadratic weighted kappa (QWK). Only one related study has been found in the literature which also performed rubric score predictions through models trained on the same dataset. At best, the predictive models had an average agreement level (QWK) of 0.53 with the human raters, below the level of agreement among human raters. In contrast, this research’s findings report an average agreement level per rubric with the two human raters’ resolved scores of 0.72 (QWK), well beyond the agreement level between the two human raters. Further, the AES system proposed in this article predicts holistic essay scores through its predicted rubric scores and produces a QWK of 0.78, a competitive performance according to recent literature where cutting-edge AES tools generate agreement levels between 0.77 and 0.81, results computed as per the same procedure as in this article. This study’s AES system goes one step further toward interpretability and the provision of high-level explanations to justify the predicted holistic and rubric scores. It contends that predicting rubric scores is essential to automated essay scoring, because it reveals the reasoning behind AIED-based AES systems. Will building AIED accountability improve the trustworthiness of the formative feedback generated by AES? Will AIED-empowered AES systems thoroughly mimic, or even outperform, a competent human rater? Will such machine-grading systems be subjected to verification by human raters, thus paving the way for a human-in-the-loop assessment mechanism? Will trust in new generations of AES systems be improved with the addition of models that explain the inner workings of a deep learning black box? This study seeks to expand these horizons of AES to make the technique practical, explainable, and trustable#FRE#
#IPC# Automated essay scoring; Deep learning; Feature importance; Natural language processing; Neural network; Rubrics#FPC#
#IRF# Abbass H.A., Social integration of artificial intelligence: Functions, automation allocation logic and human-autonomy trust, Cognitive Computation, 11, 2, pp. 159-171, (2019); 
Automatic text scoring using neural networks, (2016); 
Balota D.A., Yap M.J., Hutchison K.A., Cortese M.J., Kessler B., Loftis B., Neely J.H., Nelson D.L., Simpson G.B., Treiman R., The English lexicon project, Behavior Research Methods, 39, 3, pp. 445-459, (2007); 
Brysbaert M., Warriner A.B., Kuperman V., Concreteness ratings for 40 thousand generally known English word lemmas, Behavior Research Methods, 46, 3, pp. 904-911, (2014); 
Coltheart M., The MRC psycholinguistic database, The Quarterly Journal of Experimental Psychology Section A, 33, 4, pp. 497-505, (1981); 
Covington M.A., McFall J.D., Cutting the Gordian knot: The moving-average type-token ratio (MATTR), Journal of Quantitative Linguistics, 17, 2, pp. 94-100, (2010); 
Automated essay scoring with string kernels and word embeddings, (2018); 
Crossley S.A., Kyle K., McNamara D.S., The tool for the automatic analysis of text cohesion (TAACO): Automatic assessment of local, global, and text cohesion, Behavior Research Methods, 48, 4, pp. 1227-1237, (2016); 
Crossley S.A., Kyle K., McNamara D.S., Sentiment analysis and social cognition engine (SEANCE): An automatic tool for sentiment, social cognition, and social-order analysis, Behavior Research Methods, 49, 3, pp. 803-821, (2017); 
Crossley S.A., Bradfleld F., Bustamante A., Using human judgments to examine the validity of automated grammar, syntax, and mechanical errors in writing, Journal of Writing Research, 11, 2, pp. 251-270, (2019); 
Crossley S.A., Kyle K., Dascalu M., The tool for the automatic analysis of cohesion 2.0: Integrating semantic similarity and text overlap, Behavior Research Methods, 51, 1, pp. 14-27, (2019); 
Cummins R., Zhang M., Briscoe E.J., Constrained multi-task learning for automated essay scoring, Association for Computational Linguistics, (2016); 
Dong F., Zhang Y., Yang J., Attention-based recurrent convolutional neural network for automatic essay scoring, Proceedings of the 21St Conference on Computational Natural Language Learning (Conll, 2017, pp. 153-162, (2017); 
Dronen N., Foltz P.W., Habermehl K., Effective sampling for large-scale automated writing evaluation systems, In Proceedings of the Second (2015) ACM Conference on Learning@Scale, pp. 3-10, (2015); 
Fergadiotis G., Wright H.H., Green S.B., Psychometric evaluation of lexical diversity indices: Assessing length effects, Journal of Speech, Language, and Hearing Research, 58, 3, pp. 840-852, (2015); 
Fonti V., Belitser E., Feature selection using lasso, VU Amsterdam Research Paper in Business Analytics, (2017); 
Gregori-Signes C., Clavel-Arroitia B., Analysing lexical density and lexical diversity in university students’ written discourse, Procedia-Social and Behavioral Sciences, 198, pp. 546-556, (2015); 
Jankowska M., Conrad C., Harris J., Keselj V., N-gram based approach for automatic prediction of essay rubric marks, Advances in Artificial Intelligence, pp. 298-303, (2018); 
Johansson V., Lexical diversity and lexical density in speech and writing: A developmental perspective, Lund Working Papers in Linguistics, 53, pp. 61-79, (2009); 
Kyle K., Measuring Syntactic Development in L2 Writing: Fine Grained Indices of Syntactic Complexity and Usage-Based Indices of Syntactic Sophistication, (2016); 
Kyle K., Crossley S., Berger C., The tool for the automatic analysis of lexical sophistication (TAALES): Version 2.0, Behavior Research Methods, 50, 3, pp. 1030-1046, (2018); 
Liang G., On B.-W., Jeong D., Kim H.-C., Choi G., Automated essay scoring: A Siamese bidirectional LSTM neural network architecture, Symmetry, 10, 12, (2018); 
Liu J., Xu Y., Zhao L., Automated essay scoring based on two-stage learning, (2019); 
Lu X., Automatic analysis of syntactic complexity in second language writing, International Journal of Corpus Linguistics, 15, 4, pp. 474-496, (2010); 
Malvern D., Richards B., Chipere N., Duran P., Lexical Diversity and Language Development, (2004); 
McCarthy P.M., Jarvis S., MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment, Behavior Research Methods, 42, 2, pp. 381-392, (2010); 
Mesgar M., Strube M., A neural local coherence model for text quality assessment, In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4328-4339, (2018); 
Murdoch W.J., Singh C., Kumbier K., Abbasi-Asl R., Yu B., Definitions, methods, and applications in interpretable machine learning, Proceedings of the National Academy of Sciences, 116, 44, pp. 22071-22080, (2019); 
Perelman L., Critique of Mark D. Shermis & Ben Hammer, contrasting state-of-the-art automated scoring of essays: Analysis, Journal of Writing Assessment, 6, 1, (2013); 
Perelman L., When “the state of the art” is counting words, Assessing Writing, 21, pp. 104-111, (2014); 
Rosebrock A., Deep Learning for Computer Vision with Python, (2017); 
Shermis M.D., State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration, Assessing Writing, 20, pp. 53-76, (2014); 
Stefanowitsch A., Gries S.T., Collostructions: Investigating the interaction of words and constructions, International Journal of Corpus Linguistics, 8, 2, pp. 209-243, (2003); 
Taghipour K., Ng H.T., A neural approach to automated essay scoring, In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1882-1891, (2016); 
Torruella J., Capsada R., Lexical statistics and typological structures: A measure of lexical richness, Social and Behavioral Sciences, 95, pp. 447-454, (2013); 
Wang Y., Wei Z., Zhou Y., Huang X., Automatic essay scoring incorporating rating schema via reinforcement learning, In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 791-797, (2018); 
Woods B., Adamson D., Miel S., Mayfield E., Formative essay feedback using predictive scoring models, In Proceedings of the 23Rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 2071-2080, (2017); 
Zhao S., Zhang Y., Xiong X., Botelho A., Heffernan N., A memory-augmented neural model for automated grading, In Proceedings of the Fourth (2017) ACM Conference on Learning@Scale, pp. 189-192, (2017); 
Zupanc K., Bosnic Z., Automated essay evaluation with semantic analysis, Knowledge-Based Systems, 120, pp. 118-132, (2017)#FRF#
