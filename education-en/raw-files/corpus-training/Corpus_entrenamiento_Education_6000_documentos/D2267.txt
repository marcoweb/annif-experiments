#ITI#Detecting Rater Biases in Sparse Rater-Mediated Assessment Networks#FTI#
#IRE# Practical constraints in rater-mediated assessments limit the availability of complete data. Instead, most scoring procedures include one or two ratings for each performance, with overlapping performances across raters or linking sets of multiple-choice items to facilitate model estimation. These incomplete scoring designs present challenges for detecting rater biases, or differential rater functioning (DRF). The purpose of this study is to illustrate and explore the sensitivity of DRF indices in realistic sparse rating designs that have been documented in the literature that include different types and levels of connectivity among raters and students. The results indicated that it is possible to detect DRF in sparse rating designs, but the sensitivity of DRF indices varies across designs. We consider the implications of our findings for practice related to monitoring raters in performance assessments.#FRE#
#IPC# differential rater functioning; missing data; performance assessment; rater bias; rater effects#FPC#
#IRF# Abedi J., Lord C., The language factor in mathematics tests, Applied Measurement in Education, 14, 3, pp. 219-234, (2001); 
Andrich D.A., A rating formulation for ordered response categories, Psychometrika, 43, 4, pp. 561-573, (1978); 
Barkaoui K., Effects of marking method and rater experience on ESL essay scores and rater performance, Assessment in Education: Principles, Policy & Practice, 18, 3, pp. 279-293, (2011); 
Bergin C., Wind S.A., Grajeda S., Tsai C.-L., Teacher evaluation: Are principals? classroom observations accurate at the conclusion of training?, Studies in Educational Evaluation, 55, pp. 19-26, (2017); 
Bonk W.J., Ockey G.J., A many-facet Rasch analysis of the second language group oral discussion task, Language Testing, 20, 1, pp. 89-110, (2003); 
Bridgeman B., Lewis C., The relationship of essay and multiple-choice scores with grades in college courses, Journal of Educational Measurement, 31, 1, pp. 37-50, (1994); 
Brown G., Glasswell K., Harland D., Accuracy in the scoring of writing: Studies of reliability and validity using a New Zealand writing assessment system, Assessing Writing, 9, 2, pp. 105-121, (2004); 
Chen C.T., Hwu B.S., Improving the assessment of differential item functioning in large-scale programs with dual-scale purification of Rasch models: The PISA example, Applied Psychological Measurement, 42, 3, pp. 206-220, (2018); 
Duckor B., Castellano K.E., Tellez K., Wihardini D., Wilson M., Examining the internal structure evidence for the performance assessment for California teachers: A validation study of the Elementary Literacy Teaching Event for tier I Teacher licensure, Journal of Teacher Education, 65, 5, pp. 402-420, (2014); 
Eckes T., Introduction to many-facet Rasch measurement: Analyzing and evaluating rater-mediated assessments, (2015); 
Engelhard G., Examining rater errors in the assessment of written composition with a many-faceted Rasch model, Journal of Educational Measurement, 31, 2, pp. 93-112, (1994); 
Engelhard G., Constructing rater and task banks for performance assessments, Journal of Outcome Measurement, 1, 1, pp. 19-33, (1997); 
Engelhard G., Differential rater functioning, Rasch Measurement Transactions, 21, 3, (2008); 
Engelhard G., Wind S.A., Rating quality studies using Rasch measurement theory, (2013); 
Engelhard G., Wind S.A., Invariant measurement with raters and rating scales: Rasch models for rater-mediated assessments, (2018); 
Gamerman D., Goncalves F.B., Soares T.M., Differential item functioning, Handbook of item response theory, 3, pp. 67-86, (2018); 
Writing assessments, (2015); 
Gyagenda I.S., Engelhard G., Using classical and modern measurement theories to explore rater, domain, and gender influences on student writing ability, Journal of Applied Measurement, 10, 3, pp. 225-246, (2009); 
Hombo C.M., Donoghue J.R., Thayer D.T., A simulation study of the effect of rater designs on ability estimation, (2001); 
Johnson R.L., Penny J.A., Gordon B., Assessing performance: Designing, scoring, and validating performance tasks, (2009); 
Kondo-Brown K., A FACETS analysis of rater bias in measuring Japanese second language writing performance, Language Testing, 19, 1, pp. 3-31, (2002); 
Linacre J.M., Many-facet Rasch measurement, (1989); 
Linacre J.M., Facets Rasch measurement, (2015); 
Little R.A., Rubin D.B., Statistical analysis with missing data, (2002); 
Marais I., Andrich D.A., Diagnosing a common rater halo effect using the polytomous Rasch model, Journal of Applied Measurement, 12, 3, pp. 194-211, (2011); 
McHorney C.A., Ware J.E., Lu J.R., Sherbourne C.D., The MOS 36-item Short-Form Health Survey (SF-36): III. Tests of data quality, scaling assumptions, and reliability across diverse patient groups, Medical Care, 32, 1, pp. 40-66, (1994); 
Myford C.M., Wolfe E.W., Strengthening the ties that bind: Improving the linking network in sparsely connected rating designs, ETS Research Report Series, 2000, 1, (2000); 
Myford C.M., Wolfe E.W., Detecting and measuring rater effects using many-facet Rasch measurement: Part I, Journal of Applied Measurement, 4, 4, pp. 386-422, (2003); 
Myford C.M., Wolfe E.W., Detecting and measuring rater effects using many-facet Rasch measurement: Part II, Journal of Applied Measurement, 5, 2, pp. 189-227, (2004); 
NAEP assessments—Assessments; 
R: A language and environment for statistical computing, (2020); 
Raczynski K.R., Cohen A.S., Engelhard G., Lu Z., Comparing the effectiveness of self-paced and collaborative frame-of-reference training on rater accuracy in a large-scale writing assessment, Journal of Educational Measurement, 52, 3, pp. 301-318, (2015); 
Raju N.S., The area between two item characteristic curves, Psychometrika, 53, 4, pp. 495-502, (1988); 
Rasch G., Probabilistic models for some intelligence and achievement tests, (1980); 
Reardon S.F., Kalogrides D., Fahle E.M., Podolsky A., Zarate R.C., The relationship between test item format and gender achievement gaps on Math and ELA tests in fourth and eighth grades, Educational Researcher, 47, 5, pp. 284-294, (2018); 
Schaefer E., Rater bias patterns in an EFL writing assessment, Language Testing, 25, 4, pp. 465-493, (2008); 
Schumacker R.E., Many-facet Rasch analysis with crossed, nested, and mixed designs, Journal of Outcome Measurement, 3, 4, pp. 323-338, (1999); 
Stafford R.E., Wolfe E.W., Casabianca J.M., Song T., Detecting rater effects under rating designs with varying levels of missingness, Journal of Applied Measurement, 19, 3, pp. 243-257, (2018); 
Wells C.S., Hambleton R.K., Model fit with residual analyses, Handbook of item response theory, 2, pp. 395-413, (2016); 
Wesolowski B.C., Wind S.A., Engelhard G., Rater fairness in music performance assessment: Evaluating model-data fit and differential rater functioning, Musicae Scientiae, 19, 2, pp. 147-170, (2015); 
Wind S.A., Examining the impacts of rater effects in performance assessments, Applied Psychological Measurement, 43, 2, pp. 159-171, (2018); 
Wind S.A., Guo W., Exploring the combined effects of rater misfit and differential rater functioning in performance assessments, Educational and Psychological Measurement, 79, 5, pp. 962-987, (2019); 
Wind S.A., Jones E., The stabilizing influences of linking set size and model? Data fit in sparse rater-mediated assessment networks, Educational and Psychological Measurement, 78, 4, pp. 679-707, (2017); 
Wind S.A., Jones E., Exploring the influence of range restrictions on connectivity in sparse assessment networks: An illustration and exploration within the context of classroom observations, Journal of Educational Measurement, 55, 2, pp. 217-241, (2018); 
Wind S.A., Jones E., The effects of incomplete rating designs in combination with rater effects, Journal of Educational Measurement, 56, 1, pp. 76-100, (2019); 
Wind S.A., Jones E., Not just generalizability: A case for multifaceted latent trait models in teacher observation systems, Educational Researcher, 48, 8, pp. 521-533, (2019); 
Wind S.A., Peterson M.E., A systematic review of methods for evaluating rating quality in language assessment, Language Testing, 35, 2, pp. 161-192, (2017); 
Wind S.A., Sebok-Syer S.S., Examining differential rater functioning using a between-subgroup outfit approach, Journal of Educational Measurement, 56, 2, pp. 217-250, (2019); 
Wind S.A., Walker A.A., Exploring the correspondence between traditional score resolution methods and person fit indices in rater-mediated writing assessments, Assessing Writing, 39, pp. 25-38, (2019); 
Winke P., Gass S., Myford C., Raters’ L2 background as a potential source of bias in rating oral performance, Language Testing, 30, 2, pp. 231-252, (2012); 
Wolfe E.W., Jiao H., Song T., A family of rater accuracy models, Journal of Applied Measurement, 16, 2, pp. 153-160, (2014); 
Wolfe E.W., Matthews S., Vickers D., The effectiveness and efficiency of distributed online, regional online, and regional face-to-face training for writing assessment raters, Journal of Technology, Learning, and Assessment, 10, 1, pp. 1-21, (2010); 
Wolfe E.W., McVay A., Application of latent trait models to identifying substantively interesting raters, Educational Measurement: Issues and Practice, 31, 3, pp. 31-37, (2012); 
Wolfe E.W., Song T., Comparison of models and indices for detecting rater centrality, Journal of Applied Measurement, 16, 3, pp. 228-241, (2015); 
Wolfe E.W., Moulder B.C., Myford C.M., Detecting differential rater functioning over time (DRIFT) using a Rasch multi-faceted rating scale model, (1999); 
Wright B.D., Stone M.H., Best test design, (1979); 
Zhang B., Walker C.M., Impact of missing data on person—Model fit and person trait estimation, Applied Psychological Measurement, 32, 6, pp. 466-479, (2008); 
Zwitser R.J., Glaser S.S.F., Maris G., Monitoring countries in a changing world: A new look at DIF in international surveys, Psychometrika, 82, 1, pp. 210-232, (2017)#FRF#
