#ITI#Detecting Differential Rater Functioning in Severity and Centrality: The Dual DRF Facets Model#FTI#
#IRE# Performance assessments heavily rely on human ratings. These ratings are typically subject to various forms of error and bias, threatening the assessment outcomes’ validity and fairness. Differential rater functioning (DRF) is a special kind of threat to fairness manifesting itself in unwanted interactions between raters and performance- or construct-irrelevant factors (e.g., examinee gender, rater experience, or time of rating). Most DRF studies have focused on whether raters show differential severity toward known groups of examinees. This study expands the DRF framework and investigates the more complex case of dual DRF effects, where DRF is simultaneously present in rater severity and centrality. Adopting a facets modeling approach, we propose the dual DRF model (DDRFM) for detecting and measuring these effects. In two simulation studies, we found that dual DRF effects (a) negatively affected measurement quality and (b) can reliably be detected and compensated under the DDRFM. Using sample data from a large-scale writing assessment (N = 1,323), we demonstrate the practical measurement consequences of the dual DRF effects. Findings have implications for researchers and practitioners assessing the psychometric quality of ratings.#FRE#
#IPC# Bayesian estimation; differential rater functioning; MCMC; rater bias#FPC#
#IRF# Beech J.R., Mackintosh I.C., Do differences in sex hormones affect handwriting style? Evidence from digit ratio and sex role identity as determinants of the sex of handwriting, Personality and Individual Differences, 39, 2, pp. 459-468, (2005); 
Boulet J.R., McKinley D.W., Investigating gender-related construct-irrelevant components of scores on the written assessment exercise of a high-stakes certification assessment, Advances in Health Sciences Education, 10, 1, pp. 53-63, (2005); 
Congdon P.J., McQueen J., The stability of rater severity in large-scale assessment programs, Journal of Educational Measurement, 37, 2, pp. 163-178, (2000); 
Eckes T., Examining rater effects in TestDaf writing and speaking performance assessments: A many-facet Rasch analysis, Language Assessment Quarterly, 2, 3, pp. 197-221, (2005); 
Eckes T., Introduction to many-facet Rasch measurement: Analyzing and evaluating rater-mediated assessments, (2015); 
Eckes T., Althaus H.-J., Language proficiency assessments in higher education admissions, Higher education admission practices: An international perspective, pp. 256-275, (2020); 
Eckes T., Jin K.-Y., Measuring rater centrality effects in writing assessment: A Bayesian facets modeling approach, Psychological Test and Assessment Modeling, 63, 1, pp. 65-94, (2021); 
Engelhard G., The measurement of writing ability with a many-faceted Rasch model, Applied Measurement in Education, 5, 3, pp. 171-191, (1992); 
Engelhard G., Differential rater functioning, Rasch Measurement Transactions, 21, 3, (2008); 
Engelhard G., Wind S.A., Invariant measurement with raters and rating scales: Rasch models for rater-mediated assessments, (2018); 
Gamerman D., Goncalves F.B., Soares T.M., Differential item functioning, Handbook of item response theory, pp. 67-86, (2018); 
Der digitale TestDaF: Zielsetzung, Konzept und Testformat [The digital TestDaF: objective, conceptualization, and test design], (2020); 
Gelman A., Carlin J.B., Stern H.S., Dunson D.B., Vehtari A., Rubin D.B., Bayesian data analysis, (2013); 
Geweke J., Evaluating the accuracy of sampling-based approaches to calculating posterior moments, Bayesian statistics 4, pp. 169-193, (1992); 
Guo W., Wind S.A., Examining the impacts of ignoring rater effects in mixed-format tests, Journal of Educational Measurement, (2021); 
Hoskens M., Wilson M., Real-time feedback on rater drift in constructed-response items: An example from the Golden State examination, Journal of Educational Measurement, 38, 2, pp. 121-145, (2001); 
Jackman S., Bayesian analysis for the social sciences, (2009); 
Jin K.-Y., Eckes T., Detecting rater centrality effects in performance assessments: A model-based comparison of centrality indices, Measurement: Interdisciplinary Research and Perspectives; 
Jin K.-Y., Wang W.-C., Assessment of differential rater functioning in latent classes with new mixture facets models, Multivariate Behavioral Research, 52, 3, pp. 391-402, (2017); 
Jin K.-Y., Wang W.-C., A new facets model for rater’s centrality/extremity response style, Journal of Educational Measurement, 55, 4, pp. 543-563, (2018); 
Johnson R.L., Penny J.A., Gordon B., Assessing performance: Designing, scoring, and validating performance tasks, (2009); 
Kondo-Brown K., A FACETS analysis of rater bias in measuring Japanese second language writing performance, Language Testing, 19, 1, pp. 3-31, (2002); 
Lamprianou I., Tsagari D., Kyriakou N., The longitudinal stability of rating characteristics in an EFL examination: Methodological and substantive considerations, Language Testing, 38, 2, pp. 273-301, (2021); 
Leckie G., Baird J.-A., Rater effects on essay scoring: A multilevel analysis of severity drift, central tendency, and rater experience, Journal of Educational Measurement, 48, 4, pp. 399-418, (2011); 
Levy R., Mislevy R.J., Bayesian psychometric modeling, (2016); 
Linacre J.M., Many-facet Rasch measurement, (1989); 
Lunn D., Jackson C., Best N., Thomas A., Spiegelhalter D., The BUGS book: A practical introduction to Bayesian analysis, (2013); 
Lunz E.M., Stahl J.A., Wright B.D., The invariance of judge severity calibration, Objective measurement: Theory into practice, 3, pp. 99-112, (1996); 
Masters G.N., A Rasch model for partial credit scoring, Psychometrika, 47, 2, pp. 149-174, (1982); 
McNamara T., Knoch U., Fan J., Fairness, justice, and language assessment: The role of measurement, (2019); 
Myford C.M., Wolfe E.W., Detecting and measuring rater effects using many-facet Rasch measurement: Part I, Journal of Applied Measurement, 4, 4, pp. 386-422, (2003); 
Myford C.M., Wolfe E.W., Detecting and measuring rater effects using many-facet Rasch measurement: Part II, Journal of Applied Measurement, 5, 2, pp. 189-227, (2004); 
Myford C.M., Wolfe E.W., Monitoring rater performance over time: A framework for detecting differential accuracy and differential scale category use, Journal of Educational Measurement, 46, 4, pp. 371-389, (2009); 
Narayanon P., Swaminathan H., Identification of items that show nonuniform DIF, Applied Psychological Measurement, 20, 3, pp. 257-274, (1996); 
Norris J., Drackert A., Test review: TestDaF, Language Testing, 35, 1, pp. 149-157, (2018); 
Osterlind S.J., Everson H.T., Differential item functioning, (2009); 
Penfield R.D., Camilli G., Differential item functioning and item bias, Handbook of statistics, 26, pp. 125-167, (2007); 
Plummer M., JAGS version 4.3.0 user manual, (2017); 
Raju N.S., The area between two item characteristic curves, Psychometrika, 53, 4, pp. 495-502, (1988); 
Rubin D.B., Bayesianly justifiable and relevant frequency calculations for the applied statistician, Annals of Statistics, 12, 4, pp. 1151-1172, (1984); 
Saal F.E., Downey R.G., Lahey M.A., Rating the ratings: Assessing the psychometric quality of rating data, Psychological Bulletin, 88, 2, pp. 413-428, (1980); 
Siddiqi I., Djeddi C., Raza A., Souici-meslati L., Automatic analysis of handwriting for gender classification, Pattern Analysis and Applications, 18, 4, pp. 887-899, (2015); 
Spiegelhalter D.J., Best N.G., Carlin B.P., van der Linde A., Bayesian measures of model complexity and fit, Journal of the Royal Statistical Society, Series B, 64, 4, pp. 583-639, (2002); 
Springer D.G., Bradley K.D., Investigating adjudicator bias in concert band evaluations: An application of the many-facets Rasch model, Musicae Scientiae, 22, 3, pp. 377-393, (2018); 
Uto M., Ueno M., A generalized many-facet Rasch model and its Bayesian estimation using Hamiltonian Monte Carlo, Behaviormetrika, 47, 2, pp. 469-496, (2020); 
Wang W.-C., Su C.-M., Qiu X.-L., Item response models for local dependence among multiple ratings, Journal of Educational Measurement, 51, 3, pp. 260-280, (2014); 
Wang W.-C., Wilson M., Exploring local item dependence using a random-effects facet model, Applied Psychological Measurement, 29, 4, pp. 296-318, (2005); 
Wind S.A., Ge Y., Detecting rater biases in sparse rater-mediated assessment networks, Educational and Psychological Measurement, (2021); 
Wind S.A., Guo W., Exploring the combined effects of rater misfit and differential rater functioning in performance assessments, Educational and Psychological Measurement, 79, 5, pp. 962-987, (2019); 
Wind S.A., Jones E., The effects of incomplete rating designs in combination with rater effects, Journal of Educational Measurement, 56, 1, pp. 76-100, (2019); 
Wind S.A., Peterson M.E., A systematic review of methods for evaluating rating quality in language assessment, Language Testing, 35, 2, pp. 161-192, (2018); 
Wind S.A., Sebok-Syer S.S., Examining differential rater functioning using a between-subgroup outfit approach, Journal of Educational Measurement, 56, 2, pp. 217-250, (2019); 
Wolfe E.W., Song T., Comparison of models and indices for detecting rater centrality, Journal of Applied Measurement, 16, 3, pp. 228-241, (2015); 
Wolfe E.W., Song T., Methods for monitoring and document rating quality, The next generation of testing: Common core standards, smarter-balanced, PARCC, and the nationwide testing movement, pp. 107-142, (2016); 
Wu M., Some IRT-based analyses for interpreting rater effects, Psychological Test and Assessment Modeling, 59, 4, pp. 453-470, (2017); 
Zhang J., Same text different processing? Exploring how raters’ cognitive and meta-cognitive strategies influence rating accuracy in essay scoring, Assessing Writing, 27, 1, pp. 37-53, (2016)#FRF#
