#ITI#Assessing Argumentation Using Machine Learning and Cognitive Diagnostic Modeling#FTI#
#IRE#In this study, we developed machine learning algorithms to automatically score students’ written arguments and then applied the cognitive diagnostic modeling (CDM) approach to examine students’ cognitive patterns of scientific argumentation. We abstracted three types of skills (i.e., attributes) critical for successful argumentation practice: making claims, using evidence, and providing warrants. We developed 19 constructed response items, with each item requiring multiple cognitive skills. We collected responses from 932 students in Grades 5 to 8 and developed machine learning algorithmic models to automatically score their responses. We then applied CDM to analyze their cognitive patterns. Results indicate that machine scoring achieved the average machine–human agreements of Cohen’s κ = 0.73, SD = 0.09. We found that students were clustered in 21 groups based on their argumentation performance, each revealing a different cognitive pattern. Within each group, students showed different abilities regarding making claims, using evidence, and providing warrants to justify how the evidence supports a claim. The 9 most frequent groups accounted for more than 70% of the students in the study. Our in-depth analysis of individual students suggests that students with the same total ability score might vary in the specific cognitive skills required to accomplish argumentation. This result illustrates the advantage of CDM in assessing the fine-grained cognition of students during argumentation practices in science and other scientific practices#FRE#
#IPC#Argumentation; Cognitive diagnostic modeling; Constructed responses; Machine learning#FPC#
#IRF#Asterhan C.S.C., Schwarz B.B., Argumentation for learning: Well-trodden paths and unexplored territories, Educational Psychologist, 51, 2, pp. 164-187, (2016); 
Berland L.K., McNeill K.L., A learning progression for scientific argumentation: Understanding student work and designing supportive instructional contexts, Science Education, 94, 5, pp. 765-793, (2010); 
Bloom J.W., Discourse, cognition, and chaotic systems: An examination of students' argument about density, Journal of the Learning Sciences, 10, 4, pp. 447-492, (2001); 
Language, discourse, argumentation, and science education, Science Education, pp. 157-166, (2017); 
Cavalcanti A.P., Barbosa A., Carvalho R., Freitas F., Tsai Y.-S., Gasevic D., Mello R.F., Automatic feedback in online learning environments: A systematic literature review, Computers and Education: Artificial Intelligence, 2, (2021); 
Chen J., de la Torre J., Zhang Z., Relative and absolute fit evaluation in cognitive diagnosis modeling, Journal of Educational Measurement, 50, 2, pp. 123-140, (2013); 
Learning progressions in science: An evidence-based approach to reform, CPRE Research Reports, (2009); 
Crombie A.C., Styles of Scientific Thinking in The European Tradition: The History of Argument and Explanation Especially in The Mathematical and Biomedical Sciences and Arts, 2, (1995); 
De La Torre J., Minchen N., Cognitively diagnostic assessments and the cognitive diagnosis model framework, Psicología Educativa, 20, 2, pp. 89-97, (2014); 
Duschl R.A., Osborne J., Supporting and promoting argumentation discourse in science education, Studies in Science Education, 38, 1, pp. 39-72, (2002); 
Einstein A., Podolsky B., Rosen N., Can quantum-mechanical description of physical reality be considered complete?, Physical Review, 47, 10, (1935); 
Erduran S., Guilfoyle L., Park W., Science and religious education teachers’ views of argumentation and its teaching, Research in Science Education, 52, 2, pp. 655-673, (2020); 
Erduran S., Ozdem Y., Park J.-Y., Research trends on argumentation in science education: A journal content analysis from 1998–2014, International Journal of STEM Education, 2, 1, pp. 1-12, (2015); 
Fine A., The Einstein-Podolsky-Rosen Argument in Quantum Theory, (2004); 
Fishman E.J., Borko H., Osborne J., Gomez F., Rafanelli S., Reigh E., Tseng A., Million S., Berson E., A practice-based professional development program to support scientific argumentation from evidence in the elementary classroom, Journal of Science Teacher Education, 28, 3, pp. 222-249, (2017); 
Gao Y., Zhai X., Andersson B., Zeng P., Xin T., Developing a learning progression of buoyancy to model conceptual change: A latent class and rule space model analysis, Research in Science Education, 50, 4, pp. 1369-1388, (2020); 
Gao Y., Zhai X., Cui Y., Xin T., Bulut O., Re-validating a learning progression of buoyancy for middle school students: A longitudinal study, Research in Science Education, pp. 1-29, (2021); 
Hattie J., Timperley H., The power of feedback, Review of Educational Research, 77, 1, pp. 81-112, (2016); 
Haudek K.C., Zhai X., Exploring the effect of assessment construct complexity on machine learning scoring of argumentation, Paper Presented at Annual Conference of National Association of Research in Science Teaching, (2021); 
Henderson J.B., MacPherson A., Osborne J., Wild A., Beyond construction: Five arguments for the role and value of critique in learning science, International Journal of Science Education, 37, 10, pp. 1668-1697, (2015); 
Henderson J.B., McNeill K.L., Gonzalez-Howard M., Close K., Evans M., Key challenges and future directions for educational research on scientific argumentation, Journal of Research in Science Teaching, 55, 1, pp. 5-18, (2018); 
Hickey D.T., Taasoobshirazi G., Cross D., Assessment as learning: Enhancing discourse, understanding, and achievement in innovative science curricula, Journal of Research in Science Teaching, 49, 10, pp. 1240-1270, (2012); 
Jescovitch L.N., Scott E.E., Cerchiara J.A., Merrill J., Urban-Lurain M., Doherty J.H., Haudek K.C., Comparison of machine learning performance using analytic and holistic coding approaches across constructed response assessments aligned to a science learning progression, Journal of Science Education and Technology, 30, 2, pp. 150-167, (2020); 
Students constructing and defending evidence-based scientific explanations, In Annual Meeting of the National Association for Research in Science Teaching., pp. 1-35, (2005); 
Large J., Lines J., Bagnall A., A probabilistic classifier ensemble weighting scheme based on cross-validated accuracy estimates, Data Mining and Knowledge Discovery, 33, 6, pp. 1674-1709, (2019); 
Lee H.-S., Gweon G.-H., Lord T., Paessel N., Pallant A., Pryputniewicz S., Machine learning-enabled automated feedback: supporting students’ revision of scientific arguments based on data drawn from simulation, Journal of Science Education and Technology, 30, 2, pp. 168-192, (2021); 
Lee H.-S., Liu O.L., Pallant A., Roohr K.C., Pryputniewicz S., Buck Z.E., Assessment of uncertainty-infused scientific argumentation, Journal of Research in Science Teaching, 51, 5, pp. 581-605, (2014); 
Lee H.-S., McNamara D., Bracey Z.B., Liu O.L., Gerard L., Sherin B., Wilson C., Pallant A., Linn M., Haudek K.C., Computerized text analysis: Assessment and research potentials for promoting learning, Proceeding for International Society of Learning Sciences, (2019); 
Lemke J., Talking science: Language, learning, and values, (1990); 
Linn M.C., Gerard L., Ryoo K., McElhaney K., Liu O.L., Rafferty A.N., Computer-guided inquiry to improve science learning, Science, 344, 6180, pp. 155-156, (2014); 
Lintean M., Rus V., Azevedo R., Automatic detection of student mental models based on natural language student input during metacognitive skill training, International Journal of Artificial Intelligence in Education, 21, 3, pp. 169-190, (2012); 
Liu S., Roehrig G., Exploring science teachers’ argumentation and personal epistemology about global climate change, Research in Science Education, 49, 1, pp. 173-189, (2017); 
Ma W., de la Torre J., GDINA: An R package for cognitive diagnosis modeling, Journal of Statistical Software, 93, 14, pp. 1-26, (2020); 
Ma W., Jiang Z., Estimating cognitive diagnosis models in small samples: Bayes modal estimation and monotonic constraints, Applied Psychological Measurement, 45, 2, pp. 95-111, (2021); 
Maestrales S., Zhai X., Touitou I., Baker Q., Krajcik J., Schneider B., Using machine learning to score multi-dimensional assessments of chemistry and physics, Journal of Science Education and Technology, 30, 2, pp. 239-254, (2021); 
Mao L., Liu O.L., Roohr K., Belur V., Mulholland M., Lee H.-S., Pallant A., Validation of automated scoring for a formative assessment that employs scientific argumentation, Educational Assessment, 23, 2, pp. 121-138, (2018); 
Maris E., Estimating multiple classification latent class models, Psychometrika, 64, 2, pp. 187-212, (1999); 
Maydeu-Olivares A., Joe H., Assessing approximate fit in categorical data analysis, Multivariate Behavioral Research, 49, 4, pp. 305-328, (2014); 
McNeill K.L., Gonzalez-Howard M., Katsh-Singer R., Loper S., Pedagogical content knowledge of argumentation: Using classroom contexts to assess high-quality PCK rather than pseudoargumentation, Journal of Research in Science Teaching, 53, 2, pp. 261-290, (2016); 
McNeill K.L., Krajcik J., Synergy between teacher practices and curricular scaffolds to support students in using domain-specific and domain-general knowledge in writing arguments to explain phenomena, Journal of the Learning Sciences, 18, 3, pp. 416-460, (2009); 
Nakamura C.M., Murphy S.K., Christel M.G., Stevens S.M., Zollman D.A., Automated analysis of short responses in an interactive synthetic tutoring system for introductory physics, Physical Review Physics Education Research, 12, 1, (2016); 
A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
Nehm R.H., Ha M., Mayfield E., Transforming biology assessment with machine learning: Automated scoring of written evolutionary explanations, Journal of Science Education and Technology, 21, 1, pp. 183-196, (2012); 
Nehm R.H., Haertig H., Human vs. computer diagnosis of students’ natural selection knowledge: Testing the efficacy of text analytic software, Journal of Science Education and Technology, 21, 1, pp. 56-73, (2012); 
Lead States N.G.S.S., Criteria for procuring and evaluating high-quality and aligned summative science assessments. Retrieved on July 3, 2021, From, (2018); 
Nichols K., Gillies R., Hedberg J., Argumentation-based collaborative inquiry in science through representational work: Impact on primary students’ representational fluency, Research in Science Education, 46, 3, pp. 343-364, (2015); 
Osborne J., Arguing to learn in science: The role of collaborative, critical, Science, 463, (2010); 
Osborne J., Erduran S., Simon S., Enhancing the quality of argumentation in school science, Journal of Research in Science Teaching, 41, 10, pp. 994-1020, (2004); 
Osborne J.F., Henderson J.B., MacPherson A., Szu E., Wild A., Yao S.Y., The development and validation of a learning progression for argumentation in science, Journal of Research in Science Teaching, 53, 6, pp. 821-846, (2016); 
Osborne J.F., Patterson A., Scientific argument and explanation: A necessary distinction?, Science Education, 95, 4, pp. 627-638, (2011); 
Schwarz B.B., Neuman Y., Gil J., Ilya M., Construction of collective and individual knowledge in argumentative activity, The Journal of the Learning Sciences, 12, 2, pp. 219-256, (2003); 
Simon S., Erduran S., Osborne J., Learning to teach argumentation: Research and development in the science classroom, International Journal of Science Education, 28, 2-3, pp. 235-260, (2006); 
Toward an integration of item-response theory and cognitive error diagnosis, Monitoring Skills and Knowledge Acquisition (, pp. 453-488, (1990); 
Toulmin S.E., The uses of argument, (1958); 
Van den Eynde S., van Kampen P., Van Dooren W., De Cock M., Translating between graphs and equations: The influence of context, direction of translation, and function type, Physical Review Physics Education Research, 15, 2, (2019); 
Von Davier M., A general diagnostic model applied to language testing data, British Journal of Mathematical and Statistical Psychology, 61, 2, pp. 287-307, (2008); 
Wang W., Song L., Chen P., Meng Y., Ding S., Attribute-level and pattern-level classification consistency and accuracy indices for cognitive diagnostic assessment, Journal of Educational Measurement, 52, 4, pp. 457-476, (2015); 
Zhai X., Practices and theories: How can machine learning assist in innovative assessment practices in science education, Journal of Science Education and Technology, 30, 2, pp. 1-11, (2021); 
Zhai X., Haudek K.C., Stuhlsatz M.A., Wilson C., Evaluation of construct-irrelevant variance yielded by machine and human scoring of a science teacher PCK constructed response assessment, Studies in Educational Evaluation, 67, (2020); 
Zhai X., He P., Krajcik J., Applying machine learning to automatically assess scientific models, Journal of Research in Science Teaching, pp. 1-30, (2022); 
Zhai X., Haudek K.C., Shi L., Nehm R., Urban-Lurain M., From substitution to redefinition: A framework of machine learning-based science assessment, Journal of Research in Science Teaching, 57, 9, pp. 1430-1459, (2020); 
Zhai X., Shi L., Nehm R., A meta-analysis of machine learning-based science assessments: Factors impacting machine-human score agreements, Journal of Science Education and Technology, 30, 3, pp. 361-379, (2021); 
Zhai X., Yin Y., Pellegrino J.W., Haudek K.C., Shi L., Applying machine learning in science assessment: A systematic review, Studies in Science Education, 56, 1, pp. 111-151, (2020); 
Zhu M., Lee H.-S., Wang T., Liu O.L., Belur V., Pallant A., Investigating the impact of automated feedback on students’ scientific argumentation, International Journal of Science Education, 39, 12, pp. 1648-1668, (2017)#FRF#
