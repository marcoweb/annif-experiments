#ITI#An Item Response Theory Model for Incorporating Response Times in Forced-Choice Measures#FTI#
#IRE# Forced-choice (FC) measures have been widely used in many personality or attitude tests as an alternative to rating scales, which employ comparative rather than absolute judgments. Several response biases, such as social desirability, response styles, and acquiescence bias, can be reduced effectively. Another type of data linked with comparative judgments is response time (RT), which contains potential information concerning respondents’ decision-making process. It would be challenging but exciting to combine RT into FC measures better to reveal respondents’ behaviors or preferences in personality measurement. Given this situation, this study aims to propose a new item response theory (IRT) model that incorporates RT into FC measures to improve personality assessment. Simulation studies show that the proposed model can effectively improve the estimation accuracy of personality traits with the ancillary information contained in RT. Also, an application on a real data set reveals that the proposed model estimates similar but different parameter values compared with the conventional Thurstonian IRT model. The RT information can explain these differences.#FRE#
#IPC# forced-choice; information entropy; log-linear model; Markov chain Monte Carlo algorithm; response time; Thurstonian item response theory model#FPC#
#IRF# Bartram D., Increasing validity with forced-choice criterion measurement formats, International Journal of Selection and Assessment, 15, pp. 263-272, (2007); 
Bassili J.N., The how and why of response latency measurement in telephone surveys, Answering questions: Methodology for determining cognitive and communicative processes in survey research, pp. 319-346, (1996); 
Bassili J.N., Scott B.S., Response latency as a signal to question problems in survey research, Public Opinion Quarterly, 60, 3, pp. 390-399, (1996); 
Birnbaum A., Some latent trait models, Statistical theories of mental test scores, pp. 397-424, (1968); 
Boehm U., Marsman M., van der Maas H.L., Maris G., An attention-based diffusion model for psychometric analyses, Psychometrika, 86, 4, pp. 938-972, (2021); 
Brown A., Maydeu-Olivares A., Item response modeling of forced-choice questionnaires, Educational and Psychological Measurement, 71, pp. 460-502, (2011); 
Brown A., Maydeu-Olivares A., Fitting a Thurstonian IRT model to forced-choice data using Mplus, Behavior Research Methods, 44, pp. 1135-1147, (2012); 
Brown A., Maydeu-Olivares A., How IRT can solve problems of ipsative data in forced-choice questionnaires, Psychological Methods, 18, pp. 36-52, (2013); 
Bunji K., Okada K., Joint modeling of the two-alternative multidimensional forced-choice personality measurement and its response time by a Thurstonian D-diffusion item response model, Behavior Research Methods, 52, pp. 1091-1107, (2020); 
Burkner P.C., On the information obtainable from comparative judgments, Psychometrika, 87, pp. 1439-1472, (2022); 
Burkner P.C., Schulte N., Holling H., On the statistical and practical limitations of Thurstonian IRT models, Educational and Psychological Measurement, 79, 5, pp. 827-854, (2019); 
Callegaro M., Murakami M.H., Tepman Z., Henderson V., Yes–no answers versus check-all in self-administered modes: A systematic review and analyses, International Journal of Market Research, 57, 2, pp. 203-224, (2015); 
Casey M.M., Tryon W.W., Validating a double-press method for computer administration of personality inventory items, Psychological Assessment, 13, pp. 521-530, (2001); 
Crowne D.P., Marlowe D., A new scale of social desirability independent of psychopathology, Journal of Consulting Psychology, 24, pp. 349-354, (1960); 
de Valpine P., Paciorek C., Turek D., Michaud N., Anderson-Bergman C., Obermeyer F., Wehrhahn Cortes C., Rodriguez A., Temple Lang D., Paganin S., NIMBLE user manual (R package manual version 0.13.1), (2022); 
de Valpine P., Turek D., Paciorek C., Anderson-Bergman C., Temple Lang D., Bodik R., Programming with models: Writing statistical algorithms for general model structures with NIMBLE, Journal of Computational and Graphical Statistics, 26, pp. 403-413, (2017); 
Dueber D.M., Love A.M.A., Toland M.D., Turner T.A., Comparison of single-response format and forced-choice format instruments using Thurstonian item response theory, Educational and Psychological Measurement, 79, 1, pp. 108-128, (2018); 
Dunn T., Lushene R., O'Neil H., Complete automation of the MMPI and a study of its response latencies, Journal of Consulting and Clinical Psychology, 39, pp. 381-387, (1972); 
Edwards A.L., Edwards personal preference schedule, (1959); 
Entink R.H.K., van der Linden W.J., Fox J.P., A Box–Cox normal model for response times, British Journal of Mathematical and Statistical Psychology, 62, 3, pp. 621-640, (2009); 
Fan Z.W., Wang C., Chang H.H., Douglas J., Utilizing response time distributions for item selection in CAT, Journal of Educational and Behavioral Statistics, 37, 5, pp. 655-670, (2012); 
Fazio R., Attitudes as object-evaluation associations: Determinants, consequences, and correlates of attitude accessibility, Attitude strength: Antecedents and consequences, pp. 247-282, (1995); 
Ferrando P.J., Person-item distance and response time: An empirical study in personality measurement, Psicológica, 27, 1, pp. 137-148, (2006); 
Ferrando P.J., Lorenzo-Seva U., An item response theory model for incorporating response time data in binary personality items, Applied Psychological Measurement, 31, 6, pp. 525-543, (2007); 
Gordon R.A., Social desirability bias: A demonstration and technique for its reduction, Teaching of Psychology, 14, 1, pp. 40-42, (1987); 
Griffith R.L., Chmielowski T., Yoshita Y., Do applicants fake? An examination of the frequency of applicant faking behavior, Personnel Review, 36, 3, pp. 341-355, (2007); 
Henninger M., Plieninger H., Different styles, different times: How response times can inform our knowledge about the response process in rating scale measurement, Assessment, 28, 5, pp. 1301-1319, (2020); 
Hicks L.E., Some properties of ipsative, normative, and forced-choice normative measures, Psychological Bulletin, 74, 3, pp. 167-184, (1970); 
Holden R.R., Response latency detection of fakers on personnel tests, Canadian Journal of Behavioural Science, 27, pp. 343-355, (1995); 
Holden R.R., Hibbs N., Incremental validity of response latencies for detecting fakers on a personality test, Journal of Research in Personality, 29, pp. 362-372, (1995); 
Holden R.R., Kroner D.G., Relative efficacy of differential response latencies for detecting faking on a self-report measure of psychopathology, Psychological Assessment, 4, 2, pp. 170-173, (1992); 
Holden R.R., Lambert C.E., Response latencies are alive and well for identifying fakers on a self-report personality inventory: A reconsideration of van Hooft and Born (2012), Behavior Research, 47, pp. 1436-1442, (2015); 
Hontangas P.M., Torre D.L.J., Ponsoda V., Leenen I., Morillo D., Abad F.J., Comparing traditional and IRT scoring of forced-choice tests, Applied Psychological Measurement, 39, 8, pp. 598-612, (2015); 
Houston J.S., Borman W.C., Farmer W.F., Bearden R.M., Development of the navy computer adaptive personality scales (NCAPS), (2006); 
Jansen M., Rasch’s model for reading speed with manifest explanatory variables, Psychometrika, 62, pp. 393-409, (1997); 
Kreitchmann R.S., Sorrel M.A., Abad F.J., On bank assembly and block selection in multidimensional forced-choice adaptive assessments, Educational and Psychological Measurement, 83, 2, pp. 294-321, (2023); 
Kuiper N., Convergent evidence for the self as a prototype: The “Inverted-U RT Effect” for self and other judgements, Personality and Social Psychology Bulletin, 7, pp. 438-443, (1981); 
Kuncel R., Response processes and relative location of subject and item, Educational and Psychological Measurement, 33, pp. 545-563, (1973); 
Lin Y., Reliability estimates for IRT-based forced-choice assessment scores, Organizational Research Methods, 25, 3, pp. 575-590, (2022); 
Lee P., Lee S., Stark S., Examining validity evidence for multidimensional forced choice measures with different scoring approaches, Personality and Individual Differences, 123, pp. 229-235, (2018); 
MacCann C., Ziegler M., Roberts R.D., Faking in personality assessment, New perspectives on faking in personality assessment, pp. 309-329, (2011); 
McCrae R.R., Costa P.T., Discriminant validity of NEO-PIR facet scales, Educational and Psychological Measurement, 52, pp. 229-237, (1992); 
Morillo D., Leenen I., Abad F.J., Hontangas P.M., Torre J., de la Ponsoda V., A dominance variant under the multi-unidimensional pairwise-preference framework: Model formulation and Markov chain Monte Carlo estimation, Applied Psychological Measurement, 40, pp. 500-516, (2016); 
Neubauer A., Malle B., Questionnaire response latencies: Implications for personality assessment and self-schema theory, European Journal of Psychological Assessment, 13, pp. 109-117, (1997); 
Paulhus D.L., Measurement and control of response bias, Measures of personality and social psychological attitudes, pp. 17-59, (1991); 
Ranger J., Modeling responses and response times in personality tests with rating scales, Psychological Test and Assessment Modeling, 55, pp. 361-382, (2013); 
Ranger J., Kuhn J.T., A flexible latent trait model for response times in tests, Psychometrika, 77, 1, pp. 31-47, (2012); 
Ranger J., Ortner T., Assessing personality traits through response latencies using IRT, Educational and Psychological Measurement, 71, pp. 389-406, (2011); 
Ratcliff R., Smith P.L., Brown S.D., McKoon G., Diffusion decision model: Current issues and history, Trends in Cognitive Sciences, 20, 4, pp. 260-281, (2016); 
R Foundation for Statistical Computing, (2022); 
Salgado J.F., Tauriz G., The Five-Factor Model, forced-choice personality inventories and performance: A comprehensive meta-analysis of academic and occupational validity studies, European Journal of Work and Organizational Psychology, 23, 1, pp. 3-30, (2014); 
Schmitt N., Oswald F.L., The impact of corrections for faking on the validity of noncognitive measures in selection settings, Journal of Applied Psychology, 91, 3, pp. 613-621, (2006); 
Shannon C.E., The mathematical theory of communication, Bell Labs Technical Journal, 3, 9, pp. 31-32, (1950); 
Silver N.C., Dunlap W.P., Averaging correlation coefficients: Should Fisher’s z transformation be used?, Journal of Applied Psychology, 72, 1, pp. 146-148, (1987); 
Stark S., Chernyshenko O.S., Drasgow F., An IRT approach to constructing and scoring pairwise preference items involving stimuli on different dimensions: The multi-unidimensional pairwise-preference model, Applied Psychological Measurement, 29, pp. 184-203, (2005); 
Stark S., Chernyshenko O.S., Drasgow F., Nye C.D., White L.A., Heffner T., Farmer W.L., From ABLE to TAPAS: A new generation of personality tests to support military selection and classification decisions, Military Psychology, 26, pp. 153-164, (2014); 
Temple D.E., Geisinger K.F., Response latency to computer-administered inventory items as an indicator of emotional arousal, Journal of Personality Assessment, 54, pp. 289-297, (1990); 
Thissen D., Timed testing: An approach using item response theory, New horizons in testing, pp. 179-203, (1983); 
Thurstone L.L., A law of comparative judgment, Psychological Review, 101, 2, pp. 266-270, (1927); 
Tyron W., Mulloy J., Further validation of computer-assessed response time to emotionally evocative stimuli, Journal of Personality Assessment, 61, pp. 231-236, (1993); 
van der Linden W.J., Using response times for item selection in adaptive testing, Journal of Educational and Behavioral Statistics, 33, 1, pp. 5-20, (2008); 
van der Linden W.J., Entink R.H.K., Fox J.P., IRT parameter estimation with response times as collateral information, Applied Psychological Measurement, 34, 5, pp. 327-347, (2010); 
Van der Linden W.J., Guo F., Bayesian procedures for identifying aberrant response-time patterns in adaptive testing, Psychometrika, 73, 3, pp. 365-384, (2008); 
van der Linden W.J., van Krimpen-Stoop E.M., Using response times to detect aberrant responses in computerized adaptive testing, Psychometrika, 68, 2, pp. 251-265, (2003); 
Van Herk H., Poortinga Y., Verhallen T., Response styles in rating scales: Evidence of method bias in data from six EU countries, Journal of Cross-Cultural Psychology, 35, pp. 346-360, (2004); 
Walton K.E., Cherkasova L., Roberts R.D., On the validity of forced choice scores derived from the Thurstonian item response theory model, Assessment, 27, 4, pp. 706-718, (2020); 
Wang C., Chang H.H., Douglas J.A., The linear transformation model with frailties for the analysis of item response times, British Journal of Mathematical and Statistical Psychology, 66, 1, pp. 144-168, (2013); 
Wetzel E., Ludtke O., Zettler I., Bohnke J.R., The stability of extreme response style and acquiescence over 8 years, Assessment, 23, 3, pp. 279-291, (2016); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, 2, pp. 163-183, (2005); 
Wong C.S., Law K.S., Wong P.M., Development and validation of a forced choice emotional intelligence measure for Chinese respondents in Hong Kong, Asia Pacific Journal of Management, 21, 4, pp. 535-559, (2004); 
Zhang B., Sun T., Drasgow F., Chernyshenko O.S., Nye C.D., Stark S., White L.A., Though forced, still valid: Psychometric equivalence of forced-choice and single-item measures, Organizational Research Methods, 23, 3, pp. 569-590, (2019)#FRF#
