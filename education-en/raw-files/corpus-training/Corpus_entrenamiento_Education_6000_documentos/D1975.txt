#ITI#Reducing Workload in Short Answer Grading Using Machine Learning#FTI#
#IRE# Machine learning methods can be used to reduce the manual workload in exam grading, making it possible for teachers to spend more time on other tasks. However, when it comes to grading exams, fully eliminating manual work is not yet possible even with very accurate automated grading, as any grading mistakes could have significant consequences for the students. Here, the evaluation of an automated grading approach is therefore extended from measuring workload in relation to the accuracy of automated grading, to also measuring the overall workload required to correctly grade a full exam, with and without the support of machine learning. The evaluation was performed during an introductory computer science course with over 400 students. The exam consisted of 64 questions with relatively short answers and a two-step approach for automated grading was applied. First, a subset of answers to the exam questions was manually graded and next used as training data for machine learning models classifying the remaining answers. A number of different strategies for how to select which answers to include in the training data were evaluated. The time spent on different grading actions was measured along with the reduction of effort using clustering of answers and automated scoring. Compared to fully manual grading, the overall reduction of workload was substantial—between 64% and 74%—even with a complete manual review of all classifier output to ensure a fair grading.#FRE#
#IPC# Automatic grading; Cluster based sampling; Machine learning; Short answer grading#FPC#
#IRF# A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom’s Taxonomy of Educational Objectives, (2001); 
Azad S., Chen B., Fowler M., West M., Zilles C., Strategies for deploying unreliable ai graders in high-transparency high-stakes exams, In: International Conference on Artificial Intelligence in Education., pp. 16-28, (2020); 
Basu S., Jacobs C., Vanderwende L., Powergrading: a clustering approach to amplify human effort for short answer grading, Transactions of the Association for Computational Linguistics, 1, pp. 391-402, (2013); 
Bonthu S., Automated short answer grading using deep learning: A survey, In: Machine Learning and Knowledge Extraction: 5Th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2021, Virtual Event, 12844, (2021); 
Breiman L., Random forests. Machine learning, 45, 1, pp. 5-32, (2001); 
Brooks M., Basu S., Jacobs C., Vanderwende L., Divide and correct: Using clusters to grade short answers at scale, In: Proceedings of the First ACM Conference on Learning@ Scale Conference, pp. 89-98, (2014); 
Burrows S., Gurevych I., Stein B., The eras and trends of automatic short answer grading, International Journal of Artificial Intelligence in Education, 25, 1, pp. 60-117, (2015); 
Cohen J., Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit, Psychological bulletin, 70, 4, (1968); 
Devlin J., Chang M.W., Lee K., Toutanova K., Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding. Arxiv, 1810, (2018); 
Filighera A., Steuer T., Rensing C., Fooling automatic short answer grading systems, International Conference on Artificial Intelligence in Education. Springer, pp. 177-190, (2020); 
Galhardi L., de Souza R.C.T., Brancher J., Automatic grading of portuguese short answers using a machine learning approach, Anais Estendidos Do XVI Simpósio Brasileiro De Sistemas De Informação, SBC, pp. 109-124, (2020); 
Geigle C., Zhai C., Ferguson D.C., An exploration of automated grading of complex assignments, In: Proceedings of the Third (2016) ACM Conference on Learning@ Scale, Pp, pp. 351-360, (2016); 
Gomaa W.H., Fahmy A.A., Arabic short answer scoring with effective feedback for students, International Journal of Computer Applications, 86, 2, (2014); 
Horbach A., Palmer A., Investigating active learning for short-answer scoring, In: Proceedings of the 11Th Workshop on Innovative Use of NLP for Building Educational Applications, Pp, pp. 301-311, (2016); 
Horbach A., Pinkal M., Semi-supervised clustering for short answer scoring, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC, 2018, pp. 4066-4071, (2018); 
Horbach A., Zesch T., The influence of variance in learner answers on automatic content scoring, Frontiers in Education, Frontiers, 4, (2019); 
Horbach A., Palmer A., Wolska M., Finding a tradeoff between accuracy and rater’s workload in grading clustered short answers, In: LREC, Citeseer, Pp, pp. 588-595, (2014); 
Hunter J.D., Matplotlib: A 2d graphics environment, Computing in Science & Engineering, 9, 3, pp. 90-95, (2007); 
Kang J., Ryu K.R., Kwon H.C., Using cluster-based sampling to select initial training set for active learning in text classification, Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 384-388, (2004); 
Ke Z., Ng V., Automated essay scoring: A survey of the state of the art, IJCAI, 19, pp. 6300-6308, (2019); 
Kishaan J., Muthuraja M., Nair D., Ploger P.G., ). Using active learning for assisted short answer grading, In: ICML 2020 Workshop on Real World Experiment Design and Active Learning, (2020); 
Scaling Short-Answer Grading by Combining Peer Assessment with Algorithmic Scoring., pp. 99-108, (2014); 
Kumar S., Chakrabarti S., Roy S., Earth mover’s distance pooling over siamese lstms for automatic short answer grading, In: IJCAI, Pp, pp. 2046-2052, (2017); 
Leacock C., Chodorow M., C-rater: Automated scoring of short-answer questions, Computers and the Humanities, 37, 4, pp. 389-405, (2003); 
Lun J., Zhu J., Tang Y., Yang M., Multiple data augmentation strategies for improving performance on automatic short answer scoring, Proceedings of the AAAI Conference on Artificial Intelligence, 34, pp. 13389-13396, (2020); 
Madnani N., Cahill A., Automated scoring: Beyond natural language processing, In: Proceedings of the 27Th International Conference on Computational Linguistics, Pp, pp. 1099-1109, (2018); 
Marvaniya S., Saha S., Dhamecha T.I., Foltz P., Sindhgatta R., Sengupta B., Creating scoring rubric from representative student answers for improved short answer grading, In: Proceedings of the 27Th ACM International Conference on Information and Knowledge Management, pp. 993-1002, (2018); 
Mieskes M., Pado U., Work smart-reducing effort in short-answer grading, Proceedings of the 7Th Workshop on NLP for Computer Assisted Language Learning (NLP4CALL 2018) at SLTC, Stockholm, 7Th November 2018, Linköping University Electronic Press, 152, pp. 57-68, (2018); 
Mohler M., Bunescu R., Mihalcea R., Learning to grade short answer questions using semantic similarity measures and dependency graph alignments, In: Proceedings of the 49Th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 752-762, (2011); 
Nielsen R.D., Ward W.H., Martin J.H., Learning to assess low-level conceptual understanding, In: Flairs Conference, pp. 427-432, (2008); 
Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay E., Scikit-learn: Machine learning in Python, Journal of Machine Learning Research, 12, pp. 2825-2830, (2011); 
Stanza: A Python Natural Language Processing Toolkit for Many Human Language, (2020); 
Reimers N., Gurevych I., Sentence-bert: Sentence embeddings using siamese bert-networks, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, (2019); 
Reimers N., Gurevych I., Making monolingual sentence embeddings multilingual using knowledge distillation, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, (2020); 
Riordan B., Horbach A., Cahill A., Zesch T., Lee C., Investigating neural architectures for short answer scoring, In: Proceedings of the 12Th Workshop on Innovative Use of NLP for Building Educational Applications, Pp, pp. 159-168, (2017); 
Settles B., (2009). Active Learning Literature Survey, (1648); 
Singhal A., Et al., Modern information retrieval: A brief overview, IEEE Data Eng Bull, 24, 4, pp. 35-43, (2001); 
Souza D.M., Felizardo K.R., Barbosa E.F., A systematic literature review of assessment tools for programming assignments, 2016 IEEE 29Th International Conference on Software Engineering Education and Training (CSEET), IEEE, pp. 147-156, (2016); 
Sung C., Dhamecha T.I., Mukhi N., Improving short answer grading using transformer-based pre-training, In: International Conference on Artificial Intelligence in Education, Springer, Pp, pp. 469-481, (2019); 
Yen S.J., Lee Y.S., Cluster-based under-sampling approaches for imbalanced data distributions, Expert Systems with Applications, 36, 3, pp. 5718-5727, (2009); 
Zesch T., Heilman M., Cahill A., Reducing annotation efforts in supervised short answer scoring, Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, pp. 124-132, (2015)#FRF#
