#ITI#A Multilevel Mixture IRT Framework for Modeling Response Times as Predictors or Indicators of Response Engagement in IRT Models#FTI#
#IRE# Disengaged item responses pose a threat to the validity of the results provided by large-scale assessments. Several procedures for identifying disengaged responses on the basis of observed response times have been suggested, and item response theory (IRT) models for response engagement have been proposed. We outline that response time-based procedures for classifying response engagement and IRT models for response engagement are based on common ideas, and we propose the distinction between independent and dependent latent class IRT models. In all IRT models considered, response engagement is represented by an item-level latent class variable, but the models assume that response times either reflect or predict engagement. We summarize existing IRT models that belong to each group and extend them to increase their flexibility. Furthermore, we propose a flexible multilevel mixture IRT framework in which all IRT models can be estimated by means of marginal maximum likelihood. The framework is based on the widespread Mplus software, thereby making the procedure accessible to a broad audience. The procedures are illustrated on the basis of publicly available large-scale data. Our results show that the different IRT models for response engagement provided slightly different adjustments of item parameters of individuals’ proficiency estimates relative to a conventional IRT model.#FRE#
#IPC# item response theory; multilevel mixture modeling; response engagement; response time#FPC#
#IRF# Asparouhov T., Muthen B., Multilevel mixture models, Advances in latent variable mixture models, pp. 27-51, (2008); 
Cui Z., On a new algorithm for removing repeating patterns in similarity analysis, Educational and Psychological Measurement, 80, pp. 446-460, (2020); 
Darrell Bock R., Lieberman M., Fitting a response model for n dichotomously scored items, Psychometrika, 35, pp. 179-197, (1970); 
De Boeck P., Jeon M., An overview of models for response times and processes in cognitive tests, Frontiers in Psychology, 10, (2019); 
Goldhammer F., Martens T., Christoph G., Ludtke O., Test-taking engagement in PIAAC (OECD education working papers no. No. 133), (2016); 
Goldhammer F., Naumann J., Kessel Y., Assessing individual differences in basic computer skills, European Journal of Psychological Assessment, 29, pp. 263-275, (2013); 
Guo H., Rios J.A., Haberman S., Liu O.L., Wang J., Paek I., A new procedure for detection of students’ rapid guessing responses using response time, Applied Measurement in Education, 29, pp. 173-183, (2016); 
Klein Entink R.H., Kuhn J.T., Hornke L.F., Fox J.P., Evaluating cognitive theory: A joint modeling approach using responses and response times, Psychological Methods, 14, pp. 54-75, (2009); 
Lee Y.H., Jia Y., Using response time to investigate students’ test-taking behaviors in a NAEP computer-based study, Large-scale Assessments in Education, 2, pp. 1-24, (2014); 
Lindner M.A., Ludtke O., Nagy G., The onset of rapid-guessing behavior over the course of testing time: A matter of motivation and cognitive resources, Frontiers in Psychology, 10, (2019); 
List M.K., Robitzsch A., Ludtke O., Koller O., Nagy G., Performance decline in low-stakes educational assessments: Different mixture modeling approaches, Large-scale Assessments in Education, 5, (2017); 
Lubke G.H., Muthen B., Investigating population heterogeneity with factor mixture models, Psychological Methods, 10, 1, (2005); 
Lu J., Wang C., Zhang J., Tao J., A mixture model for responses and response times with a higher-order ability structure to detect rapid guessing behaviour, British Journal of Mathematical and Statistical Psychology, 73, pp. 261-288, (2020); 
Molenaar D., de Boeck P., Response mixture modeling: Accounting for heterogeneity in item characteristics across response times, Psychometrika, 83, pp. 279-297, (2018); 
Molenaar D., Oberski D., Vermunt J., De Boeck P., Hidden Markov item response theory models for responses and response times, Multivariate Behavioral Research, 51, pp. 606-626, (2016); 
Muthen L.K., Muthen B.O., Mplus user’s guide, (2017); 
Nagy G., Nagengast B., Becker M., Rose N., Frey A., Item position effects in a reading comprehension test: An IRT study of individual differences and individual correlates, Psychological Test and Assessment Modeling, 60, pp. 165-187, (2018); 
Nagy G., Nagengast B., Frey A., Becker M., Rose N., A multilevel study of position effects in PISA achievement tests: Student- and school-level predictors in the German tracked school system, Assessment in Education Principles Policy and Practice, 26, 4, pp. 422-443, (2019); 
PISA 2015 technical report, (2017); 
Pokropek A., Grade of membership response time model for detecting guessing behaviors, Journal of Educational and Behavioral Statistics, 41, 3, pp. 300-325, (2016); 
Rios J.A., Guo H., Mao L., Liu O.L., Evaluating the impact of careless responding on aggregated-scores: To filter unmotivated examinees or not?, International Journal of Testing, 17, pp. 74-104, (2017); 
Schnipke D.L., Scrams D.J., Modeling item response times with a two-state mixture model: A new method of measuring speededness, Journal of Educational Measurement, 34, 3, pp. 213-232, (1997); 
Ulitzsch E., Davier M., Pohl S., A hierarchical latent response model for inferences about examinee engagement in terms of guessing and item-level non-response, British Journal of Mathematical and Statistical Psychology, 73, pp. 83-112, (2020); 
van Barneveld C., Pharand S.-L., Ruberto L., Haggarty D., Student motivation in large-scale assessments, Improving large-scale assessment in education: Theory, issues and practice, pp. 43-61, (2013); 
Van Den Noortgate W., De Boeck P., Meulders M., Cross-classification multilevel logistic models in psychometrics, Journal of Educational and Behavioral Statistics, 28, pp. 369-386, (2003); 
van der Linden W.J., A lognormal model for response times on test items, Journal of Educational and Behavioral Statistics, 31, pp. 181-204, (2006); 
van der Linden W.J., A hierarchical framework for modeling speed and accuracy on test items, Psychometrika, 72, pp. 287-308, (2007); 
Vermunt J.K., 7. Multilevel latent class models, Sociological Methodology, 33, pp. 213-239, (2003); 
Vermunt J.K., Multilevel latent variable modeling: An application in education testing, Austrian Journal of Statistics, 37, pp. 285-299, (2008); 
Wang C., Xu G., A mixture hierarchical model for response times and response accuracy, British Journal of Mathematical and Statistical Psychology, 68, pp. 456-477, (2015); 
Wang C., Xu G., Shang Z., Kuncel N., Detecting aberrant behavior and item preknowledge: A comparison of mixture modeling method and residual method, Journal of Educational and Behavioral Statistics, 43, 4, (2018); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, pp. 237-252, (2015); 
Wise S.L., Rapid-guessing behavior: Its identification, interpretation, and implications, Educational Measurement Issues and Practice, 36, pp. 52-61, (2017); 
Wise S.L., An information-based approach to identifying rapid-guessing thresholds, Applied Measurement in Education, 32, pp. 325-336, (2019); 
Wise S.L., DeMars C.E., An application of item response time: The effort-moderated IRT model, Journal of Educational Measurement, 43, pp. 19-38, (2006); 
Wise S.L., Kong X., Response time effort: A new measure of examinee motivation in computer-based tests, Applied Measurement in Education, 18, pp. 163-183, (2005); 
Wise S.L., Ma L., Setting response time thresholds for a CAT item pool: The normative threshold method, (2012); 
Wise S.L., Smith L.F., The validity of assessment when students don’t give good effort, Handbook of human and social conditions in assessment, pp. 204-220, (2016)#FRF#
