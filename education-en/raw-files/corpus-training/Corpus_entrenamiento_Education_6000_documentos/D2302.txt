#ITI#Evaluating Different Scoring Methods for Multiple Response Items Providing Partial Credit#FTI#
#IRE# The multiple response structure can underlie several different technology-enhanced item types. With the increased use of computer-based testing, multiple response items are becoming more common. This response type holds the potential for being scored polytomously for partial credit. However, there are several possible methods for computing raw scores. This research will evaluate several approaches found in the literature using an approach that evaluates how the inclusion of scoring related to the selection/nonselection of both relevant and irrelevant information is incorporated extending Wilson’s approach. Results indicated all methods have potential, but the plus/minus and true/false methods seemed the most promising for items using the “select all that apply” instruction set. Additionally, these methods showed a large increase in information per time unit over the dichotomous method.#FRE#
#IPC# multiple response items; polytomous scoring; technology-enhanced items#FPC#
#IRF# Standards for educational and psychological testing, (2014); 
Bauer D., Holzer M., Kopp V., Fischer M., Pick-N multiple choice-exams: A comparison of scoring algorithms, Advances in Health Sciences Education, 16, 2, pp. 211-221, (2011); 
Becker K.A., Soni H., (2013); 
Betts J., Exploring innovative item types on computer-based testing, (2013); 
Betts J., Measuring clinical judgment in nursing: Integrating technology enhanced items, (2018); 
Betts J., Muntean W., Using signal detection theory and IRT methods with multiple response items, (2017); 
Biggs J.B., Collis K.F., Evaluating the quality of learning: The SOLO taxonomy, (1982); 
Bock R.D., Estimating item parameters and latent ability when responses are scored in two or more nominal categories, Psychometrika, 37, 1, pp. 29-51, (1972); 
Clyne C.M., The effects of different scoring methodologies on item and test characteristics of technology-enhanced items, (2015); 
Collet L.S., Elimination scoring: An empirical evaluation, Journal of Educational Measurement, 8, 3, pp. 209-214, (1971); 
Coombs C.H., Milholland J.E., Womer J.F., The assessment of partial knowledge, Educational and Psychological Measurement, 16, 1, pp. 13-37, (1956); 
Couch B.A., Hubbard J.K., Brassil C.E., Multiple-true-false questions reveal the limits of the multiple-choice format for detecting students with incomplete understandings, BioScience, 68, 6, pp. 455-463, (2018); 
Cronbach L.J., Note on the multiple true-false test exercise, Journal of Educational Psychology, 30, 8, pp. 628-631, (1939); 
Cronbach L.J., An experimental comparison of multiple true-false and multiple multiple-choice tests, Journal of Educational Psychology, 32, 7, pp. 533-543, (1941); 
Cronbach L.J., Response sets and test validity, Educational and Psychological Measurement, 6, 4, pp. 475-494, (1946); 
DeCarlo L.T., Signal detection theory and generalized linear models, Psychological Methods, 3, 2, pp. 186-205, (1998); 
DeCarlo L.T., Signal detection theory with item effects, Journal of Mathematical Psychology, 55, 3, pp. 229-239, (2011); 
DeMars C.E., (2008); 
Domnich A., Panatto D., Arata L., Bevilacqua I., Apprato L., Gasparini R., Amicizia D., Impact of different scoring algorithms applied to multiple-mark survey items on outcome assessment: An in-field study on health-related knowledge, Journal of Preventive Medicine and Hygiene, 56, 4, pp. E162-E171, (2015); 
Dressel P.L., Schmid J., Some modifications of the multiple-choice item, Educational and Psychological Measurement, 13, 4, pp. 574-595, (1953); 
Duncan G.T., Milton E.O., Multiple-answer multiple-choice test items: Responding and scoring through Bayes and minimax strategies, Psychometrika, 43, 1, pp. 43-57, (1978); 
Eggen T.J.H.M., Lampe T.T.M., Comparison of the reliability of scoring methods of multiple-response items, matching items, and sequencing items, Cadmo, 19, 2, pp. 85-104, (2011); 
Frary R.B., Partial-credit scoring methods for multiple-choice tests, Applied Measurement in Education, 2, 1, pp. 79-96, (1989); 
Frisbie D.A., Sweeney D.C., The relative merits of multiple true-false achievement tests, Journal of Educational Measurement, 19, 1, pp. 29-35, (1982); 
Haladyna T.M., Kramer G., An empirical investigation of poly-scoring of multiple-choice item responses, (2005); 
Hohensinn C., Kubinger K.D., Applying item response theory methods to examine the impact of different response formats, Educational and Psychological Measurement, 71, 4, pp. 732-746, (2011); 
Hsu T.-C., Moss P.A., Khampalikit C., The merits of multiple-answer items as evaluated by using six scoring formulas, Journal of Experimental Education, 52, 3, pp. 152-158, (1984); 
Jiao H., Liu J., Hynie K., Woo A., Gorham J., Comparison between dichotomous and polytomous scoring on innovative items in a large-scale computerized adaptive test, Educational and Psychological Measurement, 72, 3, pp. 493-509, (2012); 
Jodoin M.G., Measurement efficiency of innovative item formats in computer-based testing, Journal of Educational Measurement, 40, 1, pp. 1-15, (2003); 
Jorion N., Betts J., Kim D., Muntean W., (2019); 
Kane M., Validation strategies: Delineating and validating proposed interpretations and uses of test scores, Handbook of test development, pp. 64-80, (2016); 
Kao S.-C., Betts J., Exploring item scoring methods for technology-enhanced items in computerized adaptive tests, (2019); 
Kim D., Woo A., Betts J., Muntean W., Evaluating scoring models to align with proposed cognitive constructs underlying item content, (2018); 
Knuth D.E., The art of computer programming: Sorting and searching, 3, (1998); 
Koch D.A., Testing goes graphical, Journal of Interactive Instruction Development, 5, pp. 14-21, (1993); 
Kutner M.K., Nachtsheim C.J., Neter J., Li W., Applied linear statistical models, (2005); 
Linacre J.M., (2020); 
Lord F.M., Applications of item response theory to practical testing problems, (1980); 
Lorie W., Application of a scoring framework for technology-enhanced items, (2014); 
Masters G.N., A Rasch model for partial credit scoring, Psychometrika, 47, 2, pp. 149-174, (1982); 
Meng H., Han C., Comparison of pretest item calibration methods in CAT, (2017); 
Morgan M., MCQ: An interactive computer program for multiple-choice self-testing, Biochemical Education, 7, 3, pp. 67-69, (1979); 
Muckle T.J., Becker K.A., Wu B., Investigating the multiple answer multiple choice item format, (2011); 
Muntean W., Betts J., Analyzing multiple response data through a signal-detection framework, (2015); 
Muntean W., Betts J., Investigating sequential item effects in a testlet model, (2016); 
Muntean W., Betts J., Developing and pretesting technology enhanced items: Issues and outcomes, (2016); 
Muntean W., Betts J., Kao S.-C., Woo A., A hierarchical framework for response times and signal detection theory, (2018); 
Muntean W., Betts J., Kim D., Exploring the similarities and differences between item response theory and item-level signal detection theory, (2016); 
Muntean W., Betts J., Kim D., Exploring the similarities and differences between item response theory and item-level signal detection theory, (2016); 
Muntean W., Betts J., Kim D., Jorion N., Scaling clinical judgment items using polytomous and super-polytomous models, (2019); 
Muntean W., Betts J., Luo X., Kim D., Woo A., Using signal-detection theory to enhance IRT Methods: A clinical judgment example, (2018); 
Nichols P.D., Kobrin J.L., Lai E., Koepfler J., The role of theories of learning and cognition in assessment design and development, The handbook of cognition and assessment: Frameworks, methodologies, and applications, pp. 15-40, (2016); 
O'Neil K., Folk V., Innovative CBT item formats in a teacher licensing program, (1996); 
Parshall C., Stewart R., Ritter J., Innovations: Sound, graphics, and alternative response modes, (1996); 
Pearson PTE Academic: Score guide, Version 11, (2018); 
Pomplun M., Omar M.H., Multiple-mark items: An alternative objective item format?, Educational and Psychological Measurement, 57, 6, pp. 949-962, (1997); 
Qian H., Woo A., Kim D., Exploring the psychometric properties of innovative items in computerized adaptive testing, Technology enhanced innovative assessment: Development, modeling, and scoring from an interdisciplinary perspective, pp. 97-118, (2017); 
Riconscente M.M., Mislevy R.J., Corrigan S., Evidence-centered design, Handbook of test development, pp. 40-63, (2016); 
Ripkey D.R., Case S.M., Swanson D.B., A “new” item format for assessing aspects of clinical competence, Academic Medicine, 71, 10, pp. 534-536, (1996); 
Samejima F., Estimation of latent ability using a response pattern of graded scores, (1969); 
Samejima F., Efficient methods of estimating the operating characteristics of item response categories and challenge to a new model for the multiple choice item. Final report, (1981); 
Sireci S.G., Zenisky A.L., Innovative item formats in computer-based testing: In pursuit of improved construct representation, Handbook of test development, pp. 329-347, (2006); 
Stocking M.L., Scale drift in on-line calibration, (1988); 
Thissen D., Steinberg L., A response model for multiple choice items, Psychometrika, 49, 4, pp. 501-519, (1984); 
Tsai F., Suen H., A brief report on the comparison of six scoring methods for multiple true-false items, Educational and Psychological Measurement, 53, 2, pp. 399-404, (1993); 
Wan L., Henly G., Measurement properties of two innovative item formats in a computer-based test, Applied Measurement in Education, 25, 1, pp. 58-78, (2012); 
Warrens M.J., de Gruijter D.N.M., Heiser W.J., A systematic comparison between classical optimal scaling and the two-parameter IRT model, Applied Psychological Measurement, 31, 2, pp. 106-120, (2007); 
Wilson M., Constructing measures: An item response modeling approach, (2005); 
Woo A., Muntean W., Betts J., Using signal-detection theory to measure cue recognition in multiple response items, (2014); 
Wright B.D., Masters G.N., Rating scale analysis, (1982)#FRF#
