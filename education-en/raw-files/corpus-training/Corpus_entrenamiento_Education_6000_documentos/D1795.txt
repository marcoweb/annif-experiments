#ITI#Investigating the reliability of aggregate measurements of learning process data: From theory to practice#FTI#
#IRE# Background: Learning analytics (LA) research often aggregates learning process data to extract measurements indicating constructs of interest. However, the warranty that such aggregation will produce reliable measurements has not been explicitly examined. The reliability evidence of aggregate measurements has rarely been reported, leaving an implicit assumption that such measurements are free of errors. Objectives: This study addresses these gaps by investigating the psychometric pros and cons of aggregate measurements. Methods: This study proposes a framework for aggregating process data, which includes the conditions where aggregation is appropriate, and a guideline for selecting the proper reliability evidence and the computing procedure. We support and demonstrate the framework by analysing undergraduates' academic procrastination and programming proficiency in an introductory computer science course. Results and Conclusion: Aggregation over a period is acceptable and may improve measurement reliability only if the construct of interest is stable during the period. Otherwise, aggregation may mask meaningful changes in behaviours and should be avoided. While selecting the type of reliability evidence, a critical question is whether process data can be regarded as repeated measurements. Another question is whether the lengths of processes are unequal and individual events are unreliable. If the answer to the second question is no, segmenting each process into a fixed number of bins assists in computing the reliability coefficient. Major Takeaways: The proposed framework can be a general guideline for aggregating process data in LA research. Researchers should check and report the reliability evidence for aggregate measurements before the ensuing interpretation#FRE#
#IPC# aggregation; internal consistency; learning analytics; learning process data; test–retest reliability#FPC#
#IRF# Standards for educational and psychological testing, (2014); 
Aleven V., McLaughlin E.A., Glenn R.A., Koedinger K.R., Instruction based on adaptive learning technologies, Handbook of research on learning and instruction, pp. 522-560, (2017); 
Becker G., How important is transient error in estimating reliability? Going beyond simulation studies, Psychological Methods, 5, 3, pp. 370-379, (2000); 
Bergner Y., Measurement and its uses in learning analytics, Handbook of learning analytics., (2017); 
Berland M., Martin T., Benton T., Petrick Smith C., Davis D., Using learning analytics to understand the learning pathways of novice programmers, Journal of the Learning Sciences, 22, 4, pp. 564-599, (2013); 
Bosch N., Paquette L., What’ s next? Sequence length and impossible loops in state transition measurement, Journal of Educational Data Mining, 13, 1, pp. 1-23, (2021); 
Bote-Lorenzo M.L., Gomez-Sanchez E., Predicting the decrease of engagement indicators in a MOOC, Proceedings of the seventh international learning analytics & knowledge conference, pp. 143-147, (2017); 
Chen B., Knight S., Wise A.F., Critical issues in designing and implementing temporal analytics, Journal of Learning Analytics, 5, 1, pp. 1-9, (2018); 
Chen B., Resendes M., Chai C.S., Hong H., Two tales of time: Uncovering the significance of sequential patterns among contribution types in knowledge-building discourse, Interactive Learning Environments, 25, 2, pp. 162-175, (2017); 
Cicchinelli A., Veas E., Pardo A., Pammer-Schindler V., Fessl A., Barreiros C., Lindstadt S., Finding traces of self-regulated learning in activity streams, Proceedings of the 8th international conference on learning analytics and knowledge, pp. 191-200, (2018); 
Cortina J.M., What is coefficient alpha? An examination of theory and applications, Journal of Applied Psychology, 78, 1, pp. 98-104, (1993); 
Cronbach L.J., The dependability of behavioral measurements: Theory of generalizability scores and profiles, (1972); 
Cui Y., Chen F., Shiri A., Fan Y., Predictive analytic models of student success in higher education, Information and Learning Science, 120, 3-4, pp. 208-227, (2019); 
Du J., Hew K.F., Liu L., What can online traces tell us about students' self-regulated learning? A systematic review of online trace data analysis, Computers & Education, 201, (2023); 
Enkavi A.Z., Eisenberg I.W., Bissett P.G., Mazza G.L., MacKinnon D.P., Marsch L.A., Poldrack R.A., Large-scale analysis of test–retest reliabilities of self-regulation measures, Proceedings of the National Academy of Sciences, 116, 12, pp. 5472-5477, (2019); 
Fan Y., Tan Y., Rakovic M., Wang Y., Cai Z., Shaffer D.W., Gasevic D., Dissecting learning tactics in MOOC using ordered network analysis, Journal of Computer Assisted Learning, 39, 1, pp. 154-166, (2022); 
Fan Y., van der Graaf J., Lim L., Rakovic M., Singh S., Kilgour J., Moore J., Molenaar I., Bannert M., Gasevic D., Towards investigating the validity of measurement of self-regulated learning based on trace data, Metacognition and Learning, 17, pp. 949-987, (2022); 
Fincham O.E., Gasevic D.V., Jovanovic J.M., Pardo A., From study tactics to learning strategies: An analytical method for extracting interpretable representations, IEEE Transactions on Learning Technologies, 12, 1, pp. 59-72, (2018); 
Flora D.B., Your coefficient alpha is probably wrong, but which coefficient omega is right? A tutorial on using r to obtain better reliability estimates, Advances in Methods and Practices in Psychological Science, 3, 4, pp. 484-501, (2020); 
Gray G., Bergner Y., A practitioner's guide to measurement in learning analytics: Decisions, opportunities, and challenges, Handbook of learning analytics, pp. 20-28, (2022); 
Greene J.A., Plumley R.D., Urban C.J., Bernacki M.L., Gates K.M., Hogan K.A., Demetriou C., Panter A.T., Modeling temporal self-regulatory processing in a higher education biology course, Learning and Instruction, 72, (2021); 
Hadwin A.F., Commentary and future directions: What can multi-modal data reveal about temporal and adaptive processes in self-regulated learning?, Learning and Instruction, 72, (2021); 
Hayes A.F., Coutts J.J., Use omega rather than Cronbach's alpha for estimating reliability. But…, Communication Methods and Measures, 14, 1, pp. 1-24, (2020); 
Hedge C., Powell G., Sumner P., The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences, Behavior Research Methods, 50, 3, pp. 1166-1186, (2018); 
Hehman E., Xie S.Y., Ofosu E.K., Nespoli G., Assessing the point at which averages are stable: A tool illustrated in the context of person perception. PsyArXiv, (2018); 
Huang N., Zhang J., Burtch G., Li X., Chen P., Combating procrastination on massive online open courses via optimal calls to action, Information Systems Research, 32, 2, pp. 301-317, (2021); 
Jiang Y., Bosch N., Baker R.S., Paquette L., Ocumpaugh J., Andres J.M.A.L., Moore A.L., Biswas G., Expert feature-engineering vs. deep neural networks: Which is better for sensor-free affect detection, Proceedings of the 19th international conference on artificial intelligence in education. Lecture notes in computer science, 10947, pp. 198-211, (2018); 
Jovanovic J., Gasevic D., Dawson S., Pardo A., Mirriahi N., Learning analytics to unveil learning strategies in a flipped classroom, The Internet and Higher Education, 33, pp. 74-85, (2017); 
Kinnebrew J.S., Segedy J.R., Biswas G., Analyzing the temporal evolution of students' behaviors in open-ended learning environments, Metacognition and Learning, 9, 2, pp. 187-215, (2014); 
Kitto K., Shum S.B., Gibson A., Embracing imperfection in learning analytics, Proceedings of the 8th international conference on learning analytics and knowledge, pp. 451-460, (2018); 
Knight S., Wise A., Ochoa X., Fostering an impactful field of learning analytics, Journal of Learning Analytics, 6, 3, pp. 1-4, (2019); 
Knight S., Wise A.F., Chen B., Time for change: Why learning analytics needs temporal analysis, Journal of Learning Analytics, 4, 3, pp. 7-17, (2017); 
Koo T.K., Li M.Y., A guideline of selecting and reporting intraclass correlation coefficients for reliability research, Journal of Chiropractic Medicine, 15, 2, pp. 155-163, (2016); 
Leppink J., van Merrienboer J.J.G., The beast of aggregating cognitive load measures in technology-based learning, Journal of Educational Technology & Society, 18, 4, pp. 230-245, (2015); 
Li S., Du H., Xing W., Zheng J., Chen G., Xie C., Examining temporal dynamics of self-regulated learning behaviors in STEM learning: A network approach, Computers & Education, 158, (2020); 
Little T.D., Cunningham W.A., Shahar G., Widaman K.F., To parcel or not to parcel: Exploring the question, weighing the merits, Structural Equation Modeling: A Multidisciplinary Journal, 9, 2, pp. 151-173, (2002); 
Little T.D., Rhemtulla M., Gibson K., Schoemann A.M., Why the items versus parcels controversy needn't be one?, Psychological Methods, 18, 3, pp. 285-300, (2013); 
Martin J., Self-regulated learning, social cognitive theory, and agency, Educational Psychologist, 39, 2, pp. 135-145, (2004); 
Matayoshi J., Karumbaiah S., Using marginal models to adjust for statistical bias in the analysis of state transitions, Proceedings of the 11th international conference on learning analytics and knowledge, pp. 449-455, (2021); 
Matsunaga M., Item parceling in structural equation modeling: A primer, Communication Methods and Measures, 2, 4, pp. 260-293, (2008); 
McDonald R.P., Test theory: A unified treatment, (1999); 
McGraw K.O., Wong S.P., Forming inferences about some intraclass correlation coefficients, Psychological Methods, 1, 1, pp. 30-46, (1996); 
Mislevy R.J., Behrens J.T., Dicerbo K.E., Levy R., Design and discovery in educational assessment: Evidence-centered design, psychometrics, and educational data mining, Journal of Educational Data Mining, 4, 1, pp. 11-48, (2012); 
Molenaar I., Advances in temporal analysis in learning and instruction, Frontline Learning Research, 2, 4, pp. 15-24, (2014); 
Motz B., Quick J., Schroeder N., Zook J., Gunkel M., The validity and utility of activity logs as a measure of student engagement, Proceedings of the ninth international conference on learning analytics & knowledge, pp. 300-309, (2019); 
Munshi A., Biswas G., Baker R., Ocumpaugh J., Hutt S., Paquette L., Analysing adaptive scaffolds that help students develop self-regulated learning behaviours, Journal of Computer Assisted Learning, 39, 2, pp. 351-368, (2023); 
Parsons S., Kruijt A., Fox E., Psychological science needs a standard practice of reporting the reliability of cognitive-behavioral measurements, Advances in Methods and Practices in Psychological Science, 2, 4, pp. 378-395, (2019); 
Reimann P., Time is precious: Variable- and event-centred approaches to process analysis in CSCL research, International Journal of Computer-Supported Collaborative Learning, 4, 3, pp. 239-257, (2009); 
Rushton J.P., Brainerd C.J., Pressley M., Behavioral development and construct validity: The principle of aggregation, Psychological Bulletin, 94, 1, pp. 18-38, (1983); 
Saqr M., Modelling within-person idiographic variance could help explain and individualize learning, British Journal of Educational Technology, 54, pp. 1077-1094, (2023); 
Saqr M., Lopez-Pernas S., The longitudinal trajectories of online engagement over a full program, Computers & Education, 175, (2021); 
Schmidt F.L., Le H., Ilies R., Beyond alpha: An empirical examination of the effects of different sources of measurement error on reliability estimates for measures of individual differences constructs, Psychological Methods, 8, 2, pp. 206-224, (2003); 
Schraw G., Wadkins T., Olafson L., Doing the things we do: A grounded theory of academic procrastination, Journal of Educational Psychology, 99, 1, pp. 12-25, (2007); 
Schwendimann B.A., Rodriguez-Triana M.J., Vozniuk A., Prieto L.P., Boroujeni M.S., Holzer A., Gillet D., Dillenbourg P., Perceiving learning at a glance: A systematic literature review of learning dashboard research, IEEE Transactions on Learning Technologies, 10, 1, pp. 30-41, (2017); 
Skrondal A., Rabe-Hesketh S., Generalized latent variable modeling: Multilevel, longitudinal, and structural equation models, (2004); 
Thorndike R.L., Research problems and techniques (report No. 3). Army air forces aviation psychology program research reports #3, (1947); 
Thorndike R.M., Thorndike-Christ T., Measurement and evaluation in psychology and education, (2010); 
Webb N.M., Shavelson R.J., Haertel E.H., 4 reliability coefficients and generalizability theory, Handbook of statistics, 26, pp. 81-124, (2006); 
Weiss R.E., Modeling longitudinal data, 1, (2005); 
Williams D.R., Mulder J., Rouder J.N., Rast P., Beneath the surface: Unearthing within-person variability and mean relations with Bayesian mixed models, Psychological Methods, 26, 1, pp. 74-89, (2021); 
Winne P.H., Construct and consequential validity for learning analytics based on trace data, Computers in Human Behavior, 112, (2020); 
Wise A.F., Knight S., Ochoa X., What makes learning analytics research matter, Journal of Learning Analytics, 8, 3, pp. 1-9, (2021); 
Xie B., Loksa D., Nelson G.L., Davidson M.J., Dong D., Kwik H., Tan A.H., Hwa L., Li M., Ko A.J., A theory of instruction for introductory programming skills, Computer Science Education, 29, 2-3, pp. 205-253, (2019); 
Yang Y., Green S.B., Coefficient alpha: A reliability coefficient for the 21st century?, Journal of Psychoeducational Assessment, 29, 4, pp. 377-392, (2011); 
Zhang J., Cunningham T., Iyer R., Baker R., Fouh E., Exploring the impact of voluntary practice and procrastination in an introductory programming course, Proceedings of the 53rd ACM technical symposium on computer science education, pp. 356-361, (2022); 
Zhang Y., Paquette L., Pinto J.D., Fan A.X., Utilizing programming traces to explore and model the dimensions of novices' code-writing skill, Computer Applications in Engineering Education, 31, 4, pp. 1041-1058, (2023)#FRF#
