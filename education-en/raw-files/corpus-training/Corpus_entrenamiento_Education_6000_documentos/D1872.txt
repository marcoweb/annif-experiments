#ITI#Associating Facial Expressions and Upper-Body Gestures with Learning Tasks for Enhancing Intelligent Tutoring Systems#FTI#
#IRE# Learning involves a substantial amount of cognitive, social and emotional states. Therefore, recognizing and understanding these states in the context of learning is key in designing informed interventions and addressing the needs of the individual student to provide personalized education. In this paper, we explore the automatic detection of learner’s nonverbal behaviors involving hand-over-face gestures, head and eye movements and emotions via facial expressions during learning. The proposed computer vision-based behavior monitoring method uses a low-cost webcam and can easily be integrated with modern tutoring technologies. We investigate these behaviors in-depth over time in a classroom session of 40 minutes involving reading and problem-solving exercises. The exercises in the sessions are divided into three categories: an easy, medium and difficult topic within the context of undergraduate computer science. We found that there is a significant increase in head and eye movements as time progresses, as well as with the increase of difficulty level. We demonstrated that there is a considerable occurrence of hand-over-face gestures (on average 21.35%) during the 40 minutes session and is unexplored in the education domain. We propose a novel deep learning approach for automatic detection of hand-over-face gestures in images with a classification accuracy of 86.87%. There is a prominent increase in hand-over-face gestures when the difficulty level of the given exercise increases. The hand-over-face gestures occur more frequently during problem-solving (easy 23.79%, medium 19.84% and difficult 30.46%) exercises in comparison to reading (easy 16.20%, medium 20.06% and difficult 20.18%)#FRE#
#IPC# Adaptive and intelligent multimedia and hypermedia systems; Computer-supported collaborative learning; Intelligent Tutoring Systems (ITSs); Neural models applied to AIED systems#FPC#
#IRF# Ambady N., Rosenthal R., Thin slices of expressive behavior as predictors of interpersonal consequences: a meta-analysis, Psychological Bulletin, 111, 2, (1992); 
Andallaza T.C.S., Rodrigo M.M.T., Lagud M.C.V., Jimenez R.J.M., Sugay J.O., Modeling the affective states of students using an intelligent tutoring system for algebra, Proc. International Workshop on Empathic Computing (IWEC), (2012); 
Anderson A.R., Christenson S.L., Sinclair M.F., Lehr C.A., Check & Connect: The importance of relationships for promoting engagement with school, Journal of School Psychology, 42, 2, pp. 95-113, (2004); 
Arroyo I., Ferguson K., Johns J., Dragon T., Meheranian H., Fisher D., Barto A., Mahadevan S., Woolf B.P., Repairing Disengagement with Non-Invasive Interventions, 2007, pp. 195-202, (2007); 
Arroyo I., Cooper D.G., Burleson W., Woolf B.P., Muldner K., Christopherson R., Emotion sensors go to school, AIED, 200, pp. 17-24, (2009); 
Bahreini K., Nadolski R., Westera W., Towards multimodal emotion recognition in e-learning environments, Interactive Learning Environments, 24, 3, pp. 590-605, (2016); 
Ball K.M., Fibonacci’s Rabbits Revisited, Strange Curves, Counting Rabbits, and Other Mathematical Explorations, (2003); 
Beattie G., Rethinking Body Language: How Hand Movements Reveal Hidden Thoughts, (2016); 
Benitti F.B.V., Exploring the educational potential of robotics in schools, Computers & Education, 58, 3, pp. 978-988, (2012); 
Bligh D., What’s the Use of Lectures, (2002); 
Boker S.M., Cohn J.F., Theobald B.J., Matthews I., Mangini M., Spies J.R., Brick T.R., Something in the way we move: motion, not perceived sex, influences nods in conversation, Journal of Experimental Psychology: Human Perception and Performance, 37, 3, pp. 874-891, (2011); 
Bosch N., D'Mello S., Co-occurring affective states in automated computer programming education, 12Th International Conference on Intelligent Tutoring Systems Proc Workshop on Ai-Supported Education for Computer Science (AIEDCS), pp. 21-30, (2014); 
Bosch N., D'Mello S., Baker R., Ocumpaugh J., Shute V., Ventura M., Wang L., Zhao W., Automatic detection of learning-centered affective states in the wild, Proc. International Conference on Intelligent User Interfaces (IUI ’15), pp. 379-388, (2015); 
Bransford J.D., Brown A.L., Cocking R.R., How People Learn: Brain, Mind, Experience and School. Committee on Developments in the Science of Learning, NRC Commission on Behavioral and Social Sciences and Education, (2000); 
Burleson W., Picard R.W., Affective agents: Sustaining motivation to learn through failure and a state of stuck, Workshop on Social and Emotional Intelligence in Learning Environments, 7Th International Conference on Intelligent Tutoring Systems, (2004); 
Busso C., Deng Z., Grimm M., Neumann U., Narayanan S., Rigid head motion in expressive speech animation: Analysis and synthesis, IEEE Transactions on Audio, Speech, and Language Processing, 15, 3, pp. 1075-1086, (2007); 
Calvo R.A., D'Mello S., Affect detection: an interdisciplinary review of models, methods, and their applications, IEEE Transactions on Affective Computing, 1, 1, pp. 18-37, (2010); 
Cao Z., Simon T., Wei S.E., Sheikh Y., Realtime multi-person 2d pose estimation using part affinity fields, IEEE Computer Vision and Pattern Recognition (CVPR), (2017); 
Cavanagh M., Students’ experiences of active engagement through cooperative learning activities in lectures, Active Learning in Higher Education, 12, 1, pp. 23-33, (2011); 
Chellappa R., Wilson C., Sirohey S., Human and machine recognition of faces: a survey, Proceedings of the IEEE, 83, 5, pp. 705-740, (1995); 
Cohn J.F., Reed L.I., Moriyama T., Xiao J., Schmidt K.L., Ambadar Z., Multimodal coordination of facial action, head rotation, and eye motion, Proc. 6Th IEEE Int. Conf. Automatic Face Gesture Recognition, pp. 129-138, (2004); 
Conati C., Jaques N., Muir M., Understanding attention to adaptive hints in educational games: an eye-tracking study, International Journal of Artificial Intelligence in Education, 23, 1, pp. 136-161, (2013); 
Cook S., Goldin-Meadow S., The role of gesture in learning: Do children use their hands to change their minds?, Journal of Cognition and Development, 7, 2, pp. 211-232, (2006); 
Dalal N., Triggs B., Histograms of oriented gradients for human detection, IEEE Computer Vision and Pattern Recognition, pp. 886-893, (2005); 
Darwin C., The Expression of the Emotions in Man and Animals, (1872); 
Davis D., A Brain-Friendly Environment for Learning, (2008); 
Ding C., Tao D., A comprehensive survey on pose-invariant face recognition, ACM Transactions on Intelligent Systems and Technology (TIST), 7, 3, (2016); 
Dirkx J., Engaging emotions in adult learning: A Jungian perspective on emotion and transformative learning, New Directions for Adult & Continuing Education, 2006, 109, pp. 15-26, (2006); 
D'Mello S., A selective meta-analysis on the relative incidence of discrete affetcive states during learning with technology, Journal of Educational Psychology, 105, 4, (2013); 
D'Mello S., Craig S.D., Gholson B., Franklin S., Picard R., Graesser A., Integrating affect sensors in an intelligent tutoring system, Affective Interactions: The Computer in The Affective Loop Workshop, pp. 7-13, (2005); 
D'Mello S., Picard R.W., Graesser A., Toward an affect-sensitive autotutor, IEEE Intelligent Systems, 22, 4, pp. 53-61, (2007); 
D'Mello S., Lehman B., Sullins J., Daigle R., Combs R., Vogt K., Perkins L., Graesser A., A time for emoting: When affect-sensitivity is and isn’t effective at promoting deep learning, International Conference on Intelligent Tutoring Systems, pp. 245-254, (2010); 
D'Mello S., Olney A., Williams C., Hays P., Gaze tutor: a gaze-reactive intelligent tutoring system, International Journal of Human-Computer Studies, 70, 5, pp. 377-398, (2012); 
D'Mello S., Dieterle E., Duckworth A., Advanced, analytic, automated (aaa) measurement of engagement during learning, Educational Psychologist, 52, 2, pp. 104-123, (2017); 
Ekman P., An argument for basic emotions, Cognition and Emotion, 6, 3, pp. 169-200, (1992); 
Ekman P., Friesen W., The repertoire of nonverbal behavior: categories, origins, usage, and coding, Semiotica, 1, 1, pp. 49-98, (1969); 
Fan R.E., Chang K.W., Hsieh C.J., Wang X.R., Lin C.J., Liblinear: a library for large linear classification, Journal of Machine Learning Research, 9, pp. 1871-1874, (2008); 
Fasel B., Luettin J., Automatic facial expression analysis: a survey, Pattern Recognition, 36, 1, pp. 259-275, (2003); 
Forbes-Riley K., Litman D., Adapting to multiple affective states in spoken dialogue, Proceedings of the 13Th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Association for Computational Linguistics, pp. 217-226, (2012); 
Fredricks J.A., Blumenfeld P.C., Paris A.H., School engagement: Potential of the concept, state of the evidence, Review of Educational Research, 74, 1, pp. 59-109, (2004); 
Gelder B.D., Towards the neurobiology of emotional body language, Nature Reviews Neuroscience, 7, 3, pp. 242-249, (2006); 
Gelder B.D., Why bodies? Twelve reasons for including bodily expressions in affective neuroscience, Philosophical Transactions of the Royal Society of London Series B, Biological Sciences, 364, 1535, pp. 3475-3484, (2009); 
Givens D.B., The Nonverbal Dictionary of Gestures Signs & Body Language Cues from Adam’s-apple-jump to Zygomatic Smile, (2002); 
Godwin K.E., Almeda M.V., Petroccia M., Baker R.S., Fisher A.V., Classroom activities and off-task behavior in elementary school children, Proc. 35Th Annual Meeting of the Cognitive Science Society, pp. 2428-2433, (2013); 
Gordon G., Spaulding S., Westlund J.K., Lee J.J., Plummer L., Martinez M., Das M., Breazeal C., Affective personalization of a social robot tutor for children’s second language skills, Proc. of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 3951-3957, (2016); 
Graesser A., D'Mello S., Chipman P., King B., McDaniel B., Exploring relationships between affect and learning with AutoTutor, Proc. Artificial Intelligence in Education, pp. 16-23, (2007); 
Hirschberg D.S., Algorithm for the longest common subsequence problem, Journal of ACM, 24, 4, pp. 664-675, (1977); 
Johns J., Woolf B., A dynamic mixture model to detect student motivation and proficiency, Proceedings of the national conference on artificial intelligence, 21, (2006); 
Jones A., Kuster D., Basedow C.A., Alves-Oliveira P., Serholt S., Hastie H., Corrigan L.J., Barendregt W., Kappas A., Paiva A., Castellano G., Empathic Robotic Tutors for Personalised Learning: A Multidisciplinary Approach, (2015); 
Kaliouby R., Robinson P., Real-time inference of complex mental states from facial expressions and head gestures, Proc. Real-Time Vision for HCI, pp. 181-200, (2005); 
Kapoor A., Burleson W., Picard R.W., Automatic prediction of frustration, International Journal of Human-computer Studies, 65, 8, pp. 724-736, (2007); 
Karg M., Samadani A.A., Gorbet R., Kuhnlenz K., Hoey J., Kulic D., Body movements for affective expression: a survey of automatic recognition and generation, IEEE Transactions on Affective Computing, 4, 4, pp. 341-359, (2013); 
Keltner D., Signs of appeasement: Evidence for the distinct displays of embarrassment, amusement and shame, Journal of Personality and Social Psychology, 68, 3, pp. 441-454, (1995); 
Kessell A.M., Tversky T., Gestures for thinking and explaining, Proc. Annual Meeting of the Cognitive Science Society, 27, (2005); 
Laptev I., On space-time interest points, International Journal of Computer Vision, 64, pp. 107-123, (2005); 
Lepper M.R., Hodell M., Intrinsic Motivation in the Classroom, (1989); 
Lepper M.R., Woolverton M., Mumme D.L., Gurtner J.L., Motivational Techniques of Expert Human Tutors: Lessons for the Design of Computer-based Tutors, (1993); 
Litman D.J., Forbes-Riley K., Predicting student emotions in computer-human tutoring dialogues, Proc. 42Nd Annual Meeting on Association for Computational Linguistics, (2004); 
Livingstone R., The Future in Education, (1941); 
Lowe D.G., Distinctive image features from scale-invariant keypoints, International Journal of Computer Vision, 60, 2, pp. 91-110, (2004); 
Mahmoud M., Robinson P., Interpreting hand-over-face gestures, Proc. International Conference on Affective Computing and Intelligent Interaction, pp. 248-255, (2011); 
Mahmoud M., Baltrusaitis T., Robinson P., Automatic analysis of naturalistic hand-over-face gestures, ACM Transactions on Interactive Intelligent Systems, 6, 2, pp. 19:1-19:18, (2016); 
Mathews M., Mitrovic A., Lin B., Holland J., Churcher N., Do Your Eyes Give It Away? Using Eye Tracking Data to Understand Students’ Attitudes towards Open Student Model Representations, pp. 422-427, (2012); 
McNeill D., Hand and Mind: What Gestures Reveal about Thought, (1992); 
Mehta D., Siddiqui M., Javaid A., Facial emotion recognition: a survey and real-world user experiences in mixed reality, Sensors, 18, 2, (2018); 
Meservy T.O., Jensen M.L., Kruse J., Burgoon J.K., Nunamaker J.F., Twitchell D.P., Tsechpenakis G., Metaxas D.N., Deception detection through automatic, unobtrusive analysis of nonverbal behavior, IEEE Intelligent Systems, 20, 5, pp. 36-43, (2005); 
Meyer D.K., Turner J.C., Discovering emotion in classroom motivation research, Educational Psychologist, 37, 2, pp. 107-114, (2002); 
Mota S., Picard R.W., Automated posture analysis for detecting learner’s interest level, 2003 Conference on Computer Vision and Pattern Recognition Workshop, 5, pp. 49-49, (2003); 
Mudrick N.V., Taub M., Azevedo R., Rowe J., Lester J., Toward affect-sensitive virtual human tutors: The influence of facial expressions on learning and emotion, 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII), pp. 184-189, (2017); 
Norman D.A., Twelve Issues for Cognitive Science, pp. 265-295, (1981); 
Pease A., Pease B., The Definitive Book of Body Language: How to Read others’ Attitudes by Their Gestures, (2006); 
Perry B.D., Fear and learning: Trauma-related factors in the adult education process, New Directions for Adult & Continuing Education, 2006, 110, pp. 21-27, (2006); 
Peters C., Asteriadis S., Karpouzis K., Investigating shared attention with a virtual agent using a gaze-based interface, Journal on Multimodal User Interfaces, 3, 1-2, pp. 119-130, (2010); 
Picard R.W., Papert S., Bender W., Blumberg B., Breazeal C., Cavallo D., Machover T., Resnick M., Roy D., Strohecker C., Affective learning – a manifesto, BT Technology Journal, 2, 4, pp. 253-269, (2004); 
Rautaray S.S., Agrawal A., Vision-based hand gesture recognition for human computer interaction: a survey, Artificial Intelligence Review, 43, 1, pp. 1-54, (2015); 
Razavian A.S., Azizpour H., Sullivan J., Carlsson S., Cnn features off-the-shelf: An astounding baseline for recognition, IEEE CVPRW, pp. 512-519, (2014); 
Reeve J., Understanding Motivation and Emotion, (2001); 
Roth W., Gestures: Their role in teaching and learning, Review of Educational Research, 71, 3, pp. 365-392, (2001); 
Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A.C., Fei-Fei L., Imagenet large scale visual recognition challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); 
Shan C., Gong S., McOwan P.W., Facial expression recognition based on local binary patterns: a comprehensive study, Image and Vision Computing, 27, 6, pp. 803-816, (2009); 
Shuck B., Albornoz C., Winberg M., Emotions and their effect on adult learning: A constructivist perspective, Proc. Sixth Annual College of Education Research Conference: Urban and International Education Section, pp. 108-113, (2007); 
Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2014); 
Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, IEEE Computer Vision and Pattern Recognition (CVPR), pp. 2818-2826, (2016); 
Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., Inception-v4, inception-resnet and the impact of residual connections on learning, AAAI, pp. 4278-4284, (2017); 
Tieleman T., Hinton G., Coursera: Neural Networks for Machine Learning, (2012); 
de La Torre F., Cohn J.F., Visual Analysis of humans: Facial expression analysis, Proc. Visual Analysis of Humans: Looking at People, pp. 377-410, (2011); 
de La Torre F., Cohn J.F., Visual Analysis of humans: Facial expression analysis, Proc. Visual Analysis of Humans: Looking at People, pp. 377-410, (2011); 
Wallbott H.G., Bodily expression of emotion, European Journal of Social Psychology, 28, 6, pp. 879-896, (1998); 
Watson W.R., Watson S.L., Reigeluth C.M., Education 3.0: Breaking the mold with technology, Interactive Learning Environments, 23, 3, pp. 332-343, (2015); 
Whitehill J., Serpell Z., Lin Y.C., Foster A., Movellan J.R., The faces of engagement: Automatic recognition of student engagement from facial expressions, IEEE Transactions on Affective Computing, 5, 1, pp. 86-98, (2014); 
Wolfe P., The role of meaning and emotion in learning, New Directions for Adult & Continuing Education, 2006, 110, pp. 35-41, (2006); 
Woolf B., Building Intelligent Interactive Tutors: Student-Centered Strategies for Revolutionizing E-learning, (2009); 
Woolf B., Burleson W., Arroyo I., Dragon T., Cooper D., Picard R., Affect-aware tutors: Recognising and responding to student affect, International Journal of Learning Technology, 4, 3-4, pp. 129-164, (2009); 
Xiao B., Georgiou P.G., Baucom B., Narayanan S., Data driven modeling of head motion toward analysis of behaviors in couple interactions, Proc. IEEE Int. Conf. Acoustic, Speech, Signal Processing, pp. 3766-3770, (2013); 
Yosinski J., Clune J., Bengio Y., Lipson H., How transferable are features in deep neural networks?, Advances in Neural Information Processing Systems (NIPS), pp. 3320-3328, (2014); 
Zeng Z., Pantic M., Roisman G.I., Huang T.S., A survey of affect recognition methods: audio, visual, and spontaneous expressions, IEEE Transactions on Pattern Analysis and Machine Intelligence, 31, 1, pp. 39-58, (2009); 
Zhan Z., Zhang L., Mei H., Fong P.S.W., Online learners’ reading ability detection based on eye-tracking sensors, Sensors, 16, 9, (2016); 
Zhao W., Chellappa R., Phillips P.J., Rosenfeld A., Face recognition: a literature survey, ACM Computing Surveys (CSUR), 35, 4, pp. 399-458, (2003)#FRF#
