#ITI#Applying machine learning to automatically assess scientific models#FTI#
#IRE# Involving students in scientific modeling practice is one of the most effective approaches to achieving the next generation science education learning goals. Given the complexity and multirepresentational features of scientific models, scoring student-developed models is time- and cost-intensive, remaining one of the most challenging assessment practices for science education. More importantly, teachers who rely on timely feedback to plan and adjust instruction are reluctant to use modeling tasks because they could not provide timely feedback to learners. This study utilized machine learning (ML), the most advanced artificial intelligence (AI), to develop an approach to automatically score student-drawn models and their written descriptions of those models. We developed six modeling assessment tasks for middle school students that integrate disciplinary core ideas and crosscutting concepts with the modeling practice. For each task, we asked students to draw a model and write a description of that model, which gave students with diverse backgrounds an opportunity to represent their understanding in multiple ways. We then collected student responses to the six tasks and had human experts score a subset of those responses. We used the human-scored student responses to develop ML algorithmic models (AMs) and to train the computer. Validation using new data suggests that the machine-assigned scores achieved robust agreements with human consent scores. Qualitative analysis of student-drawn models further revealed five characteristics that might impact machine scoring accuracy: Alternative expression, confusing label, inconsistent size, inconsistent position, and redundant information. We argue that these five characteristics should be considered when developing machine-scorable modeling tasks#FRE#
#IPC# artificial intelligence; artificial neural networks; deep learning; inclusive assessment; machine learning; natural language processing; scientific model#FPC#
#IRF# (2019); 
Ainsworth S., DeFT: A conceptual framework for considering learning with multiple representations, Learning and Instruction, 16, 3, pp. 183-198, (2006); 
Akaygun S., Jones L.L., Words or pictures: A comparison of written and pictorial explanations of physical and chemical equilibria, International Journal of Science Education, 36, 5, pp. 783-807, (2014); 
Arora S., Du S.S., Hu W., Li Z., Salakhutdinov R., Wang R., On exact computation with an infinitely wide neural net, Advances in Neural Information Processing Systems, 32, (2019); 
Baumfalk B., Bhattacharya D., Vo T., Forbes C., Zangori L., Schwarz C., Impact of model-based science curriculum and instruction on elementary students' explanations for the hydrosphere, Journal of Research in Science Teaching, 56, 5, pp. 570-597, (2019); 
Bertolini R., Finch S.J., Nehm R.H., Testing the impact of novel assessment sources and machine learning methods on predictive outcome modeling in undergraduate biology, Journal of Science Education and Technology, 30, 2, pp. 193-209, (2021); 
ChanLin L., Formats and prior knowledge on learning in a computer-based lesson, Journal of Computer Assisted Learning, 17, 4, pp. 409-419, (2001); 
Chen Y.-C., Epistemic uncertainty and the support of productive struggle during scientific modeling for knowledge co-development, Journal of Research in Science Teaching, 59, pp. 383-422, (2021); 
Ciresan D.C., Meier U., Masci J., Gambardella L.M., Schmidhuber J., Flexible, high performance convolutional neural networks for image classification. In Twenty-second international joint conference on artificial intelligence, (2011); 
Deng L., The MNIST database of handwritten digit images for machine learning research [best of the web], IEEE Signal Processing Magazine, 29, 6, pp. 141-142, (2012); 
Fleiss J.L., Levin B., Paik M.C., Statistical methods for rates and proportions, (2013); 
Furtak E.M., Confronting dilemmas posed by three-dimensional classroom assessment: Introduction to a virtual issue of science education, Science Education, 101, 5, pp. 854-867, (2017); 
Gerard L., Kidron A., Linn M., Guiding collaborative revision of science explanations, International Journal of Computer-Supported Collaborative Learning, 14, 3, pp. 291-324, (2019); 
Gilbert J.K., Treagust D.F., Towards a coherent model for macro, submicro and symbolic representations in chemical education, Multiple representations in chemical education, pp. 333-350, (2009); 
Gobert J.D., Leveraging technology and cognitive theory on visualization to promote students' science, Visualization in science education, pp. 73-90, (2005); 
Gonzalez-Howard M., McNeill K.L., Marco-Bujosa L.M., Proctor C.P., ‘Does it answer the question or is it French fries?’: An exploration of language supports for scientific argumentation, International Journal of Science Education, 39, 5, pp. 528-547, (2017); 
Ha M., Nehm R., The impact of misspelled words on automated computer scoring: a case study of scientific explanations, Journal of Science Education and Technology, 25, 3, pp. 358-374, (2016); 
Harris C.J., Krajcik J.S., Pellegrino J.W., DeBarger A.H., Designing knowledge-in-use assessments to promote deeper learning, Educational Measurement: Issues and Practice, 38, 2, pp. 53-67, (2019); 
Harrison A.G., Treagust D.F., A typology of school science models, International Journal of Science Education, 22, 9, pp. 1011-1026, (2000); 
Haudek K.C., Prevost L.B., Moscarella R.A., Merrill J., Urban-Lurain M., What are they thinking? Automated analysis of student writing about acid–base chemistry in introductory biology, CBE—Life Sciences Education, 11, 3, pp. 283-293, (2012); 
Haudek K., Zhai X., Exploring the effect of construct complexity on machine learning assessments of argumentation. Paper presented at the 2021 annual conference of the National Association of Research in Science Teaching, Orlando, Florida, pp. 1-22, (2021); 
He K., Zhang X., Ren S., Sun J., pp. 630-645, (2016); 
Heijnes D., van Joolingen W., Leenaars F., Stimulating scientific reasoning with drawing-based modeling, Journal of Science Education and Technology, 27, 1, pp. 45-56, (2018); 
Hestenes D., Modeling games in the Newtonian world, American Journal of Physics, 60, 8, pp. 732-748, (1992); 
Jescovitch L.N., Scott E.E., Cerchiara J.A., Merrill J., Urban-Lurain M., Doherty J.H., Haudek K.C., Comparison of machine learning performance using analytic and holistic coding approaches across constructed response assessments aligned to a science learning progression, Journal of Science Education and Technology, 30, 2, pp. 150-167, (2021); 
Jong J.-P., Chiu M.-H., Chung S.-L., The use of modeling-based text to improve students' modeling competencies, Science Education, 99, 5, pp. 986-1018, (2015); 
Ke L., Sadler T.D., Zangori L., Friedrichsen P.J., Developing and using multiple models to promote scientific literacy in the context of socio-scientific issues, Science & Education, 30, 3, pp. 589-607, (2021); 
Ke L., Schwarz C.V., Supporting students' meaningful engagement in scientific modeling through epistemological messages: A case study of contrasting teaching approaches, Journal of Research in Science Teaching, 58, 3, pp. 335-365, (2021); 
Krajcik J., Merritt J., Engaging students in scientific practices: What does constructing and revising models look like in the science classroom?, The Science Teacher, 79, 3, (2012); 
LaDue N.D., Libarkin J.C., Thomas S.R., Visual representations on high school biology, chemistry, earth science, and physics assessments, Journal of Science Education and Technology, 24, 6, pp. 818-834, (2015); 
Lee H.-S., Gweon G.-H., Lord T., Paessel N., Pallant A., Pryputniewicz S., Machine learning-enabled automated feedback: Supporting students' revision of scientific arguments based on data drawn from simulation, Journal of Science Education and Technology, 30, 2, pp. 168-192, (2021); 
Lee H.-S., McNamara D., Bracey Z.B., Liu O.L., Gerard L., Sherin B., Wilson C., Pallant A., Linn M., Haudek K.C., Computerized text analysis: Assessment and research potentials for promoting learning, (2019); 
Lehrer R., Schauble L., Cultivating model-based reasoning in science education, (2006); 
Lehrer R., Schauble L., Scientific thinking and science literacy, Handbook of child psychology, pp. 153-196, (2006); 
Lehrer R., Schauble L., Seeding evolutionary thinking by engaging children in modeling its foundations, Science Education, 96, 4, pp. 701-724, (2012); 
Lehrer R., Schauble L., The developing scientific thinking, Handbook of child psychology and developmental science, pp. 671-714, (2015); 
Lemke J., Talking science: Language, learning, and values, (1990); 
Liu O.L., Rios J.A., Heilman M., Gerard L., Linn M.C., Validation of automated scoring of science assessments, Journal of Research in Science Teaching, 53, 2, pp. 215-233, (2016); 
Maestrales S., Zhai X., Touitou I., Baker Q., Krajcik J., Schneider B., Using machine learning to score multi-dimensional assessments of chemistry and physics, Journal of Science Education and Technology, 30, 2, pp. 239-254, (2021); 
Marshall J.A., Carrejo D.J., Students' mathematical modeling of motion, Journal of Research in Science Teaching, 45, 2, pp. 153-173, (2008); 
Matuk C., Zhang J., Uk I., Linn M.C., Qualitative graphing in an authentic inquiry context: How construction and critique help middle school students to reason about cancer, Journal of Research in Science Teaching, 56, 7, pp. 905-936, (2019); 
Mislevy R., Haertel G., Implications of evidence-centered design for educational testing, Educational Measurement: Issues and Practice, 25, 4, pp. 6-20, (2006); 
Mitchell T.M., Does machine learning really work?, AI Magazine, 18, 3, pp. 11-20, (1997); 
Namdar B., Shen J., Modeling-oriented assessment in K-12 science education: A synthesis of research from 1980 to 2013 and new directions, International Journal of Science Education, 37, 7, pp. 993-1023, (2015); 
Science and engineering for grades 6–12: Investigation and design at the center, (2019); 
A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
Nehm R.H., Ha M., Mayfield E., Transforming biology assessment with machine learning: Automated scoring of written evolutionary explanations, Journal of Science Education and Technology, 21, 1, pp. 183-196, (2012); 
Neumann K., Waight N., The digitalization of science education: Déjà vu all over again?, Journal of Research in Science Teaching, 57, 9, pp. 1519-1528, (2020); 
Next generation science assessment, (2021); 
Next generation science standards: For states, by states, (2013); 
Pei B., Xing W., Lee H.S., Using automatic image processing to analyze visual artifacts created by students in scientific argumentation, British Journal of Educational Technology, 50, 6, pp. 3391-3404, (2019); 
Pellegrino J.W., Wilson M.R., Koenig J.A., Beatty A.S., Developing assessments for the next generation science standards, (2014); 
Recht B., Roelofs R., Schmidt L., Shankar V., Do cifar-10 classifiers generalize to cifar-10?, Preprint, (2018); 
Riordan B., Bichler S., Bradford A., Chen J.K., Wiley K., Gerard L., Linn M.C., An empirical investigation of neural methods for content scoring of science explanations. In Proceedings of the fifteenth workshop on innovative use of NLP for building educational applications, (2020); 
Rosenberg J.M., Krist C., Combining machine learning and qualitative methods to elaborate students' ideas about the generality of their model-based explanations, Journal of Science Education and Technology, 30, 2, pp. 255-267, (2021); 
Ryoo K., Bedell K., Swearingen A., Promoting linguistically diverse students' short-term and long-term understanding of chemical phenomena using visualizations, Journal of Science Education and Technology, 27, 6, pp. 508-522, (2018); 
Schneider B., Krajcik J., Lavonen J., Salmela-Aro K., Klager C., Bradford L., Chen I.-C., Baker Q., Touitou I., Peek-Brown D., Dezendorf R., Maestrales S., Bartz K., Improving science achievement—Is it possible? Evaluating the efficacy of a high school chemistry and physics project-based learning intervention, Educational Researcher, 51, 2, pp. 109-121, (2022); 
Schwarz C.V., Passmore C., Reiser B.J., Helping students make sense of the world using next generation science and engineering practices, (2017); 
Schwarz C.V., Reiser B.J., Davis E.A., Kenyon L., Acher A., Fortus D., Shwartz Y., Hug B., Krajcik J., Developing a learning progression for scientific modeling: Making scientific modeling accessible and meaningful for learners, Journal of Research in Science Teaching, 46, 6, pp. 632-654, (2009); 
Shemwell J.T., Capps D.K., Learning abstraction as a modeling competence, Towards a competence-based view on models and modeling in science education, pp. 291-307, (2019); 
Singha K., Loheide S.P., Linking physical and numerical modelling in hydrogeology using sand tank experiments and COMSOL multiphysics, International Journal of Science Education, 33, 4, pp. 547-571, (2011); 
Spiro R.J., Cognitive flexibility theory: Advanced knowledge acquisition in ill-structured domains, (1988); 
Stenning K., Oberlander J., A cognitive theory of graphical and linguistic reasoning: Logic and implementation, Cognitive Science, 19, 1, pp. 97-140, (1995); 
Stratford S.J., Krajcik J., Soloway E., Secondary students' dynamic modeling processes: Analyzing, reasoning about, synthesizing, and testing models of stream ecosystems, Journal of Science Education and Technology, 7, 3, pp. 215-234, (1998); 
Stroupe D., Examining classroom science practice communities: How teachers and students negotiate epistemic agency and learn science-as-practice, Science Education, 98, 3, pp. 487-516, (2014); 
Sung S.H., Li C., Chen G., Huang X., Xie C., Massicotte J., Shen J., How does augmented observation facilitate multimodal representational thinking? Applying deep learning to decode complex student construct, Journal of Science Education and Technology, 30, 2, pp. 210-226, (2021); 
Tippett C.D., Refutation text in science education: A review of two decades of research, International Journal of Science and Mathematics Education, 8, 6, pp. 951-970, (2010); 
Tversky B., Spatial schemas in depictions, Spatial schemas and abstract thought, 79, (2001); 
Tytler R., The role of visualisation in science: a response to “Science teachers' use of visual representations”, Studies in Science Education, 57, 1, pp. 129-139, (2021); 
Tytler R., Prain V., Aranda G., Ferguson J., Gorur R., Drawing to reason and learn in science, Journal of Research in Science Teaching, 57, 2, pp. 209-231, (2020); 
Vitale J.M., Lai K., Linn M.C., Taking advantage of automated assessment of student-constructed graphs in science, Journal of Research in Science Teaching, 52, 10, pp. 1426-1450, (2015); 
Wertheim J., Osborne J., Quinn H., Pecheone R., Schultz S., Holthuis N., Martin P., An analysis of existing science assessments and the implications for developing assessment tasks for the NGSS, (2016); 
Wilkerson-Jerde M.H., Gravel B.E., Macrander C.A., Exploring shifts in middle school learners' modeling activity while generating drawings, animations, and computational simulations of molecular diffusion, Journal of Science Education and Technology, 24, 2-3, pp. 396-415, (2015); 
Wilson C., Haudek K., Jonathan O., Stuhlsatz M., Cheuk T., Donovan B., Bracey Z., Mercado Santiago M., Zhai X., Using automated analysis to assess middle school students’ competence with scientific argumentation; 
Zhai X., Practices and theories: How can machine learning assist in innovative assessment practices in science education, Journal of Science Education and Technology, 30, 2, pp. 1-11, (2021); 
Zhai X., Assessing high-school students' modeling performance on Newtonian mechanics, Journal of Research in Science Teaching, pp. 1-41, (2022); 
Zhai X., Haudek K.C., Shi L., Nehm R., Urban-Lurain M., From substitution to redefinition: A framework of machine learning-based science assessment, Journal of Research in Science Teaching, 57, 9, pp. 1430-1459, (2020); 
Zhai X., Haudek K.C., Stuhlsatz M.A., Wilson C., Evaluation of construct-irrelevant variance yielded by machine and human scoring of a science teacher PCK constructed response assessment, Studies in Educational Evaluation, 67, (2020); 
Zhai X., Krajcik J., Pellegrino J., On the validity of machine learning-based next generation science assessments: A validity inferential network, Journal of Science Education and Technology, 30, 2, pp. 298-312, (2021); 
Zhai X., Shi L., Nehm R., A meta-analysis of machine learning-based science assessments: Factors impacting machine-human score agreements, Journal of Science Education and Technology, 30, 3, pp. 361-379, (2021); 
Zhai X., Yin Y., Pellegrino J.W., Haudek K.C., Shi L., Applying machine learning in science assessment: A systematic review, Studies in Science Education, 56, 1, pp. 111-151, (2020); 
zu Belzen A.U., Kruger D., van Driel J., Towards a competence-based view on models and modeling in science education, (2019)#FRF#
