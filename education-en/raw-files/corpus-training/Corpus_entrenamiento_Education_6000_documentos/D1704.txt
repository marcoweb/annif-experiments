#ITI#The role of rapid guessing and test-taking persistence in modelling test-taking engagement#FTI#
#IRE# Background: Item response times in computerized assessments are frequently used to identify rapid guessing behaviour as a manifestation of response disengagement. However, non-rapid responses (i.e., with longer response times) are not necessarily engaged, which means that response-time-based procedures could overlook disengaged responses. Therefore, the identification of disengaged responses could be improved by considering additional indicators of disengagement. We investigated the extent to which decreases in individuals' item solution probabilities over the course of a test reflect disengaged response behaviour. Objectives: To disentangle different types of possibly disengaged responses and better understand non-effortful test-taking behaviour, we augmented responses-time-based procedures for identifying rapid guessing with strategies for detecting disengaged responses on the basis of performance declines in non-rapid responses. Methods: We combined item response theory (IRT) models for rapid guessing and test-taking persistence to examine the capability of response times and item positions to capture response disengagement. We used a computerized assessment in which science items were randomly distributed across positions for each student. This allowed us to estimate individual differences in test-taking persistence (i.e., the duration for which the initial level of performance is maintained) while accounting for rapid responses. Results and Conclusions: Response times did not fully explain disengagement; item responses reflected test-taking persistence even when rapid responses were accounted for. This interpretation was supported by a strong correlation of test-taking persistence with decreases in self-reported test-taking effort. Furthermore, our results suggest that IRT models for test-taking persistence can effectively account for the undesirable impact of low test-taking effort even when response times are unavailable#FRE#
#IPC# item response theory; process data; rapid guessing behaviour; response time; test-taking engagement; test-taking persistence#FPC#
#IRF# Akaike H., A new look at the statistical model identification, IEEE Transactions on Automatic Control, 19, pp. 716-723, (1974); 
Attali Y., Bar-Hillel M., Guess where: The position of correct answers in multiple-choice test items as a psychometric variable, Journal of Educational Measurement, 40, pp. 109-128, (2003); 
Borghans L., Schils T., The leaning tower of Pisa: decomposing achievement test scores into cognitive and noncognitive components, (2012); 
Borgonovi F., Biecek P., An international comparison of students' ability to endure fatigue and maintain motivation during a low-stakes test, Learning and Individual Differences, 49, pp. 128-137, (2016); 
Borgonovi F., Ferrara A., Piacentini M., Performance decline in a low-stakes test at age 15 and educational attainment at age 25: Cross-country longitudinal evidence, Journal of Adolescence, 92, pp. 114-125, (2021); 
De Gruijter D.N., Small N does not always justify Rasch model, Applied Psychological Measurement, 10, pp. 187-194, (1986); 
Debeer D., Buchholz J., Hartig J., Janssen R., Student, school, and country differences in sustained test-taking effort in the 2009 PISA reading assessment, Journal of Educational and Behavioral Statistics, 39, pp. 502-523, (2014); 
Debeer D., Janssen R., Modeling item-position effects within an IRT framework, Journal of Educational Measurement, 50, pp. 164-185, (2013); 
Edwards M.C., Purple unicorns, true models, and other thigs I've never seen, Measurement: Interdisciplinary Research and Perspectives, 11, pp. 107-111, (2013); 
Goldhammer F., Martens T., Christoph G., Ludtke O., Test-taking engagement in PIAAC (OECD education working papers no. no. 133), (2016); 
Goldhammer F., Martens T., Ludtke O., Conditioning factors of test-taking engagement in PIAAC: An exploratory IRT modelling approach considering person and item characteristics, Large-scale Assessments in Education, 5, pp. 1-25, (2017); 
Goldhammer F., Naumann J., Stelter A., Toth K., Rolke H., Klieme E., The time on task effect in reading and problem solving is moderated by task difficulty and skill: Insights from a computer-based large-scale assessment, Journal of Educational Psychology, 106, pp. 608-626, (2014); 
Kroehne U., Deribo T., Goldhammer F., Rapid guessing rates across administration mode and test setting, Psychological Test and Assessment Modeling, 62, pp. 147-177, (2020); 
Kunter M., Schumer G., Artelt C., Baumert J., Klieme E., Neubrand M., Et al., PISA 2000: Documentation of the study instruments, (2002); 
Leary L.F., Dorans N.J., Implications for altering the context in which test items appear: A historical perspective on an immediate concern, Review of Educational Research, 55, pp. 387-413, (1985); 
Lee Y.H., Jia Y., Using response time to investigate students' test-taking behaviours in a NAEP computer-based study, Large-scale Assessments in Education, 2, pp. 1-24, (2014); 
Lindner C., Nagy G., Retelsdorf J., The need for self-control in achievement tests: Changes in students' state self-control capacity and effort investment, Social Psychology of Education, 21, pp. 1113-1131, (2018); 
Lindner M.A., Representational and decorative pictures in science and mathematics tests: Do they make a difference?, Learning and Instruction, 68, (2020); 
Lindner M.A., Ludtke O., Grund S., Koller O., The merits of representational pictures in educational assessment: Evidence for cognitive and motivational effects in a time-on-task analysis, Contemporary Educational Psychology, 51, pp. 482-492, (2017); 
Lindner M.A., Ludtke O., Nagy G., The onset of rapid-guessing behaviour over the course of testing time: A matter of motivational and cognitive resources, Frontiers in Psychology, 10, (2019); 
List M.K., Robitzsch A., Ludtke O., Koller O., Nagy G., Performance decline in low-stakes educational assessments: Different mixture modeling approaches, Large-scale Assessments in Education, 5, (2017); 
Lubke G.H., Muthen B., Investigating population heterogeneity with factor mixture models, Psychological Methods, 10, pp. 21-39, (2005); 
Meyers J.L., Miller G.E., Way W.D., Item position and item difficulty change in an IRT-based common item equating design, Applied Measurement in Education, 22, pp. 38-60, (2009); 
Molenaar D., de Boeck P., Response mixture modeling: Accounting for heterogeneity in item characteristics across response times, Psychometrika, 83, pp. 279-297, (2018); 
Molenaar D., Oberski D., Vermunt J., De Boeck P., Hidden Markov item response theory models for responses and response times, Multivariate Behavioral Research, 51, pp. 606-626, (2016); 
Muthen L.K., Muthen B.O., Mplus user's guide, (1998); 
Nagy G., Ludtke O., Koller O., Modeling test context effects in longitudinal achievement data: Examining position effects in the longitudinal German PISA 2012 assessment, Psychological Test and Assessment Modeling, 58, pp. 641-670, (2016); 
Nagy G., Nagengast B., Becker M., Rose N., Frey A., Item position effects in a reading comprehension test: An IRT study of individual differences and individual correlates, Psychological Test and Assessment Modeling, 60, pp. 165-187, (2018); 
Nagy G., Nagengast B., Frey A., Becker M., Rose N., A multilevel study of position effects in PISA achievement tests: Student-and school-level predictors in the German tracked school system, Assessment in Education: Principles, Policy & Practice, 26, pp. 422-443, (2019); 
Nagy G., Robitzsch A., A continuous HYBRID IRT model for modeling changes in guessing behaviour in proficiency tests, Psychological Test and Assessment Modeling, 63, pp. 361-395, (2021); 
Nagy G., Ulitzsch E., A multilevel mixture IRT framework for modelling response times as predictors or indicators of response engagement in IRT models, Educational and Psychological Measurement, (2021); 
Pools E., Monseur C., Student test-taking effort in low-stakes assessments: Evidence from the English version of the PISA 2015 science test, Large-scale Assessments in Education, 9, pp. 1-31, (2021); 
Ren X., Goldhammer F., Moosbrugger H., Schweizer K., How does attention relate to the ability-specific and position-specific components of reasoning measured by APM?, Learning and Individual Differences, 22, pp. 1-7, (2012); 
Rose N., Nagy G., Nagengast B., Frey A., Becker M., Modeling multiple item context effects with generalized linear mixed models, Frontiers in Psychology, 10, (2019); 
Schnipke D.L., Scrams D.J., Modelling item response times with a two-state mixture model: A new method of measuring speediness, Journal of Educational Measurement, 34, pp. 213-232, (1997); 
Schwarz G., Estimating the dimension of a model, Annals of Statistics, 6, pp. 461-464, (1978); 
Schweizer K., Schreiner M., Gold A., The confirmatory investigation of APM items with loadings as a function of the position and easiness of items: A two-dimensional model of APM, Psychology Science Quarterly, 51, pp. 47-64, (2009); 
Sclove S.L., Application of model-selection criteria to some problems in multivariate analysis, Psychometrika, 52, pp. 333-343, (1987); 
Sen S., Cohen A.S., Kim S.H., Model selection for multilevel mixture Rasch models, Applied Psychological Measurement, 43, pp. 272-289, (2019); 
Shin J., Bulut O., Gierl M.J., The effect of the Most-attractive-distractor location on multiple-choice item difficulty, The Journal of Experimental Education, 88, pp. 643-659, (2020); 
Ulitzsch E., Penk C., von Davier M., Pohl S., Model meets reality: Validating a new behavioral measure for test-taking effort, Educational Assessment, 26, pp. 104-124, (2021); 
Weirich S., Hecht M., Bohme K., Modeling item position effects using generalized linear mixed models, Applied Psychological Measurement, 38, pp. 535-548, (2014); 
Weirich S., Hecht M., Penk C., Roppelt A., Bohme K., Item position effects are moderated by changes in test-taking effort, Applied Psychological Measurement, 41, pp. 115-129, (2017); 
Wise S., Kuhfeld M., A method for identifying partial test-taking engagement, Applied Measurement in Education, 34, pp. 250-161, (2021); 
Wise S.L., An investigation of the differential effort received by items on a low-stakes computer-based test, Applied Measurement in Education, 19, pp. 93-112, (2006); 
Wise S.L., Strategies for managing the problem of unmotivated examinees in low-stakes testing programs, Journal of General Education, 58, pp. 152-166, (2009); 
Wise S.L., Effort analysis: Individual score validation of achievement test data, Applied Measurement in Education, 28, pp. 237-252, (2015); 
Wise S.L., Rapid-guessing behaviour: Its identification, interpretation, and implications, Educational Measurement: Issues and Practice, 36, pp. 52-61, (2017); 
Wise S.L., An information-based approach to identifying rapid-guessing thresholds, Applied Measurement in Education, 32, pp. 325-336, (2019); 
Wise S.L., DeMars C.E., Examinee noneffort and the validity of program assessment results, Educational Assessment, 15, pp. 27-41, (2010); 
Wise S.L., Gao L., A general approach to measuring test-taking effort on computer-based tests, Applied Measurement in Education, 30, pp. 343-354, (2017); 
Wise S.L., Pastor D.A., Kong X.J., Correlates of rapid-guessing behaviour in low-stakes testing: Implications for test development and measurement practice, Applied Measurement in Education, 22, pp. 185-205, (2009); 
Wise S.L., Smith L.F., A model of examinee test-taking effort, High-stakes testing in education: Science and practice in Kâ€“12 settings, pp. 139-153, (2011); 
Wise S.L., Smith L.F., The validity of assessment when students don't give good effort, Handbook of human and social conditions in assessment, pp. 204-220, (2016); 
Yildirim-Erbasli S.N., Bulut O., The impact of students' test-taking effort on growth estimates in low-stakes educational assessments, Educational Research and Evaluation, 26, pp. 368-386, (2021)#FRF#
