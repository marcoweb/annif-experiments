#ITI#Deep neural networks for collaborative learning analytics: Evaluating team collaborations using student gaze point prediction#FTI#
#IRE#Automatic assessment and evaluation of team performance during collaborative tasks is key to the research on learning analytics and computer-supported cooperative work. There is growing interest in the use of gaze-oriented cues for evaluating the collaboration and cooperativeness of teams. However, collecting gaze data using eye-trackers is not always feasible due to time and cost constraints. In this paper, we introduce an automated team assessment tool based on gaze points and joint visual attention (JVA) information drawn from computer vision solutions. We evaluated team collaborations in an undergraduate anatomy learning activity (N = 60, 30 teams) as a test user study. The results indicate that higher JVA was positively associated with student learning outcomes (r(30) = 0.50, p < 0.005). Moreover, teams who participated in two experimental groups and used interactive 3D anatomy models, had higher JVA (F(1,28) = 6.65, p < 0.05) and better knowledge retention (F(1,28) = 7.56, p < 0.05) than those in the control group. Also, no significant difference was observed based on JVA for different gender compositions of teams. The findings from this work have implications in learning sciences and collaborative computing by providing a novel joint attention-based measure to objectively evaluate team collaboration dynamics. Implications for practice or policy: † Student learning outcomes can be improved by receiving constructive feedback about team performances using our gaze-based collaborative learning method. † Underrepresented and underserved minorities of science, technology, engineering and mathematics disciplines can be engaged in more collaborative problem-solving and team-based learning activities since our method offers a broader reach by automating collaboration assessment process. † Course leaders can assess the quality of attention and engagement among students and can monitor or assist larger numbers of students simultaneously#FRE#
#IPC#co-located team-based learning; collaborative learning analytics; deep learning; experimental design; gaze following; joint visual attention#FPC#
#IRF#Andersson J., Net effect of memory collaboration: How is collaboration affected by factors such as friendship, gender and age?, Scandinavian Journal of Psychology, 42, 4, pp. 367-375, (2001); 
Baker R. S., Yacef K., The state of educational data mining in 2009: A review and future visions, Journal of Educational Data Mining, 1, 1, pp. 3-17, (2009); 
Barmaki R., Yu K., Pearlman R., Shingles R., Bork F., Osgood G. M., Navab N., Enhancement of anatomical education using augmented reality: An empirical study of body painting, Anatomical Sciences Education, 16, 6, pp. 599-609, (2019); 
Barzansky B., Educational programs in US medical schools, 1996-1997, JAMA: The Journal of the American Medical Association, 278, 9, pp. 744-749, (1997); 
Bear J. B., Woolley A. W., The role of gender in team collaboration and performance, Interdisciplinary Science Reviews, 36, 2, pp. 146-153, (2011); 
Bente G., Eschenburg F., Kramer N. C., Virtual gaze. A pilot study on the effects of computer simulated gaze in avatar-based conversations, Lecture notes in computer science: Vol. 4563. Virtual reality, pp. 185-194, (2007); 
Bertsimas D., Gupta S., Fairness and collaboration in network air traffic flow management: An optimization approach, Transportation Science, 50, 1, pp. 57-76, (2016); 
Brennan S. E., Chen X., Dickinson C. A., Neider M. B., Zelinsky G. J., Coordinating cognition: The costs and benefits of shared gaze during collaborative search, Cognition, 106, 3, pp. 1465-1477, (2008); 
Brooks M., More women than men enrolled in US med schools for first time, Medscape, (2017); 
Bruinsma Y., Koegel R. L., Koegel L. K., Joint attention and children with autism: A review of the literature, Mental Retardation and Developmental Disabilities Research Reviews, 10, 3, pp. 169-175, (2004); 
Bryant T., Radu I., Schneider B., A qualitative analysis of joint visual attention and collaboration with high-and low-achieving groups in computer-mediated learning, Proceedings of the 13th International Conference on Computer Supported Collaborative Learning, 2, pp. 923-924, (2019); 
Carpenter M., Tomasello M., Joint attention and imitative learning in children, chimpanzees, and enculturated chimpanzees, Social Development, 4, 3, pp. 217-237, (1995); 
de Freitas S., Griffiths M., Online gaming as an educational tool in learning and training, British Journal of Educational Technology, 38, 3, pp. 535-537, (2007); 
De Paola M., Gioia F., Scoppa V., Teamwork, leadership and gender, (2018); 
Eagly A. H., Carli L. L., The female leadership advantage: An evaluation of the evidence, The Leadership Quarterly, 14, 6, pp. 807-834, (2003); 
Fernandez-Sanz L., Misra S., Analysis of cultural and gender influences on teamwork performance for software requirements analysis in multinational environments, IET Software, 6, 3, pp. 167-175, (2012); 
Flor M., Yoon S.-Y., Hao J., Liu L., von Davier A., Automated classification of collaborative problem solving interactions in simulated science tasks, Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pp. 31-41, (2016); 
Garzon J., Baldiris S., Gutierrez J., Pavon J., How do pedagogical approaches affect the impact of augmented reality on education? A meta-analysis and research synthesis, Educational Research Review, 31, (2020); 
Gelderblom H., Matthee M., Hattingh M., Weilbach L., High school learners’ continuance intention to use electronic textbooks: A usability study, Education and Information Technologies, 24, 2, pp. 1753-1776, (2019); 
Golenhofen N., Heindl F., Grab-Kroll C., Messerer D. A., Bockers T. M., Bockers A., The use of a mobile learning tool by medical students in undergraduate anatomy and its effects on assessment outcomes, Anatomical Sciences Education, 13, 1, pp. 8-18, (2020); 
Guo Z., Barmaki R., Collaboration analysis using object detection, Proceedings of the 12th International Conference on Educational Data Mining, pp. 695-698, (2019); 
Hackett M., Proctor M., The effect of autostereoscopic holograms on anatomical knowledge: A randomised trial, Medical Education, 52, 11, pp. 1147-1155, (2018); 
Hansen D. W., Ji Q., In the eye of the beholder: A survey of models for eyes and gaze, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32, 3, pp. 478-500, (2009); 
Harari D., Tenenbaum J. B., Ullman S., Discovery and usage of joint attention in images, (2018); 
Herring S. C., Computer-mediated communication: Linguistic, social, and cross-cultural perspectives, (1996); 
Hirotani M., Stets M., Striano T., Friederici A. D., Joint attention helps infants learn new words: Event-related potential evidence, Neuroreport, 20, 6, pp. 600-605, (2009); 
Huang K., Bryant T., Schneider B., Identifying collaborative learning States using unsupervised machine learning on eye-tracking, physiological and motion sensor data, Proceedings of the 12th International Conference on Educational Data Mining, pp. 318-323, (2019); 
Innes J. E., Booher D. E., Collaborative rationality as a strategy for working with wicked problems, Landscape and Urban Planning, 154, pp. 8-10, (2016); 
Kim Y., D'Angelo C., Cafaro F., Ochoa X., Espino D., Kline A., Hamilton E., Lee S., Butail S., Liu L., Trajkova M., Tscholl M., Hwang J., Lee S., Kwon K., Multimodal data analytics for assessing collaborative interactions, Proceedings of the 14th International Conference on Learning Sciences, 5, pp. 2547-2554, (2020); 
Kotsiantis S. B., Use of machine learning techniques for educational proposes: A decision support system for forecasting students’ grades, Artificial Intelligence Review, 37, 4, pp. 331-344, (2012); 
Lemos R. R., Rudolph C. M., Batista A. V., Conceicao K. R., Pereira P. F., Bueno B. S., Fiuza P. J., Mansur S. S., Design of a Web3D serious game for human anatomy education: A Web3D game for human anatomy education, Handbook of research on immersive digital games in educational environments, pp. 586-611, (2019); 
Li Q., Lau R. W. H., Shih T. K., Li F. W. B., Technology supports for distributed and collaborative learning over the internet, ACM Transactions on Internet Technology, 8, 2, pp. 1-24, (2008); 
Lian D., Yu Z., Gao S., Believe it or not, we know what you are looking at!, Lecture notes in computer science: Vol. 11363. Asian conference on computer vision, pp. 35-50, (2018); 
Lipponen L., The challenges for computer supported collaborative learning in elementary and secondary level: Finnish perspectives, Proceedings of the 1999 Conference on Computer Support for Collaborative Learning, pp. 519-528, (1999); 
Luursema J.-M., Verwey W. B., Kommers P. A., Annema J.-H., The role of stereopsis in virtual anatomical learning, Interacting with Computers, 20, 4-5, pp. 455-460, (2008); 
Luursema J.-M., Verwey W. B., Kommers P. A., Geelkerken R. H., Vos H. J., Optimizing conditions for computer-assisted anatomical learning, Interacting with Computers, 18, 5, pp. 1123-1138, (2006); 
Maresky H., Oikonomou A., Ali I., Ditkofsky N., Pakkal M., Ballyk B., Virtual reality and cardiac anatomy: Exploring immersive three-dimensional cardiac imaging, a pilot study in undergraduate medical anatomy education, Clinical Anatomy, 32, 2, pp. 238-243, (2019); 
Marieb E. N., Essentials of human anatomy & physiology laboratory manual, (2015); 
Marin-Jimenez M. J., Zisserman A., Eichner M., Ferrari V., Detecting people looking at each other in videos, International Journal of Computer Vision, 106, 3, pp. 282-296, (2014); 
Markus J., Mundy P., Morales M., Delgado C. E., Yale M., Individual differences in infant skills as predictors of child-caregiver joint attention and language, Social Development, 9, 3, pp. 302-315, (2000); 
Meadows L. A., Sekaquaptewa D., Paretti M. C., Interactive panel: Improving the experiences of marginalized students on engineering design teams, Proceedings of the ASEE Annual Conference & Exposition, 26, pp. 1-23, (2015); 
Meiksins P., Beddoes K., Layne P., McCusker M., Camargo E., Women in engineering: A review of the 2014 literature, Consulting – Specifying Engineer, (2015); 
Miura G., Okada S., Task-independent multimodal prediction of group performance based on product dimensions, Proceedings of the 2019 International Conference on Multimodal Interaction, pp. 264-273, (2019); 
Morris S. B., Estimating effect sizes from pretest-posttest-control group designs, Organizational Research Methods, 11, 2, pp. 364-386, (2008); 
Mukherjee S. S., Robertson N. M., Deep head pose: Gaze-direction estimation in multimodal video, IEEE Transactions on Multimedia, 17, 11, pp. 2094-2107, (2015); 
Murray G., Oertel C., Predicting group performance in task-based interaction, Proceedings of the 20th ACM International Conference on Multimodal Interaction, pp. 14-20, (2018); 
Ng J., Hu X., Luo M., Chu S. K. W., Relations among participation, fairness and performance in collaborative learning with Wiki-based analytics, Proceedings of the Association for Information Science and Technology, 56, 1, pp. 463-467, (2019); 
Nicholson D. T., Chalk C., Funnell W. R. J., Daniel S. J., Can virtual reality improve anatomy education? A randomised controlled study of a computer-generated three-dimensional anatomical ear model, Medical Education, 40, 11, pp. 1081-1087, (2006); 
Ochoa X., Chiluiza K., Mendez G., Luzardo G., Guaman B., Castells J., Expertise estimation based on simple multimodal features, Proceedings of the 15th ACM on International Conference on Multimodal Interaction, pp. 583-590, (2013); 
Okita S., Bailenson J., Schwartz D., Mere belief of social action improves complex learning, Proceedings of the 8th International Conference for The Learning Sciences, 2, pp. 132-139, (2008); 
Otsuka K., Kasuga K., Kohler M., Estimating visual focus of attention in multiparty meetings using deep convolutional neural networks, Proceedings of the 20th ACM International Conference on Multimodal Interaction, pp. 191-199, (2018); 
Patacchiola M., Cangelosi A., Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods, Pattern Recognition, 71, pp. 132-143, (2017); 
Pietinen S., Bednarik R., Tukiainen M., Shared visual attention in collaborative programming: A descriptive analysis, Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of Software Engineering, pp. 21-24, (2010); 
Prinsen F. R., Volman M. L., Terwel J., Gender-related differences in computer-mediated communication and computer-supported collaborative learning, Journal of Computer Assisted Learning, 23, 5, pp. 393-409, (2007); 
Rabbitt P., Donlan C., Watson P., McInnes L., Bent N., Unique and interactive effects of depression, age, socioeconomic advantage, and gender on cognitive performance of normal healthy older people, Psychology and Aging, 10, 3, pp. 307-313, (1995); 
Recasens A., Khosla A., Vondrick C., Torralba A., Where are they looking?, Advances in Neural Information Processing Systems, pp. 199-207, (2015); 
Rezende E., Ruppert G., Carvalho T., Ramos F., De Geus P., Malicious software classification using transfer learning of resnet-50 deep neural network, Proceedings of the 16th IEEE International Conference on Machine Learning and Applications, pp. 1011-1014, (2017); 
Rose C., Wang Y.-C., Cui Y., Arguello J., Stegmann K., Weinberger A., Fischer F., Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning, International Journal of Computer-Supported Collaborative Learning, 3, 3, pp. 237-271, (2008); 
Santini T., Fuhl W., Kasneci E., CalibMe: Fast and unsupervised eye tracker calibration for gaze-based pervasive human-computer interaction, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 2594-2605, (2017); 
Schaf F. M., Muller D., Bruns F. W., Pereira C. E., Erbe H.-H., Collaborative learning and engineering workspaces, Annual Reviews in Control, 33, 2, pp. 246-252, (2009); 
Schaie K. W., Willis S. L., Age difference patterns of psychometric intelligence in adulthood: Generalizability within and across ability domains, Psychology and Aging, 8, 1, (1993); 
Schneider B., Pea R., Real-time mutual gaze perception enhances collaborative learning and collaboration quality, International Journal of Computer-Supported Collaborative Learning, 8, 4, pp. 375-397, (2013); 
Schneider B., Sharma K., Cuendet S., Zufferey G., Dillenbourg P., Pea R., Leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning groups, International Journal of Computer-Supported Collaborative Learning, 13, 3, pp. 241-261, (2018); 
Silva J. N., Southworth M., Raptis C., Silva J., Emerging applications of virtual reality in cardiovascular medicine, JACC: Basic to Translational Science, 3, 3, pp. 420-430, (2018); 
Soller A., Lesgold A., Linton F., Goodman B., What makes peer interaction effective? Modeling effective communication in an intelligent CSCL, Proceedings of 1999 AAAI Fall Symposium: Psychological Models of Communication in Collaborative Systems, pp. 116-123, (1999); 
Spikol D., Ruffaldi E., Cukurova M., Using multimodal learning analytics to identify aspects of collaboration in project-based learning, Proceedings of the 2017 Conference on Computer Support for Collaborative Learning, pp. 263-270, (2017); 
Subburaj S. K., Stewart A. E. B., Ramesh Rao A., D'Mello S. K., Multimodal, multiparty modeling of collaborative problem solving performance, Proceedings of the 2020 International Conference on Multimodal Interaction, pp. 423-432, (2020); 
Sung H.-Y., Hwang G.-J., A collaborative game-based learning approach to improving students’ learning performance in science courses, Computers & Education, 63, pp. 43-51, (2013); 
Tao C., Zhang Q., Zhou Y., Collaborative learning with limited interaction: Tight bounds for distributed exploration in multi-armed bandits, Proceedings of the 2019 IEEE 60th Annual Symposium on Foundations of Computer Science, pp. 126-146, (2019); 
van der Meulen H., Varsanyi P., Westendorf L., Kun A. L., Shaer O., Towards understanding collaboration around interactive surfaces: Exploring joint visual attention, Proceedings of the 29th Annual Symposium on User Interface Software and Technology, pp. 219-220, (2016); 
Van Rheden V., Maurer B., Smit D., Murer M., Tscheligi M., LaserViz: Shared gaze in the Co-located physical world, Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction, pp. 191-196, (2017); 
Vrzakova H., Amon M. J., Stewart A. E. B., D'Mello S. K., Dynamics of visual attention in multiparty collaborative problem solving using multidimensional recurrence quantification analysis, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-14, (2019); 
Wahn B., Schwandt J., Kruger M., Crafa D., Nunnendorf V., Konig P., Multisensory teamwork: Using a tactile or an auditory display to exchange gaze information improves performance in joint visual search, Ergonomics, 59, 6, pp. 781-795, (2016); 
Webb N. M., Troper J. D., Fall R., Constructive activity and learning in collaborative small groups, Journal of Educational Psychology, 87, 3, pp. 406-423, (1995); 
Wegge J., Roth C., Neubach B., Schmidt K.-H., Kanfer R., Age and gender diversity as determinants of performance and health in a public organization: The role of task complexity and group size, Journal of Applied Psychology, 93, 6, pp. 1301-1313, (2008); 
Wendler D., One-time general consent for research on biological samples, BMJ: British Medical Journal, 332, 7540, pp. 544-547, (2006); 
Whalen C., Schreibman L., Joint attention training for children with autism using behavior modification procedures, Journal of Child Psychology and Psychiatry, 44, 3, pp. 456-468, (2003); 
Yammine K., Violato C., A meta-analysis of the educational effectiveness of three-dimensional visualization technologies in teaching anatomy, Anatomical Sciences Education, 8, 6, pp. 525-538, (2015); 
Yucel Z., Salah A. A., Mericli C., Mericli T., Valenti R., Gevers T., Joint attention by gaze interpolation and saliency, IEEE Transactions on Cybernetics, 43, 3, pp. 829-842, (2013); 
Zhao Q., Sheng T., Wang Y., Tang Z., Chen Y., Cai L., Ling H., M2det: A single-shot object detector based on multi-level feature pyramid network, Proceedings of the AAAI Conference on Artificial Intelligence, 33, pp. 9259-9266, (2019); 
Zhu X., Ramanan D., Face detection, pose estimation, and landmark localization in the wild, Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2879-2886, (2012); 
Zou K. H., Tuncali K., Silverman S. G., Correlation and simple linear regression, Radiology, 227, 3, pp. 617-628, (2003)#FRF#
