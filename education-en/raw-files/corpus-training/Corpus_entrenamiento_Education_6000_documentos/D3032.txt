#ITI#Vertical Scales, Deceleration, and Empirical Benchmarks for Growth#FTI#
#IRE# Empirical growth benchmarks, as introduced by Hill, Bloom, Black, and Lipsey (2008), are a well-known way to contextualize effect sizes in education research. Past work on these benchmarks, both positive and negative, has largely avoided confronting the role of vertical scales, yet technical issues with vertical scales trouble the use of such benchmarks. This article introduces vertical scales and outlines their role in the creation of empirical benchmarks for growth. I then outline three strands of recent vertical scale research that call into question the grounds for relying on these benchmarks. I conclude with recommendations for researchers looking to contextualize observed effects of educational interventions without confounding their effects with vertical scaling artifacts#FRE#
#IPC# assessment; educational policy; effect size; evaluation; experimental research; item response theory; program evaluation; psychometrics; research methodology; testing#FPC#
#IRF# Baird M.D., Pane J.F., Translating standardized effects of education programs into more interpretable metrics, Educational Researcher, 48, 4, pp. 217-228, (2019); 
Betebenner D., Norm- and criterion-referenced student growth, Educational Measurement: Issues and Practice, 28, 4, pp. 42-51, (2009); 
Birnbaum A., Some latent trait models and their use in inferring an examinee’s ability, Statistical theories of mental test scores, pp. 395-479, (1968); 
Bloom H.S., Hill C.J., Black A.R., Lipsey M.W., Performance trajectories and performance gaps as achievement effect-size benchmarks for educational interventions, Journal of Research on Educational Effectiveness, 1, 4, pp. 289-328, (2008); 
Bolt D.M., Deng S., Lee S., IRT model misspecification and measurement of growth in vertical scaling, Journal of Educational Measurement, 51, 2, pp. 141-162, (2014); 
Briggs D.C., Measuring growth with vertical scales, Journal of Educational Measurement, 50, 2, pp. 204-226, (2013); 
Briggs D.C., Interpreting and visualizing the unit of measurement in the Rasch Model, Measurement, 146, pp. 961-971, (2019); 
Briggs D.C., Dadey N., Making sense of common test items that do not get easier over time: Implications for vertical scale designs, Educational Assessment, 20, 1, pp. 1-22, (2015); 
Briggs D.C., Domingue B., The gains from vertical scaling, Journal of Educational and Behavioral Statistics, 38, 6, pp. 551-576, (2013); 
Briggs D.C., Weeks J.P., The impact of vertical scaling decisions on growth interpretations, Educational Measurement: Issues and Practice, 28, 4, pp. 3-14, (2009); 
Castellano K.E., Ho A.D., Practical differences among aggregate-level conditional status metrics: From median student growth percentiles to value-added models, Journal of Educational and Behavioral Statistics, 40, 1, pp. 35-68, (2015); 
Cohen J., Statistical power analysis for the behavioral sciences, (1969); 
Dadey N., Briggs D.C., A meta-analysis of growth trends from vertically scaled assessments, Practical Assessment, Research and Evaluation, 17, (2012); 
Domingue B., Evaluating the equal-interval hypothesis with test score scales, Psychometrika, 79, 1, pp. 1-19, (2014); 
Embretson S.E., Reise S.P., Item Response Theory for Psychologists, (2000); 
Haebara T., Equating logistic ability scales by a weighted least squares method, Japanese Psychological Research, 22, 3, pp. 144-149, (1980); 
Hill C.J., Bloom H.S., Black A.R., Lipsey M.W., Empirical benchmarks for interpreting effect sizes in research, Child Development Perspectives, 2, 3, pp. 172-177, (2008); 
Kolen M.J., Brennan R.L., Test Equating, Scaling, and Linking, (2014); 
Kraft M.A., Interpreting effect sizes of education interventions, Educational Researcher, 49, 4, pp. 241-253, (2020); 
Li Y., Lissitz R.W., Exploring the full-information bifactor model in vertical scaling with construct shift, Applied Psychological Measurement, 36, 1, pp. 3-20, (2012); 
Lord F.M., Novick M.R., Statistical theories of mental test scores, (1968); 
Lortie-Forgues H., Sio U.N., Inglis M., How should educational effects be communicated to teachers?, Educational Researcher, 50, 6, pp. 345-354, (2021); 
Loyd B.H., Hoover H.D., Vertical equating using the Rasch model, Journal of Educational Measurement, 17, 3, pp. 179-193, (1980); 
Luce R.D., Tukey J.W., Simultaneous conjoint measurement: A new type of fundamental measurement, Journal of Mathematical Psychology, 1, 1, pp. 1-27, (1964); 
Marco G.L., Item characteristic curve solutions to three intractable testing problems, (1977); 
Martineau J.A., Distorting value added: The use of longitudinal, vertically scaled student achievement data for growth-based, value-added accountability, Journal of Educational and Behavioral Statistics, 31, 1, pp. 35-62, (2006); 
Maul A., Torres Irribarra D., Wilson M., On the philosophical foundations of psychological measurement, Measurement, 79, pp. 311-320, (2016); 
Rasch G., Probabilistic models for some intelligence and attainment tests, (1960); 
Samejima F., Logistic positive exponent family of models: Virtue of asymmetric item characteristic curves, Psychometrika, 65, 3, pp. 319-335, (2000); 
Simpson A., Benchmarking a misnomer: A note on “Interpreting Effect Sizes in Education Interventions., Educational Researcher, (2021); 
Stocking M.L., Lord F.M., Developing a common metric in Item Response Theory, Applied Psychological Measurement, 7, 2, pp. 201-210, (1983); 
Tong Y., Kolen M.J., Comparisons of methodologies and results in vertical scaling for educational achievement tests, Applied Measurement in Education, 20, 2, pp. 227-253, (2007); 
Tong Y., Kolen M.J., Scaling: An ITEMS Module: Winter 2010, Educational Measurement: Issues and Practice, 29, 4, pp. 39-48, (2010)#FRF#
