#ITI#Using automated analysis to assess middle school students' competence with scientific argumentation#FTI#
#IRE# Argumentation is fundamental to science education, both as a prominent feature of scientific reasoning and as an effective mode of learning—a perspective reflected in contemporary frameworks and standards. The successful implementation of argumentation in school science, however, requires a paradigm shift in science assessment from the measurement of knowledge and understanding to the measurement of performance and knowledge in use. Performance tasks requiring argumentation must capture the many ways students can construct and evaluate arguments in science, yet such tasks are both expensive and resource-intensive to score. In this study we explore how machine learning text classification techniques can be applied to develop efficient, valid, and accurate constructed-response measures of students' competency with written scientific argumentation that are aligned with a validated argumentation learning progression. Data come from 933 middle school students in the San Francisco Bay Area and are based on three sets of argumentation items in three different science contexts. The findings demonstrate that we have been able to develop computer scoring models that can achieve substantial to almost perfect agreement between human-assigned and computer-predicted scores. Model performance was slightly weaker for harder items targeting higher levels of the learning progression, largely due to the linguistic complexity of these responses and the sparsity of higher-level responses in the training data set. Comparing the efficacy of different scoring approaches revealed that breaking down students' arguments into multiple components (e.g., the presence of an accurate claim or providing sufficient evidence), developing computer models for each component, and combining scores from these analytic components into a holistic score produced better results than holistic scoring approaches. However, this analytical approach was found to be differentially biased when scoring responses from English learners (EL) students as compared to responses from non-EL students on some items. Differences in the severity between human and computer scores for EL between these approaches are explored, and potential sources of bias in automated scoring are discussed#FRE#
#IPC# argumentation; assessment; automated analysis; bias; machine learning#FPC#
#IRF# Aggarwal C.C., Zhai C., Mining text data, (2012); 
Anderson C.W., de los Santos E.X., Bodbyl S., Covitt B.A., Edwards K.D., Hancock J.B., Lin Q., Morrison Thomas C., Penuel W.R., Welch M.M., Designing educational systems to support enactment of the next generation science standards, Journal of Research in Science Teaching, 55, 7, pp. 1026-1052, (2018); 
Au W., High stakes testing and curricular control: A qualitative metasynthesis, Educational Researcher, 36, 5, pp. 258-267, (2007); 
Bachelard G., The Philosophy of No: A philosophy of the new scientific mind, (1968); 
Baker D., The schooled society: The educational transformation of global culture, (2014); 
Barton A.C., Feminist science education, (1998); 
Bejar I.I., Williamson D.M., Mislevy R.J., Human scoring, Automated scoring of complex tasks in computer-based testing, pp. 49-81, (2006); 
Boone W.J., Staver J.R., Yale M.S., Rasch analysis in the human sciences, (2014); 
Bouillion L.M., Gomez L.M., Connecting school and community with science learning: Real world problems and school-community partnerships as contextual scaffolds, Journal of Research in Science Teaching, 38, 8, pp. 878-898, (2001); 
Briggs D.C., Wilson M., Generalizability in item response modeling, Journal of Educational Measurement, 44, 2, pp. 131-155, (2007); 
Caliskan A., Bryson J., Narayanan A., Semantics derived automatically from language corpora contain human-like biases, Science, 356, pp. 183-186, (2017); 
Cavagnetto A., Argument to foster scientific literacy: A review of argument interventions in K-12 science contexts, Review of Educational Research, 80, 3, pp. 336-371, (2010); 
Chi M., Active-constructive-interactive: A conceptual framework for differentiating learning activities, Topics in Cognitive Science, 1, pp. 73-105, (2009); 
Crombie A.C., Styles of scientific thinking in the European tradition: The history of argument and explanation especially in the mathematical and biomedical sciences and arts, (1994); 
Cronbach L.J., Rajaratnam N., Gleser G.C., Theory of generalizability: A liberalization of reliability theory, British Journal of Statistical Psychology, 16, 2, pp. 137-163, (1963); 
Driver R., Newton P., Osborne J.F., Establishing the norms of scientific argumentation in classrooms, Science Education, 84, 3, pp. 287-312, (2000); 
Ford M.J., Disciplinary authority and accountability in scientific practice and learning, Science Education, 92, 3, pp. 404-423, (2008); 
Fuller T., The voice of liberal learning: Michael Oakeshott on education, (1989); 
Ha M., Nehm R., The impact of misspelled words on automated computer scoring: A case study of scientific explanations, Journal of Science Education and Technology, 25, pp. 358-374, (2016); 
Hattie J., The black box of tertiary assessment: An impending revolution, Tertiary assessment and higher education student outcomes: Policy, practice and research, pp. 259-275, (2009); 
Henard F., Roseveare D., Fostering quality teaching in higher education: Policies and practices: An IMHE guide for higher education institutions, (2012); 
James G., Witten D., Hastie T., Tibshirani R., An introduction to statistical learning, 112, (2013); 
Jescovitch L.N., Scott E.E., Cerchiara J.A., Doherty J.H., Wenderoth M.P., Merrill J.E., Urban-Lurain M., Haudek K.C., Deconstruction of holistic rubrics into analytic rubrics for large-scale assessments of students' reasoning of complex science concepts, Research Evaluation, 24, 7, pp. 1-13, (2019); 
Jescovitch L.N., Scott E.E., Cerchiara J.A., Merrill J., Urban-Lurain M., Doherty J.H., Haudek K.C., Comparison of machine learning performance using analytic and holistic coding approaches across constructed response assessments aligned to a science learning progression, Journal of Science Education and Technology, 30, pp. 150-167, (2020); 
Jurka T.P., Collingwood L., Boydstun A.E., Grossman E., Van Atteveldt W., RTextTools: A supervised learning package for text classification, The R Journal, 5, 1, pp. 6-12, (2013); 
Kelly G., Takao A., Epistemic levels in argument: An analysis of university oceanography students' use of evidence in writing, Science Education, 86, 3, pp. 314-342, (2002); 
Krippendorff K., Computing Krippendorff's alpha-reliability, (2011); 
Kuhn M., Johnson K., Applied predictive modeling, 26, (2013); 
Landis J.R., Koch G.G., The measurement of observer agreement for categorical data, Biometrics, 33, (1977); 
Lee H.-S., Gweon G.-H., Lord T., Paessel N., Pallant A., Pryputniewicz S., Machine learning-enabled automated feedback: Supporting students' revision of scientific arguments based on data drawn from simulation, Journal of Science Education and Technology, 30, 5, pp. 168-192, (2021); 
Lee H.-S., Pallant A., Pryputniewicz S., Liu O.L., Measuring students' scientific argumentation associated with uncertain current science. Paper presented at the National Association for Research in Science Teaching. San Juan, Puerto Rico, (2013); 
Lee H.-S., Pallant A., Pryputniewicz S., Lord T., Mulholland M., Liu O.L., Automated text scoring and real-time adjustable feedback: Supporting revision of scientific arguments involving uncertainty, Science Education, 103, pp. 590-622, (2019); 
Linacre J.M., Many-facet Rasch measurement, (1989); 
Linacre J.M., Winsteps® Rasch measurement computer program, (2020); 
Liu O.L., Brew C., Blackmore J., Gerard L., Madhok J., Linn M.C., Automated scoring of constructed-response science items: Prospects and obstacles, Educational Measurement: Issues and Practice, 33, 2, pp. 19-28, (2014); 
Liu O.L., Rios J.A., Heilman M., Gerard L., Linn M.C., Validation of automated scoring of science assessments, Journal of Research in Science Teaching, 53, 2, pp. 215-233, (2016); 
MacPherson A., Morell L., Dozier S., A learning progression for interdependent relationships in ecosystems [roundtable session]. AERA annual meeting, San Francisco, CA, (2020); 
Mao L., Liu O.L., Roohr K., Belur V., Mulholland M., Lee H.-S., Pallant A., Validation of automated scoring for a formative assessment that employs scientific argumentation, Educational Assessment, 23, 2, pp. 121-138, (2018); 
Mercier H., Sperber D., Why do humans reason? Arguments for an argumentative theory, Behavioral and Brain Sciences, 34, 2, pp. 57-74, (2011); 
Moharreri K., Ha M., Nehm R.H., EvoGrader: An online formative assessment tool for automatically evaluating written evolutionary explanations, Evolution: Education and Outreach, 7, 1, pp. 1-14, (2014); 
Myford C.M., Wolfe E.W., Detecting and measuring rater effects using many-facet Rasch measurement: Part 1, Journal of Applied Measurement, 4, 4, pp. 386-422, (2003); 
The condition of education 2010, (2011); 
English language learners in public schools, (2020); 
Taking science to school: Learning and teaching in grades K-8, (2007); 
A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, (2012); 
STEM learning is everywhere: Summary of a convocation on building learning systems. Planning committee on STEM learning is everywhere: Engaging schools and empowering teachers to integrate formal, informal, and after-school education to enhance teaching and learning in grades K-8, (2014); 
Nehm R.H., Ha M., Mayfield E., Transforming biology assessment with machine learning: Automated scoring of written evolutionary explanations, Journal of Science Education and Technology, 21, 1, pp. 183-196, (2012); 
Next generation science standards: For states, by states, (2013); 
The future of education and skills: Education 2030, (2018); 
Osborne J.F., Arguing to learn in science: The role of collaborative, critical discourse, Science, 328, pp. 463-466, (2010); 
Osborne J.F., Henderson B., MacPherson A., Szu E., Wild A., Shi-Ying Y., The development and validation of a learning progression for argumentation in science, Journal of Research in Science Teaching, 53, 6, pp. 821-846, (2016); 
Pellegrino J.W., Wilson M.R., Koenig J.A., Beatty A.S., Developing assessments for the next generation science standards, (2014); 
Perry A.M., Lee N.T., AI is coming to schools, and if we're not careful, so will its biases, (2019); 
Rasch G., Probabilistic models for some intelligence and attainment tests, (1960); 
Designing, evaluating, and deploying automated scoring systems with validity in mind: Methodological design decisions, Applied Measurement in Education, 31, 3, (2018); 
Rychen D.S., Salganik L.H., Key competencies for a successful life and a well-functioning society, (2003); 
Sandoval W.A., Conceptual and epistemic aspects of students' scientific explanations, Journal of the Learning Sciences, 12, 1, pp. 5-51, (2003); 
Shaw J.M., Threats to the validity of science performance assessments for English language learners, Journal of Research in Science Teaching, 34, pp. 721-743, (1997); 
Shaw J.M., Bunch G.C., Geaney E.R., Analyzing language demands facing English learners on science performance assessments: The SALD framework, Journal of Research in Science Teaching, 47, 8, (2010); 
Shermis M.D., Contrasting state-of-the-art in the machine scoring of short-form constructed responses, Educational Assessment, 20, 1, pp. 46-65, (2015); 
Simon S., Erduran S., Osborne J.F., Learning to teach argumentation: Research and development in the science classroom, International Journal of Science Education, 28, 2-3, pp. 235-260, (2006); 
Solano-Flores G., Who is given tests in what language by whom, when, and where? The need for probabilistic views of language learning in the testing of English language learners, Educational Researcher, 37, 4, pp. 189-199, (2008); 
Sripathi K.N., Moscarella R.A., Yoho R., You H.S., Urban-Lurain M., Merrill J., Haudek K., Mixed student ideas about mechanisms of human weight loss, CBE—Life Sciences Education, 18, 3, (2019); 
Sunal D.W., Wright E.L., The impact of state and national standards on K-12 science teaching, (2006); 
Artificial intelligence market in the US education sector 2018-2022. Market Research Report, (2018); 
Toulmin S., The uses of argument, (1958); 
Uhl J.D., Sripathi K.N., Meir E., Merrill J., Urban-Lurain M., Haudek K.C., Automated writing assessments measure undergraduate learning after completion of a computer-based cellular respiration tutorial, CBE—Life Sciences Education, 20, 3, (2021); 
Expanding a national network for automated analysis of constructed response assessments to reveal student thinking in STEM, Computers in Education Journal, 6, 2, (2015); 
Wang C., Liu X., Wang L., Sun Y., Zhang H., Automated scoring of Chinese grades 7–9 students' competence in interpreting and arguing from evidence, Journal of Science Education and Technology, 30, 2, pp. 269-282, (2021); 
Wiley J., Hastings P., Blaum D., Jaeger A.J., Hughes S., Wallace P., Griffin T.D., Britt M.A., Different approaches to assessing the quality of explanations following a multiple-document inquiry activity in science, International Journal of Artificial Intelligence in Education, 27, 4, pp. 758-790, (2017); 
Wiliam D., What counts as evidence of educational achievement? The role of constructs in the pursuit of equity in assessment, Review of Research in Education, 34, pp. 254-284, (2010); 
Williamson D.M., Xi X., Breyer F.J., A framework for evaluation and use of automated scoring, Educational Measurement: Issues and Practice, 31, 1, pp. 2-13, (2012); 
Wilson M., Constructing measures: An item response modeling approach, (2005); 
Wilson M., Measuring progressions: Assessment structures underlying a learning progression, Journal of Research in Science Teaching, 46, 6, pp. 716-730, (2009); 
Wright B.D., Masters G.N., Rating scale analysis, (1982); 
Yao S.Y., Investigating the validity of a scientific argumentation assessment using psychometric methods. [Doctoral dissertation, University of California-Berkeley, Berkeley, CA], (2013); 
Zhai X., Haudek K., Shi L., Nehm R.H., Urban-Lurain M., From substitution to redefinition: A framework of machine learning-based science assessment, Journal of Research in Science Teaching, 57, 9, pp. 1430-1459, (2020); 
Zhai X., Haudek K.C., Stuhlsatz M.A.M., Wilson C., Evaluation of construct-irrelevant variance yielded by machine and human scoring of a science teacher PCK constructed response assessment, Studies in Educational Evaluation, 67, (2020); 
Zhai X., Yin Y., Pellegrino J.W., Haudek K.C., Shi L., Applying machine learning in science assessment: A systematic review, Studies in Science Education, 56, 1, pp. 111-151, (2020)#FRF#
