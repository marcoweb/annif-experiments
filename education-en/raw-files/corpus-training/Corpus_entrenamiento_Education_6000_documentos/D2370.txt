#ITI#Diagnostic Classification Model for Forced-Choice Items and Noncognitive Tests#FTI#
#IRE# The forced-choice (FC) item formats used for noncognitive tests typically develop a set of response options that measure different traits and instruct respondents to make judgments among these options in terms of their preference to control the response biases that are commonly observed in normative tests. Diagnostic classification models (DCMs) can provide information regarding the mastery status of test takers on latent discrete variables and are more commonly used for cognitive tests employed in educational settings than for noncognitive tests. The purpose of this study is to develop a new class of DCM for FC items under the higher-order DCM framework to meet the practical demands of simultaneously controlling for response biases and providing diagnostic classification information. By conducting a series of simulations and calibrating the model parameters with a Bayesian estimation, the study shows that, in general, the model parameters can be recovered satisfactorily with the use of long tests and large samples. More attributes improve the precision of the second-order latent trait estimation in a long test, but decrease the classification accuracy and the estimation quality of the structural parameters. When statements are allowed to load on two distinct attributes in paired comparison items, the specific-attribute condition produces better a parameter estimation than the overlap-attribute condition. Finally, an empirical analysis related to work-motivation measures is presented to demonstrate the applications and implications of the new model.#FRE#
#IPC# Bayesian estimation; diagnostic classification model; forced-choice format; pairwise comparison#FPC#
#IRF# Aitchison J., The statistical analysis of compositional data, (1986); 
Andrich D., Hyperbolic cosine latent trait models for unfolding direct responses and pairwise preferences, Applied Psychological Measurement, 19, 3, pp. 269-290, (1995); 
Andrich D., Luo G., A law of comparative preference: Distinctions between models of personal preference and impersonal judgment in pair comparison designs, Applied Psychological Measurement, 43, 3, pp. 181-194, (2019); 
Borislow B., The Edwards Personal Preference Schedule (EPPS) and fakability, Journal of Applied Psychology, 42, 1, pp. 22-27, (1958); 
Brooks S.P., Gelman A., General methods for monitoring convergence of iterative simulations, Journal of Computational and Graphical Statistics, 7, 4, pp. 434-455, (1998); 
Brown A., Item response models for forced-choice questionnaires: A common framework, Psychometrika, 81, 1, pp. 135-160, (2016); 
Brown A., Maydeu-Olivares A., Item response modeling of forced-choice questionnaires, Educational and Psychological Measurement, 71, 3, pp. 460-502, (2011); 
Brown A., Maydeu-Olivares A., How IRT can solve problems of ipsative data in forced-choice questionnaires, Psychological Methods, 18, 1, pp. 36-52, (2013); 
Burkner P.-C., Schulte N., Holling H., On the statistical and practical limitations of Thurstonian IRT models, Educational and Psychological Measurement, 79, 5, pp. 827-854, (2019); 
Chen C.-W., Wang W.-C., Chiu M.-M., Ro S., Item selection and exposure control methods for computerized adaptive testing with multidimensional ranking items, Journal of Educational Measurement, 57, 2, pp. 343-369, (2020); 
Chen J., de la Torre J., A general cognitive diagnosis model for expert-defined polytomous attributes, Applied Psychological Measurement, 37, 6, pp. 419-437, (2013); 
Chen J., de la Torre J., Zhang Z., Relative and absolute fit evaluation in cognitive diagnosis modeling, Journal of Educational Measurement, 50, 2, pp. 123-140, (2013); 
Chen Y., Liu J., Xu G., Ying Z., Statistical analysis of Q-matrix based diagnostic classification models, Journal of the American Statistical Association, 110, pp. 850-866, (2015); 
Christiansen N.D., Burns G.N., Montgomery G.E., Reconsidering forced-choice item formats for applicant personality assessment, Human Performance, 18, 3, pp. 267-307, (2005); 
Cornwell J.M., Dunlap W.P., On the questionable soundness of factoring ipsative data: A response to Saville and Willson (1992), Journal of Occupational and Organizational Psychology, 67, 2, pp. 89-100, (1994); 
Culpepper S.A., Balamuta J.J., Inferring latent structure in polytomous data with a higher-order diagnostic model, Multivariate Behavioral Research; 
de la Torre J., The generalized DINA model framework, Psychometrika, 76, 2, pp. 179-199, (2011); 
de la Torre J., Douglas J.A., Higher-order latent trait models for cognitive diagnosis, Psychometrika, 69, 3, pp. 333-353, (2004); 
de la Torre J., Hong Y., Deng W., Factors affecting the item parameter estimation and classification accuracy of the dina model, Journal of Educational Measurement, 47, 2, pp. 227-249, (2010); 
de la Torre J., Minchen N., Cognitively diagnostic assessments and the cognitive diagnosis model framework, Psicología Educativa, 20, 2, pp. 89-97, (2014); 
de la Torre J., Ponsoda V., Leenen I., Hontangas P., (2012); 
DeVito A.J., Review of the Myers-Briggs type indicator, Ninth Mental Measurements Yearbook, 2, pp. 1030-1032, (1985); 
Donovan J.J., Dwight S.A., Hurtz G.M., As assessment of the prevalence, severity, and verifiability of entry-level applicant faking using the randomized response technique, Human Performance, 16, 1, pp. 81-106, (2003); 
Embretson S.E., Yang X., A multicomponent latent trait model for diagnosis, Psychometrika, 78, 1, pp. 14-36, (2013); 
Fals-Stewart W., Bircher G.R., Schafer J., Lucente S., The personality of marital distress: An empirical typology, Journal of Personality Assessment, 62, 2, pp. 223-241, (1994); 
Fang G., Liu J., Ying Z., On the identifiability of diagnostic classification models, Psychometrika, 84, 1, pp. 19-40, (2019); 
Goldberg L.R., The development of markers for the Big-Five factor structure, Psychological Assessment, 4, 1, pp. 26-42, (1992); 
Hartz S.M., A Bayesian framework for the unified model for assessing cognitive abilities: Blending theory with practicality, (2002); 
Hausknecht J.P., Candidate persistence and personality test practice effects: Implications for staffing system management, Personnel Psychology, 63, 2, pp. 299-324, (2010); 
Hontangas P.M., de la Torre J., Ponsoda V., Leenen I., Morillo D., Abad F.J., Comparing traditional and IRT scoring of forced-choice tests, Applied Psychological Measurement, 39, 8, pp. 598-612, (2015); 
Hsu C.-L., Wang W.-C., Variable-length computerized adaptive testing using the higher order DINA model, Journal of Educational Measurement, 52, 2, pp. 125-143, (2015); 
Huang H.-Y., Multilevel cognitive diagnosis models for assessing changes in latent attributes, Journal of Educational Measurement, 54, 4, pp. 440-480, (2017); 
Huang H.-Y., Effects of item calibration errors on computerized adaptive testing under cognitive diagnosis models, Journal of Classification, 35, pp. 437-465, (2018); 
Huang H.-Y., Utilizing response times in cognitive diagnostic computerized adaptive testing under the higher-order deterministic input, noisy “and” gate model, British Journal of Mathematical and Statistical Psychology, 73, 1, pp. 109-141, (2020); 
Huang H.-Y., Wang W.-C., The random-effect DINA model, Journal of Educational Measurement, 51, 1, pp. 75-97, (2014); 
Joo S.H., Lee P., Stark S., Adaptive testing with the GGUM-RANK multidimensional forced choice model: Comparison of pair, triplet, and tetrad scoring, Behavior Research Methods, 52, 2, pp. 761-772, (2020); 
Joo S.H., Lee P., Stark S., Development of information functions and indices for the GGUM-RANK multidimensional forced choice IRT model, Journal of Educational Measurement, 55, 3, pp. 357-372, (2018); 
Junker B.W., Sijtsma K., Cognitive assessment models with few assumptions, and connections with nonparametric item response theory, Applied Psychological Measurement, 25, 3, pp. 258-272, (2001); 
Kuder G.F., Diamond E.E., Kuder occupational interest survey: General manual, (1979); 
Lee Y.-S., Park Y.S., Taylan D., International Journal of Testing, 11, 2, pp. 144-177, (2011); 
Leighton J.P., Gierl M.J., Cognitive diagnostic assessment for education: Theory and applications, (2007); 
Liu H., You X., Wang W., Ding S., Chang H.-H., The development of computerized adaptive testing with cognitive diagnosis for an English achievement test in China, Journal of Classification, 30, 2, pp. 152-172, (2013); 
Liu R., Huggins-Manley A.C., Bulut O., Retrofitting diagnostic classification models to responses from IRT-based assessment forms, Educational and Psychological Measurement, 78, 3, pp. 357-383, (2017); 
Lord F.M., Application of item response theory to practical testing problems, (1980); 
Matthews G., Oddy K., Ipsative and normative scales in adjectival measurement of personality: Problems of bias and discrepancy, International Journal of Selection and Assessment, 5, 2, pp. 169-182, (1997); 
Maydeu-Olivares A., Linear item response theory, nonlinear item response theory, and factor analysis: A unified framework, Contemporary psychometrics: A festschrift for Roderick P. McDonald, pp. 73-100, (2005); 
Maydeu-Olivares A., Brown A., Item response modeling of paired comparison and ranking data, Multivariate Behavioral Research, 45, 6, (2010); 
Meade A.W., Psychometric problems and issues involved with creating and using Ipsative measures for selection, Journal of Occupational and Organizational Psychology, 77, 4, pp. 531-552, (2004); 
Morillo D., Leenen I., Abad F.J., Hontangas P.M., de la Torre J., Ponsoda V., A dominance variant under the multi-unidimensional pairwise-preference framework: Model formulation and Markov Chain Monte Carlo Estimation, Applied Psychological Measurement, 40, 7, pp. 500-516, (2016); 
Myers I.B., Mary H.M., Naomi Q., Allan H., MBTI handbook: A guide to the development and use of the Myers-Briggs type indicator consulting, (1998); 
Plummer M., (2003); 
Ravand H., Application of a cognitive diagnostic model to a high-stakes reading comprehension test, Journal of Psychoeducational Assessment, 34, 8, pp. 782-799, (2016); 
Ravand H., Baghaei P., Diagnostic classification models: Recent developments, practical issues, and prospects, International Journal of Testing, 20, 1, pp. 24-56, (2020); 
Revuelta J., Halty L., Ximenez C., Validation of a questionnaire for personality profiling using cognitive diagnostic modeling, The Spanish Journal of Psychology, 21, (2018); 
Rosse J.G., Stecher M.D., Miller J.L., Levin R.A., The impact of response distortion on preemployment testing and hiring decisions, Journal of Applied Psychology, 83, 4, pp. 634-644, (1998); 
Rupp A.A., Templin J.L., Unique characteristics of diagnostic classification models: A comprehensive review of the current state-of-the-art, Measurement: Interdisciplinary Research and Perspectives, 6, 4, (2008); 
Rupp A.A., Templin J.L., Henson R.A., Diagnostic measurement: Theory, methods, and applications, (2010); 
OPQ32r technical manual, (2013); 
Solomon A., Haaga D.A.F., Arnow B.A., Is clinical depression distinct from subthreshold depressive symptoms? A review of the continuity issue in depression research, Journal of Nervous and Mental Disease, 189, 8, pp. 498-506, (2001); 
Stark S., Chernyshenko O.S., Drasgow F., An IRT approach to constructing and scoring pairwise preference items involving stimuli on different dimensions: The multi-unidimensional pairwise-preference model, Applied Psychological Measurement, 29, 3, pp. 184-203, (2005); 
Stark S., Chernyshenko O.S., Drasgow F., White L.A., Adaptive testing with multidimensional pairwise preference items: Improving the efficiency of personality and other noncognitive assessments, Organizational Research Methods, 15, 3, pp. 463-487, (2012); 
Sun J., Xin T., Zhang S., de la Torre J., A polytomous extension of the generalized distance discriminating method, Applied Psychological Measurement, 37, 7, pp. 503-521, (2013); 
Super D.E., The Bernreuter Personality Inventory: A review of research, Psychological Bulletin, 39, 2, pp. 94-125, (1942); 
Templin J.L., Bradshaw L., Measuring the reliability of diagnostic classification model examinee estimates, Journal of Classification, 30, 2, pp. 251-275, (2013); 
Templin J.L., Henson R.A., Measurement of psychological disorders using cognitive diagnosis models, Psychological Methods, 11, 3, pp. 287-305, (2006); 
Thissen D., Bad questions: An essay involving item response theory, Journal of Educational and Behavioral Statistics, 41, 1, (2016); 
Thurstone L.L., A law of comparative judgment, Psychological Review, 34, 4, pp. 273-286, (1927); 
Wang C., Mutual information item selection method in cognitive diagnostic computerized adaptive testing with short test length, Educational and Psychological Measurement, 73, 6, pp. 1017-1035, (2013); 
Wang W.-C., Qiu X.-L., Chen C.-W., Ro S., Item response theory models for multidimensional ranking items, Quantitative psychology research, pp. 49-65, (2016); 
Wang W.-C., Qiu X.L., Chen C.-W., Ro S., Jin K.-Y., Item response theory models for ipsative tests with multidimensional pairwise comparison items, Applied Psychological Measurement, 41, 8, pp. 600-613, (2017); 
Xu G., Zhang S., Identifiability of diagnostic classification models, Psychometrika, 81, 3, pp. 625-649, (2016); 
Yang M., Inceoglu I., Silvester J., (2010); 
Zhan P., Jiao H., Liao D., Cognitive diagnosis modeling incorporating item response times, British Journal of Mathematical and Statistical Psychology, 71, 2, pp. 262-286, (2018); 
Zhan P., Jiao H., Man K., Wang L., Using JAGS for Bayesian cognitive diagnosis modeling: A tutorial, Journal of Educational and Behavioral Statistics, 44, 4, pp. 473-503, (2019)#FRF#
