#ITI#An experimental comparison of multiple-choice and short-answer questions on a high-stakes test for medical students#FTI#
#IRE# Recent advances in automated scoring technology have made it practical to replace multiple-choice questions (MCQs) with short-answer questions (SAQs) in large-scale, high-stakes assessments. However, most previous research comparing these formats has used small examinee samples testing under low-stakes conditions. Additionally, previous studies have not reported on the time required to respond to the two item types. This study compares the difficulty, discrimination, and time requirements for the two formats when examinees responded as part of a large-scale, high-stakes assessment. Seventy-one MCQs were converted to SAQs. These matched items were randomly assigned to examinees completing a high-stakes assessment of internal medicine. No examinee saw the same item in both formats. Items administered in the SAQ format were generally more difficult than items in the MCQ format. The discrimination index for SAQs was modestly higher than that for MCQs and response times were substantially higher for SAQs. These results support the interchangeability of MCQs and SAQs. When it is important that the examinee generate the response rather than selecting it, SAQs may be preferred. The results relating to difficulty and discrimination reported in this paper are consistent with those of previous studies. The results on the relative time requirements for the two formats suggest that with a fixed testing time fewer SAQs can be administered, this limitation more than makes up for the higher discrimination that has been reported for SAQs. We additionally examine the extent to which increased difficulty may directly impact the discrimination of SAQs. © NBME 2023.#FRE#
#IPC# Constructed response; Item performance; Multiple choice; Short answer#FPC#
#IRF# Baldwin P., A problem with the bookmark procedure’s correction for guessing, Educational Measurement, 40, pp. 7-15, (2021); 
Bridgeman B., A simple answer to a simple question on answer changing, Journal of Educational Measurement, 49, pp. 467-468, (2012); 
Clauser B.E., Margolis M.J., Swanson D.B., An examination of the contribution of the computer-based case simulations to the USMLE Step 3 examination, Academic Medicine (RIME Supplement), 77, 10, pp. S80-S82, (2002); 
Heemskerk L., Norman G., Chou S., Mintz M., Mandin H., McLaughlin K., The effect of question format and task difficulty on reasoning strategies and diagnostic performance in internal medicine residents, Advances in Health Sciences Education, 13, 4, pp. 453-462, (2008); 
Hift R.J., Should essays and other “open-ended”-type questions retain a place in written summative assessment in clinical medicine?, BMC Medical Education, 14, 1, pp. 1-18, (2014); 
Hubbard J.P., Levit E.J., Early History of NBME, (1985); 
Lindquist E.F., Hoover H.D., Some notes on corrections for guessing and related problems, Educational Measurement: Issues and Practice, 34, 2, pp. 15-19, (2015); 
Newble D.I., Baxter A., Elmslie R.G., A comparison of multiple-choice tests and free-response tests in examinations of clinical competence, Medical Education, 13, 4, pp. 263-268, (1979); 
Norman G.R., Smith E.K.M., Powles A.C.P., Rooney P.J., Henry N.L., Dodd P.E., Factors underlying performance on written tests of knowledge, Medical Education, 21, 4, pp. 297-304, (1987); 
Norton D.W., A suggested method for estimating the proportion of a group selecting the correct response to a multiple-choice item by guessing, (1950); 
Ouyang W., Harik P., Clauser B.E., Paniagua M.A., An investigation of answer changes on the USMLE® Step 2 Clinical Knowledge examination, BMC Medical Education, 19, 1, (2019); 
Sam A.H., Field S.M., Collares C.F., van der Vleuten C.P., Wass V.J., Melville C., Harris J., Meeran K., Very-short-answer questions: Reliability, discrimination and acceptability, Medical Education, 52, 4, pp. 447-455, (2018); 
Sam A.H., Hameed S., Harris J., Meeran K., Validity of very short answer versus single best answer questions for undergraduate assessment, BMC Medical Education, 16, 1, pp. 1-4, (2016); 
Sam A.H., Westacott R., Gurnell M., Wilson R., Meeran K., Brown C., Comparing single-best-answer and very-short-answer questions for the assessment of applied medical knowledge in 20 UK medical schools: Cross-sectional study, British Medical Journal Open, 9, 9, (2019); 
Schuwirth L.W., Van Der Vleuten C.P., Different written assessment methods: What can be said about their strengths and weaknesses?, Medical Education, 38, 9, pp. 974-979, (2004); 
Schuwirth L.W.T., van der Vleuten C.P.M., Donkers H.H.L.M., A closer look at cueing effects in multiple choice questions, Medical Education, 30, pp. 44-49, (1996); 
Yaneva V., Ha L.A., Mee J., Zhou Y., Harik P., ACTA: Short-Answer Grading in High-Stakes Medical Exams. To appear, Proceedings of the 18Th Workshop on Innovative Use of NLP for Building Educational Applications, (2023); 
Swanson D.B., Holtzman K.Z., Allbee K., Clauser B.E., Psychometric characteristics and response times for content-parallel extended-matching and one-best-answer items in relation to number of options, Academic Medicine (RIME Supplement), 81, 10, pp. S52-S55, (2006); 
Wainer H., Thissen D., Combining multiple-choice and constructed-response test scores: Toward a Marxist theory of test construction, Applied Measurement in Education, 6, pp. 103-118, (1993); 
Yamamoto K., He Q., Shin H.J., von Davier M., Developing a machine-supported coding system for constructed-response items in PISA ETS RR–17–47, (2017); 
Yaneva V., von Davier M., Advancing natural language processing in educational assessment, NCME educational measurement and assessment book series, (2023)#FRF#
