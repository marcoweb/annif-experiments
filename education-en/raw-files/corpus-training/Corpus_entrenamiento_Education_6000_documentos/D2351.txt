#ITI#Detecting Cheating in Large-Scale Assessment: The Transfer of Detectors to New Tests#FTI#
#IRE# Recent approaches to the detection of cheaters in tests employ detectors from the field of machine learning. Detectors based on supervised learning algorithms achieve high accuracy but require labeled data sets with identified cheaters for training. Labeled data sets are usually not available at an early stage of the assessment period. In this article, we discuss the approach of adapting a detector that was trained previously with a labeled training data set to a new unlabeled data set. The training and the new data set may contain data from different tests. The adaptation of detectors to new data or tasks is denominated as transfer learning in the field of machine learning. We first discuss the conditions under which a detector of cheating can be transferred. We then investigate whether the conditions are met in a real data set. We finally evaluate the benefits of transferring a detector of cheating. We find that a transferred detector has higher accuracy than an unsupervised detector of cheating. A naive transfer that consists of a simple reuse of the detector increases the accuracy considerably. A transfer via a self-labeling (SETRED) algorithm increases the accuracy slightly more than the naive transfer. The findings suggest that the detection of cheating might be improved by using existing detectors of cheating at an early stage of an assessment period.#FRE#
#IPC# cheating; data mining; transfer learning#FPC#
#IRF# Bernardi R., Baca A., Landers K., Witek M., Methods of cheating and deterrents to classroom cheating: An international study, Ethics & Behavior, 18, pp. 373-391, (2008); 
Bezirhan U., von Davier M., Grabovsky I., Modeling item revisit behavior: The hierarchical speed-accuracy-revisits model, Educational and Psychological Measurement, 81, pp. 363-387, (2021); 
Biecek P., Szczurek E., Vingron M., Tiuryn J., The R package bgmm: Mixture modeling with uncertain knowledge, Journal of Statistical Software, 47, pp. 1-31, (2012); 
Bishop S., Egan K., Detecting erasures and unusual gain scores, Handbook of quantitative methods for detecting cheating on tests, pp. 193-213, (2017); 
Boughton K., Smith J., Ren H., Using response time data to detect compromised items and/or people, Handbook of quantitative methods for detecting cheating on tests, pp. 177-190, (2017); 
Bruzzone L., Marconcini M., Domain adaptation problems: A DASVM classification technique and a circular validation strategy, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32, pp. 770-787, (2010); 
Burlak G., Hernandez J., Ochoa A., Munoz J., The use of data mining to determine cheating in online student assessment, (2006); 
Chen M., Chen C., Detect exam cheating pattern by data mining, Fuzzy systems and data mining, pp. 25-34, (2017); 
Chuang C., Craig S., Femiani J., Detecting probable cheating during online assessments based on time delay and head pose, Higher Education Research & Development, 36, pp. 1123-1137, (2017); 
Cizek G., Wollack J., Exploring cheating on tests: The context, the concern, and the challenges, Handbook of quantitative methods for detecting cheating on tests, pp. 3-19, (2017); 
Cizek G., Wollack J., Handbook of quantitative methods for detecting cheating on tests, (2017); 
Cohen J., Statistical power analysis for the behavioral sciences, (1988); 
Diedenhof B., Musch J., PageFocus: Using paradata to detect and prevent cheating on online achievement tests, Behavior Research Methods, 49, pp. 1444-1459, (2017); 
Elkan C., The foundation of cost-sensitive learning, (2001); 
Fekken G., Holden R., Response latency evidence for viewing personality traits as schema indicators, Journal of Research in Personality, 26, pp. 103-120, (1992); 
Gonzalez M., Rosado-Falcon O., Rodriguez J., ssc: Semi-supervised classification methods, (2019); 
Gretton A., Borgwardt K., Rasch M., Scholkopf B., Smola A., A kernel two-sample test, Journal of Machine Learning Research, 13, pp. 723-773, (2012); 
Hastie T., Tibshirani R., Friedman J., The elements of statistical learning, (2009); 
Hediger S., Michel L., Naf J., hyporf: Random forest two-sample tests, (2021); 
Hediger S., Michel L., Naf J., On the use of random forest for two-sample testing, Computational Statistics and Data Analysis, 170, (2022); 
Hodge V., Austin J., A survey of outlier detection methodologies, Artificial Intelligence Review, 22, pp. 85-126, (2004); 
Holland P., Assessing unusual agreement between the incorrect answers of two examinees using the K-index: Statistical theory and empirical support, (1996); 
Huang J., Smola A., Gretton A., Borgwardt K., Scholkopf B., Correcting sample selection bias by unlabeled data, (2006); 
Kim D., Woo A., Dickison P., Identifying and investigating aberrant responses using psychometrics-based and machine learning-based approaches, Handbook of quantitative methods for detecting cheating on tests, pp. 71-97, (2017); 
Kingston N., Clark A., Test fraud: Statistical detection and methodology, (2014); 
Kisung Y., maotai: Tools for matrix algebra, optimization and inference, (2022); 
Kouw W., Loog M., An introduction to domain adaptation and transfer learning, arXiv e-prints, (2019); 
Krijthe J.H., Rssl: R package for semi-supervised learning, Reproducible research in pattern recognition, pp. 104-115, (2016); 
Kroehne U., Goldhammer F., How to conceptualize, represent, and analyze log data from technology-based assessments? A generic framework and an application to questionnaire items, Behaviometrika, 45, pp. 527-563, (2018); 
Li M., Zhou Z., Self-training with editing, Advances in knowledge discovery and data mining, pp. 611-621, (2005); 
Long M., Wang J., Ding G., Pan S., Yu P., Adaptation regularization: A general framework for transfer learning, IEEE Transactions on Knowledge and Data Engineering, 26, pp. 1076-1089, (2014); 
Man K., Harring J., Assessing preknowledge cheating via innovative measures: A multiple-group analysis of jointly modeling item responses, response times, and visual fixation counts, Educational and Psychological Measurement, 81, pp. 441-465, (2020); 
Man K., Harring J., Ouyang Y., Thomas S., Response time based nonparametric Kullback-Leibler divergence measure for detecting aberrant test-taking behavior, International Journal of Testing, 18, pp. 155-177, (2018); 
Man K., Harring J., Sinharay S., Use of data mining methods to detect test fraud, Journal of Educational Measurement, 56, pp. 251-279, (2019); 
Mandrekar J., Receiver operating characteristic curve in diagnostic test assessment, Journal of Thoracic Oncology, 5, pp. 1315-1316, (2010); 
Mansour Y., Mohri M., Rostamizadeh A., Domain adaptation with multiple sources, Advances in neural information processing systems, pp. 1-8, (2008); 
Mansour Y., Mohri M., Rostamizadeh A., Domain adaptation: Learning bounds and algorithms, (2009); 
Marianti S., Fox J.-P., Avetisyan M., Veldkamp M., Tijmstra J., Testing for aberrant behavior in response time modeling, Journal of Educational and Behavioral Statistics, 39, pp. 426-451, (2014); 
McCabe D., Cheating and honor: Lessons from a long-term research project, Handbook of academic integrity, pp. 187-198, (2016); 
Pan S., Tsang I., Kwok J., Yang Q., Domain adaptation via transfer component analysis, IEEE Transactions on Neural Networks, 22, pp. 199-210, (2011); 
Pan S., Yang Q., A survey on transfer learning, IEEE Transactions on Knowledge and Data Engineering, 22, pp. 1345-1359, (2010); 
Ponce H., Mayer R., Sitthiworachart J., Lopez M., Effects on response time and accuracy of technology-enhanced cloze tests: An eye-tracking study, Educational Technology Research and Development, 68, pp. 2033-2053, (2020); 
Quinonero Candela J., Sugiyama M., Schwaighofer A., Lawrence N., Dataset shift in machine learning, (2008); 
Ranger J., Schmidt N., Wolgast A., The detection of cheating on e—exams in higher education—The performance of several old and some new indicators, Frontiers in Psychology, 11, (2020); 
Ranger J., Schmidt N., Wolgast A., Detection of cheating on e—exams—The performance of transferred detection rules, Multimediales Lehren und Lernen an der Martin-Luther-Universität Halle-Wittenberg. Befunde und Ansätze aus dem Forschungsförderprogramm des Zentrums für multimediales Lehren und Lernen, pp. 131-146, (2021); 
Rettinger D., Kramer Y., Situational and personal causes of student cheating, Research in Higher Education, 50, pp. 293-313, (2009); 
Robin X., Turck N., Hainard A., Tiberti N., Lisacek J., Sanchez J., Muller M., PROC: An open-source package for R and S+ to analyze and compare ROC curves, BMC Bioinformatics, 12, (2011); 
Sato C., The construction and interpretation of s—p tables, (1975); 
Scrucca L., Fop M., Murphy T., Raftery A., Mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models, The R Journal, 8, pp. 289-317, (2016); 
Shao L., Zhu F., Li X., Transfer learning for visual categorization: A Survey, IEEE Transactions on Neural Networks and Learning Systems, 26, pp. 1019-1034, (2014); 
Shimodaira H., Improving predictive inference under covariate shift by weighting the log-likelihood function, Journal of Statistical Planning and Inference, 90, pp. 227-244, (2000); 
Sijtsma K., A coefficient of deviance of response patterns, Kwantitatieve Methoden, 7, pp. 131-145, (1986); 
Sinharay S., A new person-fit statistic for the lognormal model for response times, Journal of Educational Measurement, 55, pp. 457-476, (2018); 
Sotaridona L., Meijer R., Statistical properties of the K-index for detecting answer copying, Journal of Educational Measurement, 39, pp. 115-132, (2002); 
Tendeiro J., Meijer R., Niessen A., PerFit: An R package for person-fit analysis in IRT, Journal of Statistical Software, 74, pp. 1-27, (2016); 
Triguero I., Garcia S., Herrera F., Self-labeled techniques for semi-supervised learning: Taxonomy, software and empirical study, Knowledge and Information Systems, 42, pp. 245-284, (2015); 
van der Flier H., Deviant response patterns and comparability of test scores, Journal of Cross-cultural Psychology, 13, pp. 267-298, (1982); 
Wang J., Zamar R., Marazzi A., Yohai V., Salibian-Barrera M., Maronna R., Konis K., robust: Robust library, (2013); 
Weinstein M., When numbers are not enough—Collection and use of collateral evidence to assess the ethics and professionalism of examinees suspected of test fraud, Handbook of quantitative methods for detecting cheating on tests, pp. 3-19, (2017); 
Weiss K., Khoshgoftaar T., Wang D., A survey of transfer learning, Journal of Big Data, 3, (2016); 
Wollack J., Fremer J., Handbook of test security, (2013); 
Xu H., Mannor S., Robustness and generalization, Machine Learning, 86, pp. 391-423, (2012); 
Zadrozny B., Learning and evaluating classifiers under sample selection bias, (2004); 
Zhang K., Scholkopf B., Muandet K., Wang Z., Domain adaptation under target and conditional shift, (2013); 
Zhang L., Gao X., Transfer adaptation learning: A decade survey, arXiv e-prints, (2019); 
Zhou T., Jiao H., Exploration of the stacking ensemble machine learning algorithm for cheating detection in large-scale assessment, Educational and Psychological Measurement, (2022); 
Zimek A., Schubert E., Kriegel H., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining, 5, pp. 363-387, (2012); 
Zopluoglu C., CopyDetect: Computing response similarity indices for multiple-choice tests, (2018); 
Zopluoglu C., Detecting examinees with item preknowledge in large-scale testing using extreme gradient boosting (XGBoost), Educational and Psychological Measurement, 79, pp. 931-961, (2019)#FRF#
