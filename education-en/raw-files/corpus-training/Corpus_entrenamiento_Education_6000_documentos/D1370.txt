#ITI#A contextualized assessment of reliability and validity of student-initiated momentary self-reports during lectures#FTI#
#IRE# The use of Experience Sampling Methods (ESM) to assess students’ experiences, motivation, and emotions by sending signals to students at random or fixed time points has grown due to recent technological advances. Such methods offer several advantages, such as capturing the construct in the moment (i.e., when the events are fresh on respondents’ minds) or providing a better understanding of the temporal and dynamic nature of the construct, and are often considered to be more valid than retrospective self-reports. This article investigates the validity and reliability of a variant of the ESM, the DEBE (an acronym for difficult, easy, boring and engaging, and pronounced ‘Debbie’) feedback, which captures student-driven (as and when the student wants to report) momentary self-reports of cognitive-affective states during a lecture. The DEBE feedback is collected through four buttons on mobile phones/laptops used by students. We collected DEBE feedback from several video lectures (N = 722, 8 lectures) in different courses and examined the threats to validity and reliability. Our analysis revealed variables such as student motivation, learning strategies, academic performance, and prior knowledge did not affect the feedback-giving behavior. Monte Carlo simulations showed that for a class size of 50 to 120, on average, 30 students can provide representative and actionable feedback, and the feedback was tolerant up to 20% of the students giving erroneous or biased feedback. The article discusses in detail the aforementioned and other validity and reliability threats that need to be considered when working with such data. These findings, although specific to the DEBE feedback, are intended to supplement the momentary self-report literature, and the study is expected to provide a roadmap for establishing validity and reliability of such novel data types. © Association for Educational Communications and Technology 2023.#FRE#
#IPC# Experience sampling method; Lectures; Reliability; Self-report; Validity#FPC#
#IRF# Chauliac M., Catrysse L., Gijbels D., Donche V., It is all in the “surv-eye”: can eye tracking data shed light on the internal consistency in self-report questionnaires on cognitive processing strategies?, Frontline Learning Research, 8, 3, pp. 26-39, (2020); 
Chavan P., Mitra R., Developing a student feedback system using a design-based research approach, 2019 IEEE Tenth International Conference on Technology for Education (T4E), pp. 1-8, (2019); 
Chavan P., Mitra R., Tcherly: A teacher-facing dashboard for online video lectures, Journal of Learning Analytics, 9, (2022); 
Chavan P., Gupta S., Mitra R., A novel feedback system for pedagogy refinement in large lecture classrooms, International Conference on Computers in Education, pp. 464-469, (2018); 
Chavan P., Mitra R., Srree Murallidharan J., Multiscale nature of student and teacher perceptions of difficulty in a mechanical engineering lecture, European Journal of Engineering Education, (2022); 
Cross A., Bayyapunedi M., Cutrell E., Agarwal A., Thies W., TypeRighting: Combining the benefits of handwriting and typeface in online educational videos, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 793-796, (2013); 
Csikszentmihalyi M., Flow. The Psychology of Optimal Experience, New York (Harperperennial), (1990); 
D'Mello S., Olney A., Williams C., Hays P., Gaze tutor: A gaze-reactive intelligent tutoring system, International Journal of Human-Computer Studies, 70, 5, pp. 377-398, (2012); 
Durik A.M., Jenkins J.S., Variability in certainty of self-reported interest: Implications for theory and research, Frontline Learning Research, 8, 3, pp. 85-103, (2020); 
Frick T.W., Chadha R., Watson C., Wang Y., Green P., College student perceptions of teaching and learning quality, Educational Technology Research and Development, 57, 5, pp. 705-720, (2009); 
Fryer L.K., Dinsmore D.L., The promise and pitfalls of self-report: Development, research design and analysis issues, and multiple methods, Frontline Learning Research, 8, 3, pp. 1-9, (2020); 
Fuller K.A., Karunaratne N.S., Naidu S., Exintaris B., Short J.L., Wolcott M.D., Singleton S., White P.J., Development of a self-report instrument for measuring in-class student engagement reveals that pretending to engage is a significant unrecognized problem, PLoS ONE, 13, 10, (2018); 
Graesser A.C., D'Mello S., Emotions during the learning of difficult material, Psychology of Learning and Motivation, 57, pp. 183-225, (2012); 
Hektner J.M., Schmidt J.A., Csikszentmihalyi M., Experience sampling method: Measuring the quality of everyday life, (2007); 
Hilliger I., Miranda C., Schuit G., Duarte F., Anselmo M., Parra D., Evaluating a learning analytics dashboard to visualize student self-reports of time-on-task: A case study in a Latin American University, LAK21: 11Th International Learning Analytics and Knowledge Conference, pp. 592-598, (2021); 
Huang J.L., Curran P.G., Keeney J., Poposki E.M., DeShon R.P., Detecting and deterring insufficient effort responding to surveys, Journal of Business and Psychology, 27, 1, pp. 99-114, (2012); 
Iaconelli R., Wolters C.A., Insufficient effort responding in surveys assessing self-regulated learning: Nuisance or fatal flaw?, Frontline Learning Research, 8, 3, pp. 104-125, (2020); 
Kizilcec R.F., Papadopoulos K., Sritanyaratana L., Showing face in video instruction: Effects on information retention, visual attention, and affect, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 2095-2102, (2014); 
Kizilcec R.F., Bailenson J.N., Gomez C.J., The instructor’s face in video instruction: Evidence from two large-scale field studies, Journal of Educational Psychology, 107, 3, (2015); 
Kuncel N.R., Crede M., Thomas L.L., The validity of self-reported grade point averages, class ranks, and test scores: A meta-analysis and review of the literature, Review of Educational Research, 75, 1, pp. 63-82, (2005); 
Larson R., Csikszentmihalyi M., The experience sampling method, Flow and the foundations of positive psychology, pp. 21-34, (2014); 
Meade A.W., Craig S.B., Identifying careless responses in survey data, Psychological Methods, 17, 3, (2012); 
Messick S., Validity of psychological assessment: Validation of inferences from persons’ responses and performances as scientific inquiry into score meaning, American Psychologist, 50, 9, (1995); 
Mitra R., Chavan P., DEBE feedback for large lecture classroom analytics, Proceedings of the 9th International Conference on Learning Analytics & Knowledge, pp. 426-430, (2019); 
Moeller J., Viljaranta J., Kracke B., Dietrich J., Disentangling objective characteristics of learning situations from subjective perceptions thereof, using an experience sampling method design, Frontline Learning Research, 8, 3, pp. 63-84, (2020); 
Morris R.C., Parker L.C., Nelson D., Pistilli M.D., Hagen A., Levesque-Bristol C., Weaver G., Development of a student self-reported instrument to assess course reform, Educational Assessment, 19, 4, pp. 302-320, (2014); 
Pekrun R., The control-value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice, Educational Psychology Review, 18, 4, pp. 315-341, (2006); 
Pekruna R., Commentary: Self-report is indispensable to assess students’ learning, Frontline Learning Research, 8, 3, pp. 185-193, (2020); 
Pintrich P.R., Smith D.A.F., Garcia T., McKeachie W.J., A manual for the use of the Motivated Strategies for Learning Questionnaire (MSLQ), Ann Arbor: National Center for Research to Improve Postsecondary Teaching and Learning (NCRIPTAL), School of Education, (1991); 
Risko E.F., Foulsham T., Dawson S., Kingstone A., The collaborative lecture annotation system (CLAS): A new TOOL for distributed learning, IEEE Transactions on Learning Technologies, 6, 1, pp. 4-13, (2012); 
Rivera-Pelayo V., Munk J., Zacharias V., Braun S., Live interest meter: Learning from quantified feedback in mass lectures, In Proceedings of the Third International Conference on Learning Analytics and Knowledge, pp. 23-27, (2013); 
Shernoff D.J., Csikszentmihalyi M., Shneider B., Shernoff E.S., Student engagement in high school classrooms from the perspective of flow theory, School Psychology Quarterly, 18, 2, (2003); 
Singh S., Leveraging student self-reports to predict learning outcomes, International Conference on Artificial Intelligence in Education, pp. 398-403, (2019); 
Srivastava N., Velloso E., Lodge J.M., Erfani S., Bailey J., Continuous evaluation of video lectures from real-time difficulty self-report, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-12, (2019); 
Teo T., An initial development and validation of a Digital Natives Assessment Scale (DNAS), Computers & Education, 67, pp. 51-57, (2013); 
van Halema N., Van Klaveren C., Drachsler H., Schmitz M., Cornelisz I., Tracking patterns in self-regulated learning using students’ self-reports and online trace data, Frontline Learning Research, 8, 3, pp. 140-163, (2020); 
Van Meter P.N., Commentary: Measurement and the study of motivation and strategy use-determining if and when self-report measures are appropriate, Frontline Learning Research, 8, 3, pp. 174-184, (2020); 
Veenman M.V., Learning to self-monitor and self-regulate, Handbook of research on learning and instruction, pp. 249-273, (2016); 
Zarraonandia T., Diaz P., Montero A., Aedo I., Onorati T., Using a google glass-based classroom feedback system to improve students to teacher communication, IEEE Access, 7, pp. 16837-16846, (2019)#FRF#
