#ITI#Treatments of Differential Item Functioning: A Comparison of Four Methods#FTI#
#IRE# Test fairness is critical to the validity of group comparisons involving gender, ethnicities, culture, or treatment conditions. Detection of differential item functioning (DIF) is one component of efforts to ensure test fairness. The current study compared four treatments for items that have been identified as showing DIF: deleting, ignoring, multiple-group modeling, and modeling DIF as a secondary dimension. Results of this study provide indications about which approach could be applied for items showing DIF for a wide range of testing environments requiring reliable treatment.#FRE#
#IPC# item response theory; multidimensionality; treatment methods of differential item functioning#FPC#
#IRF# Ackerman T.A., A didactic explanation of item bias, item impact, and item validity from a multidimensional perspective, Journal of Educational Measurement, 29, 1, pp. 67-91, (1992); 
Ackerman T.A., Using multidimensional item response theory to understand what items and tests are measuring, Applied Measurement in Education, 7, 4, pp. 255-278, (1994); 
Cai L., flexMIRT version 3.51: Flexible multilevel multidimensional item analysis and test scoring [Computer software], (2017); 
Camilli G., A conceptual analysis of differential item functioning in terms of a multidimensional item response model, Applied Psychological Measurement, 16, 2, pp. 129-147, (1992); 
Camilli G., Test fairness, Educational Measurement, 4, pp. 221-256, (2006); 
Chiesi F., Ciancaleoni M., Galli S., Morsanyi K., Primi C., Item response theory analysis and differential item functioning across age, gender and country of a short form of the advanced progressive matrices, Learning and Individual Differences, 22, 3, pp. 390-396, (2012); 
Cho S.J., Suh Y., Lee W.Y., After differential item functioning is detected: IRT item calibration and scoring in the presence of DIF, Applied Psychological Measurement, 40, 8, pp. 573-591, (2016); 
Dorans N.J., Cook L.L., Fairness in educational assessment and measurement, (2016); 
Fidalgo A.M., Mellenbergh G.J., Muniz J., Effects of amount of DIF, test length, and purification type on robustness and power of Mantel-Haenszel procedures, Methods of Psychological Research Online, 5, 3, pp. 43-53, (2000); 
Finch H., Item parameter estimation for the MIRT model: Bias and precision of confirmatory factor analysisâ€“based models, Applied Psychological Measurement, 34, 1, pp. 10-26, (2010); 
Finch H.W., French B.F., Detection of crossing differential item functioning: A comparison of four methods, Educational and Psychological Measurement, 67, 4, pp. 565-582, (2007); 
Fleishman J.A., Spector W.D., Altman B.M., Impact of differential item functioning on age and gender differences in functional disability, 57, pp. S275-S284, (2002); 
Gierl M.J., Using dimensionality-based DIF analyses to identify and interpret constructs that elicit group differences, Educational Measurement, 24, 1, pp. 3-14, (2005); 
Gosz J.K., Walker C.M., An empirical comparison of multidimensional item response data using TESTFACT and NOHARM, (2002); 
Klockars A.J., Lee Y., Simulated tests of differential item functioning using SIBTEST with and without impact, Journal of Educational Measurement, 45, 3, pp. 271-285, (2008); 
Liaw Y.L., When can multidimensional item response theory (MIRT) models be a solution for differential item functioning (DIF)? A Monte Carlo simulation study, (2015); 
Millsap R.E., Kwok O.M., Evaluating the impact of partial factorial invariance on selection in two populations, Psychological Methods, 9, 1, pp. 93-115, (2004); 
Reckase M.D., Multidimensional item response theory models, Multidimensional item response theory, pp. 79-112, (2009); 
Roussos L., Stout W., A multidimensionality-based DIF analysis paradigm, Applied Psychological Measurement, 20, 4, pp. 355-371, (1996); 
Scott N.W., Fayers P.M., Aaronson N.K., Bottomley A., de Graeff A., Groenvold M., Koller M., Petersen M.A., Sprangers M.A., Differential item functioning (DIF) analyses of health-related quality of life instruments using logistic regression, Health and Quality of Life Outcomes, (2010); 
Shealy R., Stout W., An item response theory model for test bias, (1991); 
Shealy R., Stout W., A model-based standardization approach that separates true bias/DIF from group ability differences and detects test bias/DTF as well as item bias/DIF, Psychometrika, 58, 2, pp. 159-194, (1993); 
Steinmetz H., Analyzing observed composite differences across groups, Methodology, 9, 1, pp. 1-12, (2013); 
Swaminathan H., Rogers H.J., Detecting differential item functioning using logistic regression procedures, Journal of Educational measurement, 27, 4, pp. 361-370, (1990); 
Tate R., A comparison of selected empirical methods for assessing the structure of responses to test items, Applied Psychological Measurement, 27, 3, pp. 159-203, (2003); 
Walker C.M., Sahin G.S., Using a multidimensional IRT framework to better understand differential item functioning (DIF): A tale of three dif detection procedures, Educational and Psychological Measurement, 77, 6, pp. 945-970, (2017); 
Zumbo B.D., Does item-level DIF manifest itself in scale-level analyses? Implications for translating language tests, Language Testing, 20, 2, pp. 136-147, (2003)#FRF#
