#ITI#Practices and Theories: How Can Machine Learning Assist in Innovative Assessment Practices in Science Education#FTI#
#IRE# As cutting-edge technologies, such as machine learning (ML), are increasingly involved in science assessments, it is essential to conceptualize how assessment practices are innovated by technologies. To partially meet this need, this article focuses on ML-based science assessments and elaborates on how ML innovates assessment practices in science education. The article starts with an articulation of the “practice” nature of assessment both of learning and for learning, identifying four essential assessment practices: identifying learning goals, eliciting performance, interpreting observations, and decision-making and action-taking. I then extend a three-dimensional framework for innovative assessments, including construct, functionality, and automaticity, and based on which to conceptualize innovative assessments in three levels: substitute, transform, and redefine. Using the framework, I elaborate on how the 10 articles included in this special issue, Applying Machine Learning in Science Assessment: Opportunity and Challenge, advanced our knowledge of the innovations that ML brought to science assessment practices. I contend that the 10 articles exemplify a great deal of effort to transform the four components of assessment practices: ML allows assessments to target complex, diverse, and structural constructs, and thus better approaching the three-dimensional science learning goals of the Next Generation Science Standards (NGSS Lead States, 2013); ML extends the approaches used to eliciting performance and collecting evidence; ML provides a means to better interpreting observations and using evidence; ML supports immediate and complex decision-making and action-taking. I conclude this article by pushing the field to consider the underlying educational theories that are needed for innovative assessment practices and the necessities of establishing a “romance” between assessment practices and the relevant educational theories, which I contend are the prominent challenges to forward innovative and ML-based assessment practices in science education#FRE#
#IPC# Artificial intelligence; Innovative assessment; Machine learning; Science#FPC#
#IRF# Abd-El-Khalick F., Boujaoude S., Duschl R., Lederman N.G., Mamlok-Naaman R., Hofstein A., Inquiry in science education: international perspectives, Science Education, 88, 3, pp. 397-419, (2004); 
A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives, Longman, (2001); 
Beatty I.D., Gerace W.J., Technology-enhanced formative assessment: a research-based pedagogy for teaching science with classroom response technology, Journal of Science Education and Technology, 18, 2, pp. 146-162, (2009); 
Bennett R.E., Educational assessment: what to watch in a rapidly changing world, Educational Measurement: Issues and Practice, 37, 4, pp. 7-15, (2018); 
Bennett R.E., Deane P., van Rijn W.P., From cognitive-domain theory to assessment practice, Educational Psychologist, 51, 1, pp. 82-107, (2016); 
Bertolini R., Finch S.J., Nehm R.H., Testing the impact of novel assessment sources and machine learning methods on predictive outcome modeling in undergraduate biology, Journal of Science Education and Technology, (2021); 
Black P., Wiliam D., Assessment and classroom learning, Assessment in Education: Principles, Policy & Practice, 5, 1, pp. 7-74, (1998); 
Chang H.-Y., Quintana C., Krajcik J., Using drawing technology to assess students’ visualizations of chemical reaction processes, Journal of Science Education and Technology, 23, 3, pp. 355-369, (2013); 
Chi M.T., Feltovich P.J., Glaser R., Categorization and representation of physics problems by experts and novices, Cognitive Science, 5, 2, pp. 121-152, (1981); 
Clark R.E., Reconsidering research on learning from media, Review of Educational Research, 53, 4, pp. 445-459, (1983); 
Clement J., Model based learning as a key research area for science education, International Journal of Science Education, 22, 9, pp. 1041-1053, (2000); 
Cronbach L.J., Meehl P.E., Construct validity in psychological tests, Psychological Bulletin, 52, 4, pp. 1-28, (1955); 
Darling-Hammond L., Next generation assessment: Moving beyond the bubble test to support 21st century learning, John Wiley & Sons, (2014); 
DeBoer G.E., Quellmalz E.S., Davenport J.L., Timms M.J., Herrmann-Abell C.F., Buckley B.C., Comparing three online testing modalities: using static, active, and interactive online testing modalities to assess middle school students’ understanding of fundamental ideas and use of inquiry skills related to ecosystems, Journal of Research in Science Teaching, 51, 4, pp. 523-554, (2014); 
Duschl R., Science education in three-part harmony: balancing conceptual, epistemic, and social learning goals, Review of Research in Education, 32, 1, pp. 268-291, (2008); 
Ferrer-Torregrosa J., Torralba J., Jimenez M.A., Garcia S., Barcia J.M., ARBOOK: development and assessment of a tool based on augmented reality for anatomy, Journal of Science Education and Technology, 24, 1, pp. 119-124, (2015); 
Frezzo D.C., Behrens J.T., Mislevy R.J., Design patterns for learning and assessment: facilitating the introduction of a complex simulation-based learning environment into a community of instructors, Journal of Science Education and Technology, 19, 2, pp. 105-114, (2010); 
Gale J., Wind S., Koval J., Dagosta J., Ryan M., Usselman M., Simulation-based performance assessment: an innovative approach to exploring understanding of physical science concepts, International Journal of Science Education, 38, 14, pp. 2284-2302, (2016); 
Gobert J.D., Pallant A., Fostering students' epistemologies of models via authentic model-based tasks, Journal of Science Education and Technology, 13, 1, pp. 7-22, (2004); 
Harris C.J., Krajcik J.S., Pellegrino J.W., DeBarger A.H., Designing knowledge-in-use assessments to promote deeper learning, Educational Measurement: Issues and Practice, 38, 2, pp. 53-67, (2019); 
Hickey D.T., Taasoobshirazi G., Cross D., Assessment as learning: enhancing discourse, understanding, and achievement in innovative science curricula, Journal of Research in Science Teaching, 49, 10, pp. 1240-1270, (2012); 
Jescovitch L.N., Scott E.E., Cerchiara J.A., Merrill J., Urban-Lurain M., Doherty J.H., Haudek K.C., Comparison of machine learning performance using analytic and holistic coding approaches across constructed response assessments aligned to a science learning progression, Journal of Science Education and Technology, pp. 1-18, (2020); 
Kane M., Validating the interpretations and uses of test scores, Journal of Educational Measurement, 50, 1, pp. 1-73, (2013); 
Kelly G.J., McDonald S., Wickman P.O., Science learning and epistemology, Second International Handbook of Science Education, pp. 281-291, (2012); 
Kloser M., Borko H., Martinez J.F., Stecher B., Luskin R., Evidence of middle school science assessment practice from classroom-based portfolios, Science Education, 101, 2, pp. 209-231, (2017); 
Krajcik J.S., Mun K., Promises and challenges of using learning technologies to promote student learning of science, Handbook of Research on Science Education, 2, pp. 337-360, (2014); 
Lamb R., Hand B., Kavner A., Computational modeling of the effects of the science writing heuristic on student critical thinking in science using machine learning, Journal of Science Education and Technology, pp. 1-15, (2020); 
Lee H.S., Gweon G.H., Lord T., Paessel N., Pallant A., Pryputniewicz S., Machine learning-enabled automated feedback: supporting students’ revision of scientific arguments based on data drawn from simulation, Journal of Science Education and Technology, (2021); 
Liaw H., Yu Y.R., Chou C.C., Chiu M.H., Relationships between facial expressions, prior knowledge, and multiple representations: A case of conceptual change for kinematics instruction, Journal of Science Education and Technology, pp. 1-12, (2020); 
Liu O.L., Lee H.S., Linn M.C., Measuring knowledge integration: validation of four-year assessments, Journal of Research in Science Teaching, 48, 9, pp. 1079-1107, (2011); 
Maestrales S.Y., Zhai X., Touitou I., Schneider B., Krajcik J., Using machine learning to evaluate multidimensional assessments of chemistry and physics, Journal of Science Education and Technology, (2021); 
Magnusson S., Krajcik J., Borko H., Nature, sources, and development of pedagogical content knowledge for science teaching, Examining Pedagogical Content Knowledge, pp. 95-132, (1999); 
McMahon D., Wright R., Cihak D.F., Moore T.C., Lamb R., Podcasts on mobile devices as a read-aloud testing accommodation in middle school science assessment, Journal of Science Education and Technology, 25, 2, pp. 263-273, (2016); 
Mercer-Mapstone L., Kuchel L., Teaching scientists to communicate: evidence-based assessment for undergraduate science education, International Journal of Science Education, 37, 10, pp. 1613-1638, (2015); 
Messick S., The interplay of evidence and consequences in the validation of performance assessments, Educational researcher, 23, 2, pp. 13-23, (1994); 
Mislevy R., Haertel G., Implications of evidence-centered design for educational testing, Educational measurement: issues and practice, 25, 4, pp. 6-20, (2006); 
Mislevy R.J., How developments in psychology and technology challenge validity argumentation, Journal of Educational Measurement, 53, 3, pp. 265-292, (2016); 
A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas, (2012); 
Nehm R.H., Ha M., Mayfield E., Transforming biology assessment with machine learning: automated scoring of written evolutionary explanations, Journal of Science Education and Technology, 21, 1, pp. 183-196, (2012); 
Neumann K., Waight N., The digitalization of science education: Déjà vu all over again?, Journal of Research in Science Teaching, 57, 9, pp. 1519-1528, (2020); 
Lead States N.G.S.S., Next generation science standards: For states, by states, National Academies Press, (2013); 
Criteria for procuring and evaluating high-quality and aligned summative science assessments, NGSS Lead States, (2018); 
Nicolaidou I., Kyza E.A., Terzian F., Hadjichambis A., Kafouris D., A framework for scaffolding students' assessment of the credibility of evidence, Journal of Research in Science Teaching, 48, 7, pp. 711-744, (2011); 
Osborne J., Arguing to learn in science: the role of collaborative, critical, Science, 463, (2010); 
Osborne J.F., Henderson J.B., MacPherson A., Szu E., Wild A., Yao S.Y., The development and validation of a learning progression for argumentation in science, Journal of Research in Science Teaching, 53, 6, pp. 821-846, (2016); 
Pellegrino J.W., Proficiency in science: assessment challenges and opportunities, Science, 340, 6130, pp. 320-323, (2013); 
Pellegrino J.W., Sciences of learning and development: Some thoughts from the learning sciences, Applied Developmental Science, pp. 1-9, (2018); 
Pellegrino J.W., Chudowsky N., Glaser R., Knowing what students know: The science and design of educational assessment, ERIC, (2001); 
Pellegrino J.W., Wilson M.R., Koenig J.A., Beatty A.S., Developing assessments for the Next Generation Science Standards, ERIC, (2014); 
Penfield R.D., Lee O., Test-based accountability: potential benefits and pitfalls of science assessment with student diversity, Journal of Research in Science Teaching, 47, 1, pp. 6-24, (2010); 
Rosenberg J.M., Krist C., Combining machine learning and qualitative methods to elaborate students’ ideas about the generality of their model-based explanations, Journal of Science Education and Technology, pp. 1-13, (2020); 
Ruiz-Primo M.A., Furtak E.M., Exploring teachers' informal formative assessment practices and students' understanding in the context of scientific inquiry, Journal of Research in Science Teaching, 44, 1, pp. 57-84, (2007); 
Schwarz C.V., Reiser B.J., Davis E.A., Kenyon L., Acher A., Fortus D., Developing a learning progression for scientific modeling: making scientific modeling accessible and meaningful for learners, Journal of Research in Science Teaching, 46, 6, pp. 632-654, (2009); 
Shavelson R., Fu A., Kurpius A., Wiley E., Evidence- based practice in science education, In Encyclopedia of Science Education, pp. 1-4, (2013); 
Shavelson R.J., Young D.B., Ayala C.C., Brandon P.R., Furtak E.M., Ruiz-Primo M.A., Yin Y., On the impact of curriculum-embedded formative assessment on learning: a collaboration between curriculum and assessment developers, Applied measurement in education, 21, 4, pp. 295-314, (2008); 
Shepard L.A., Penuel W.R., Pellegrino J.W., Using learning and motivation theories to coherently link formative assessment, grading practices, and large-scale assessment, Educational measurement: issues and practice, 37, 1, pp. 21-34, (2018); 
Sung S.H., Li C., Chen G., Huang X., Xie C., Massicotte J., Shen J., How does augmented observation facilitate multimodal representational thinking? Applying deep learning to decode complex student construct, Journal of Science Education and Technology, pp. 1-17, (2020); 
Wang C., Liu X., Wang L., Sun Y., Zhang H., Automated scoring of Chinese grades 7–9 students’ competence in interpreting and arguing from evidence, Journal of Science Education and Technology, pp. 1-14, (2020); 
Wiliam D., What is assessment for learning?, Studies in Educational Evaluation, 37, 1, pp. 3-14, (2011); 
Wilson M., Constructing measures, An Item Response Modeling Approach, (2005); 
Yoo J., Kim J., Can Online Discussion Participation Predict Group Project Performance? Investigating the Roles of Linguistic Features and Participation Patterns, International Journal of Artificial Intelligence in Education, 24, 1, pp. 8-32, (2014); 
Zhai X., Applying machine learning in science assessment: Opportunity and challenges, A Call for a Special Issue in Journal of Science Education and Technology, (2019); 
Zhai X., Advancing automatic guidance in virtual science inquiry: from ease of use to personalization, Educational Technology Research and Development, (2021); 
Zhai X., Haudek K.C., Shi L., Nehm R., Urban-Lurain M., From substitution to redefinition: a framework of machine learning-based science assessment, Journal of Research in Science Teaching, 57, 9, pp. 1430-1459, (2020); 
Zhai X., Haudek K.C., Stuhlsatz M.A., Wilson C., Evaluation of construct-irrelevant variance yielded by machine and human scoring of a science teacher PCK constructed response assessment, Studies in Educational Evaluation, 67, (2020); 
Zhai X., Krajcik J., Pellegrino J., On the validity of machine learning-based Next Generation Science Assessments: A validity inferential network, Journal of Science Education and Technology, (2020); 
Zhai X., Li M., Guo Y., Teachers’ use of learning progression-based formative assessment to inform teachers’ instructional adjustment: a case study of two physics teachers’ instruction, International Journal of Science Education, 40, 15, pp. 1832-1856, (2018); 
Zhai X., Shi L., Nehm R., A meta-analysis of machine learning-based science assessments: factors impacting machine-human score agreements, Journal of Science Education and Technology, (2020); 
Zhai X., Yin Y., Pellegrino J.W., Haudek K.C., Shi L., Applying machine learning in science assessment: A systematic review, Studies in Science Education, 56, 1, pp. 111-151, (2020); 
Zhang M., Google photos Tags Two African-Americans As Gorillas Through Facial Recognition Software. Retrieved on January 3, 2021 From, (2015)#FRF#
