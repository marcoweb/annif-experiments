#ITI#Modeling of Item Response Functions Under the D-Scoring Method#FTI#
#IRE# This study presents new models for item response functions (IRFs) in the framework of the D-scoring method (DSM) that is gaining attention in the field of educational and psychological measurement and largescale assessments. In a previous work on DSM, the IRFs of binary items were estimated using a logistic regression model (LRM). However, the LRM underestimates the item true scores at the top end of the D-scale (ranging from 0 to 1), especially for relatively difficult items. This entails underestimation of true D-scores, inaccuracy in the estimates of their standard errors, and other psychometric issues. The inverse-regression adjustments used to fix this problem are too complicated for regular applications of the DSM and not in line with its simplicity. This issue is resolved with the IRF models proposed in this study, referred to as rational function models (RFMs) with one parameter (RFM1), two parameters (RFM2), and three parameters (RFM3). The proposed RFMs are discussed and illustrated with simulated and real data.#FRE#
#IPC# D-scoring method; item response function; scaling; true scores#FPC#
#IRF# Al-Mashary F., Dimitrov D.M., Multistage testing application at the National Center for Assessment in Saudi Arabia, (2018); 
Atanasov D.V., Dimitrov D.M., DELTA: A computer program for test scoring, equating, and item analysis under the D-scoring method, (2018); 
Atanasov D.V., Dimitrov D.M., SATSE-D: A system for automated test scoring and equating under the D-scoring method: User’s guide, (2019); 
Atanasov D.V., Dimitrov D.M., SATA-D: A system for automated test assembly, User’s guide, (2019); 
Baker F.B., Kim S.-H., Item response theory: Parameter estimation techniques, (2004); 
Birnbaum A., Some latent trait models and their use in inferring an examinee’s ability, Statistical theories of mental test scores, pp. 397-479, (1968); 
Dimitrov D.M., An approach to scoring and equating tests with binary items: Piloting with large-scale assessments, Educational and Psychological Measurement, 76, pp. 954-975, (2016); 
Dimitrov D.M., The delta scoring method of tests with binary items: A note on true score estimation and equating, Educational and Psychological Measurement, 78, pp. 805-825, (2017); 
Dimitrov D.M., Luo U., Testing for item fit under the D-scoring model, (2017); 
Dimitrov D.M., Luo U., Robustness of the D-scoring model to violation of IRT assumptions, (2017); 
Dimitrov D.M., Luo U., A note on the D-scoring method adapted for polytomous test items, Educational and Psychological Measurement, 79, 3, (2018); 
Domingue B.W., Dimitrov D.M., A comparison of IRT theta estimates and delta scores from the perspective of additive conjoint measurement, (2015); 
Efron B., Bootstrap methods: Another look at the jackknife, Annals of Statistics, 7, pp. 1-26, (1979); 
Fox J.P., Bayesian item response modeling: Theory and applications, (2010); 
Hambleton R.K., Swaminathan H., Rogers H.J., Fundamentals of item response theory, (1991); 
Han K.T., Fixing the c parameter in the three-parameter logistic model, Practical Assessment, Research & Evaluation, 17, 1, pp. 1-24, (2012); 
Han K.T., Dimitrov D.M., Al-Mashary F., Implementing multistage tests using D-Scoring Method, (2018); 
Kelley T., Ebel R., Linacre J.M., Item discrimination indices, Rasch Measurement Transactions, 16, 3, pp. 883-884, (2002); 
Samejima F., Estimation of latent ability using a response pattern of graded scores, (1969); 
Samejima F., The graded response model, Handbook of modern item response theory, pp. 85-100, (1996); 
Smith R.M., Wind S.A., Rasch measurement models: Interpreting WINSTEPS and FACETS Outputs, (2018)#FRF#
