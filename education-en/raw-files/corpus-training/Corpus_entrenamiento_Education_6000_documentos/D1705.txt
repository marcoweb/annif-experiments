#ITI#The use of annotations to explain labels: Comparing results from a human-rater approach to a deep learning approach#FTI#
#IRE# Background: Deep learning methods, where models do not use explicit features and instead rely on implicit features estimated during model training, suffer from an explainability problem. In text classification, saliency maps that reflect the importance of words in prediction are one approach toward explainability. However, little is known about whether the salient words agree with those identified by humans as important. Objectives: The current study examines in-line annotations from human annotators and saliency map annotations from a deep learning model (ELECTRA transformer) to understand how well both humans and machines provide evidence for their assigned label. Methods: Data were responses to test items across a mix of United States subjects, states, and grades. Humans were trained to annotate responses to justify a crisis alert label and two model interpretability methods (LIME, Integrated Gradients) were used to obtain engine annotations. Human inter-annotator agreement and engine agreement with the human annotators were computed and compared. Results and Conclusions: Human annotators agreed with one another at similar rates to those observed in the literature on similar tasks. The annotations derived using the integrated gradients (IG) agreed with human annotators at higher rates than LIME on most metrics; however, both methods underperformed relative to the human annotators. Implications: Saliency map-based engine annotations show promise as a form of explanation, but do not reach human annotation agreement levels. Future work should examine the appropriate unit for annotation (e.g., word, sentence), other gradient based methods, and approaches for mapping the continuous saliency values to Boolean annotations#FRE#
#IPC# artificial intelligence; automated scoring; crisis alerts; engine annotation; explainability; human annotation#FPC#
#IRF# Alvarez-Mells D., Jaakkola T.S., On the robustness of interpretability methods, arXiv, (2018); 
Standards for educational and psychological testing, (2014); 
Arstein R., Poesio M., Inter-coder agreement for computational linguistics, Computational Linguistics, 34, 4, pp. 555-596, (2008); 
Baehrens D., Schroeter T., Harmeling S., Kawanabe M., Hansen K., MAzller K.-R., How to explain individual classification decisions, Journal of Machine Learning Research, 11, pp. 1802-1831, (2010); 
Bayerl P.S., Paul K.I., What determines inter-coder agreement on manual annotations? A meta-analytic investigation, Computational Linguistics, 37, 4, pp. 699-725, (2011); 
Burkhardt A., Lottridge S., Woolf S., A rubric for the detection of students in crisis, Educational Measurement: Issues and Practice, 40, 2, pp. 72-80, (2021); 
Carlile W., Gurrapdi N., Ke Z., Ng V., Give me more feedback: Annotating argument persuasiveness and related attributes in student essays. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 621–631), (2018); 
Clark K., Luong M.-T., Le Q., Manning C., ELECTRA: Pre-training text encoders as discriminators rather than generators. Paper Presented at the Seventh International Conference on Learning Representations. arXiv, (2020); 
Danilevsky M., Qian K., Aharonov R., Katsis Y., Kawa B., Prithviraj S., A survey of the state of explainable AI for natural language processing. Proceedings of the 1<sup>st</sup> conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10<sup>th</sup> International Joint Conference on Natural Language Processing (pp. 447–450), (2020); 
Devlin J., Chang M.-W., Lee K., Toutanova K., BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv, (2018); 
DiCerbo K., Assessment for learning with diverse learners in a digital world: Toward assessment in the service of learning, Educational Measurement: Issues and Practice, 39, 3, pp. 90-93, (2020); 
Ghosh D., Klebanov B., Song Y., An exploratory study of argumentative writing by young students: A transformer-based approach,  Workshop on Innovative Use of NLP for Building Educational Applications, pp. 145-150, (2020); 
Gordon E., Toward assessment in the service of learning, Educational Measurement: Issues and Practice, 39, 3, pp. 72-78, (2020); 
Hellman S., Murray W.R., Wiemerslage A., Rosenstein M., Foltz P.W., Becker L., Derr M., Multiple instance learning for content feedback localization without annotation,  Workshop on Innovative Use of NLP for Building Educational Applications, pp. 30-40, (2020); 
Jain S., Wallace B.C., Attention is not explanation. arXiv, (2019); 
Jo Y., Mayfield E., Reed C., Hovy E., Machine-aided annotation for fine-gained proposition types, Proceedings of the 12th Conference on Language Resources and Evaluation, pp. 1008-1018, (2020); 
Ke Z., Inamdar H., Lin H., Ng V., Give me more feedback II: Annotating thesis strength and related attributes in student essays,  Annual Meeting of the Association for Computational Linguistics, pp. 3994-4004, (2019); 
Klie J.-C., Bugert M., Boullosa B., Eckart de Castilho R., Gurevych I., The INCEpTION platform: Machine-assisted and knowledge-oriented interactive annotation, Proceedings of System Demonstrations of the 27th International Conference on Computational Linguistics: System Demonstrations, pp. 5-9, (2018); 
Krippendorff K., Content analysis: An introduction to its methodology, (2019); 
Lundberg S., Lee S.-I., A unified approach to interpreting model predictions,  Conference on Neural Information Processing Systems, pp. 4765-4774, (2017); 
Madnani N., Heilman M., Tetreault J., Chodorow M., Identifying high-level organizational elements in argumentative discourse, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language Technologies, pp. 20-28, (2012); 
Mathet Y., Widlocher A., Metivier J.-P., The unified and holistic method gamma (γ) for inter-annotator agreement measure and alignment, Computational Linguistics, 41, 3, pp. 437-478, (2015); 
Mayfield E., Black A., Should you fine-tune BERT for automated essay scoring?, Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pp. 151-162, (2020); 
Mochales-Palau R., Moens M.-F., Argumentation mining: The detection, classification and structure of arguments in text, Proceedings of the 12th International Conference on Artificial Intelligence and Law, pp. 98-107, (2009); 
Ormerod C.M., Harris A.E., Neural network approach to classifying alarming student responses to online assessment. arXiv, (2018); 
Pestian J.P., Matykiewicz P., Linn-Gust M., Sourth B., Uzuner O., Wiebe J., Cohen B., Hurdle J., Brew C., Sentiment analysis of suicide notes: A shared task, Biomedical Informatics Insights, 5, pp. 3-16, (2012); 
Pruthi D., Gupta M., Dhingra B., Neubig G., Lipton Z.C., Learning to deceive with attention-based explanations. arXiv, (2020); 
Ribeiro M.T., Sigh S., Guestrin C., “Why should I trust you?” Explaining the predictions of any classifier. arXiv, (2016); 
Rodriguez P., Jafari A., Ormerod C., Language models and automated essay scoring. arXiv, (2019); 
Serrano S., Smith N.A., Is attention interpretable? Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2931-2951, (2019); 
Simonyan K., Vedaldi A., Zisserman A., Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv, (2013); 
Smilkov D., Thorat N., Kim B., Viegas F., Wattenberg M., SmoothGrad: Removing noise by adding noise. arXiv, (2017); 
Song Y., Heilman M., Klebanov B.B., Dean P., Applying argumentation schemes for essay scoring, Proceedings of the First Workshop on Argumentation Mining, pp. 69-78, (2014); 
Stab C., Gurevych I., Annotating argument components and relations in persuasive essays, Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pp. 1501-1510, (2014); 
Stab C., Gurevych I., Parsing argumentation structures in persuasive essays, Computational Linguistics, 43, 3, pp. 619-660, (2017); 
Sundararajan M., Taly A., Yan Q., Axiomatic attribution for deep networks, Proceedings of the 34th International Conference on Machine Learning, (2017); 
Taghipour K., Ng H.-T., A neural approach to automated essay scoring, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1882-1891, (2016); 
Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A., Kaiser L., Polosukhin I., Attention is all you need. arXiv, (2017); 
Wang A., Singh A., Michael J., Hill F., Levey O., Bowman S., GLUE: A multi-task benchmark and analysis platform for natural language understanding. Paper Presented at the International Conference on Learning Representations. arXiv, (2019); 
Williamson D., Xi X., Breyer J., A framework for the evaluation and use of automated scoring, Educational Measurement: Issues and Practice, 31, 1, pp. 2-13, (2012)#FRF#
