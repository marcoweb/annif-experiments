#ITI#Towards a more nuanced conceptualisation of differential examiner stringency in OSCEs#FTI#
#IRE# Quantitative measures of systematic differences in OSCE scoring across examiners (often termed examiner stringency) can threaten the validity of examination outcomes. Such effects are usually conceptualised and operationalised based solely on checklist/domain scores in a station, and global grades are not often used in this type of analysis. In this work, a large candidate-level exam dataset is analysed to develop a more sophisticated understanding of examiner stringency. Station scores are modelled based on global grades—with each candidate, station and examiner allowed to vary in their ability/stringency/difficulty in the modelling. In addition, examiners are also allowed to vary in how they discriminate across grades—to our knowledge, this is the first time this has been investigated. Results show that examiners contribute strongly to variance in scoring in two distinct ways—via the traditional conception of score stringency (34% of score variance), but also in how they discriminate in scoring across grades (7%). As one might expect, candidate and station account only for a small amount of score variance at the station-level once candidate grades are accounted for (3% and 2% respectively) with the remainder being residual (54%). Investigation of impacts on station-level candidate pass/fail decisions suggest that examiner differential stringency effects combine to give false positive (candidates passing in error) and false negative (failing in error) rates in stations of around 5% each but at the exam-level this reduces to 0.4% and 3.3% respectively. This work adds to our understanding of examiner behaviour by demonstrating that examiners can vary in qualitatively different ways in their judgments. For institutions, it emphasises the key message that it is important to sample widely from the examiner pool via sufficient stations to ensure OSCE-level decisions are sufficiently defensible. It also suggests that examiner training should include discussion of global grading, and the combined effect of scoring and grading on candidate outcomes.#FRE#
#IPC# Borderline regression; Examiner stringency; OSCE; Standard setting#FPC#
#IRF# Bartman I., Smee S., Roy M., A method for identifying extreme OSCE examiners, The Clinical Teacher, 10, 1, pp. 27-31, (2013); 
Bates D., Machler M., Bolker B., Walker S., Fitting linear mixed-effects models using lme4, Journal of Statistical Software, 67, 1, pp. 1-48, (2015); 
Bell A., Fairbrother M., Jones K., Fixed and random effects models: Making an informed choice, Quality & Quantity, 53, 2, pp. 1051-1074, (2019); 
Cohen J., Statistical power analysis for the behavioral sciences, (1988); 
Cook D.A., Brydges R., Ginsburg S., Hatala R., A contemporary approach to validity arguments: A practical guide to Kane’s framework, Medical Education, 49, 6, pp. 560-575, (2015); 
Crowson M., Multilevel modeling in R using lme4 package (Video), Youtube, (2020); 
What is the PLAB 2 Exam? What is the PLAB 2 Exam?, (2020); 
PLAB (Professional and Linguistic Assessments Board), Professional and Linguistic Assessments Board, (2020); 
PLAB reports, (2022); 
Harasym P., Woloschuk W., Cunning L., Undesired variance due to examiner stringency/leniency effect in communication skill scores assessed in OSCEs, Advances in Health Sciences Education: Theory and Practice, (2008); 
Hatala R., Cook D.A., Brydges R., Hawkins R., Advances in Health Sciences Education: Theory and Practice, Constructing A Validity Argument for the Objective Structured Assessment of Technical Skills (OSATS): A Systematic Review of Validity Evidence, (2015); 
Hays R., Gupta T.S., Veitch J., The practical value of the standard error of measurement in borderline pass/fail decisions, Medical Education, 42, 8, pp. 810-815, (2008); 
Hodges B., Assessment in the post-psychometric era: Learning to love the subjective and collective, Medical Teacher, 35, 7, pp. 564-568, (2013); 
Homer M., Re-conceptualising and accounting for examiner (cut-score) stringency in a ‘high frequency, small cohort’ performance test, Advances in Health Sciences Education, (2020); 
Homer M., Pass/fail decisions and standards: The impact of differential examiner stringency on OSCE outcomes, Advances in Health Sciences Education, (2022); 
Homer M., Setting defensible minimum-stations-passed standards in OSCE-type assessments, Medical Teacher, (2023); 
IBM SPSS Statistics for Windows, Version 28.0, IBM Corp, (2021); 
Ilgen J.S., Ma I.W.Y., Hatala R., Cook D.A., A systematic review of validity evidence for checklists versus global rating scales in simulation-based assessment, Medical Education, 49, 2, pp. 161-173, (2015); 
Khan K.Z., Gaunt K., Ramachandran S., Pushkar P., The Objective Structured Clinical Examination (OSCE): AMEE Guide No. 81. Part II: organisation & administration, Medical Teacher, 35, 9, pp. e1447-e1463, (2013); 
Kramer A., Muijtjens A., Jansen K., Dusman H., Tan L., van der Vleuten C., Comparison of a rational and an empirical standard setting procedure for an OSCE, Objective Structured Clinical Examinations. Medical Education, 37, 2, pp. 132-139, (2003); 
Livingston S.A., Lewis C., Estimating the consistency and accuracy of classifications based on test scores, Journal of Educational Measurement, 32, 2, pp. 179-197, (1995); 
McKinley D.W., Norcini J.J., How to set standards on performance-based examinations: AMEE Guide No. 85, Medical Teacher, 36, 2, pp. 97-110, (2014); 
McManus I., Thompson M., Mollon J., Assessment of examiner leniency and stringency ('hawk-dove effect’) in the MRCP(UK) clinical examination (PACES) using multi-facet Rasch modelling, BMC Medical Education, 6, 1, (2006); 
Montgomery D.C., Peck E.A., Vining G.G., Introduction to linear regression analysis, (2012); 
Morris T.P., White I.R., Crowther M.J., Using simulation studies to evaluate statistical methods, Statistics in Medicine, 38, 11, pp. 2074-2102, (2019); 
Nimon K., Statistical assumptions of substantive analyses across the general linear model: A mini-review, Frontiers in Psychology, (2012); 
Norman G., Bordage G., Page G., Keane D., How specific is case specificity?, Medical Education, 40, 7, pp. 618-623, (2006); 
Osterlind S.J., Everson H.T., Differential item functioning, (2009); 
Pearce J., In defence of constructivist, utility-driven psychometrics for the ‘post-psychometric era’, Medical Education, 54, 2, pp. 99-102, (2020); 
Pell G., Fuller R., Homer M., Roberts T., How to measure the quality of the OSCE: A review of metrics—AMEE guide no. 49, Medical Teacher, 32, 10, pp. 802-811, (2010); 
Schauber S.K., Hecht M., Nouns Z.M., Why assessment in medical education needs a solid foundation in modern test theory, Advances in Health Sciences Education: Theory and Practice, 23, 1, pp. 217-232, (2018); 
Thompson B., Effect sizes, confidence intervals, and confidence intervals for effect sizes, Psychology in the Schools, 44, 5, pp. 423-432, (2007); 
Valentine N., Durning S.J., Shanahan E.M., van der Vleuten C., Schuwirth L., The pursuit of fairness in assessment: Looking beyond the objective, Medical Teacher, (2022); 
Wong W.Y.A., Thistlethwaite J., Moni K., Roberts C., Using cultural historical activity theory to reflect on the sociocultural complexities in OSCE examiners’ judgements, Advances in Health Sciences Education, 28, 1, pp. 27-46, (2023); 
Yeates P., Cope N., Hawarden A., Bradshaw H., McCray G., Homer M., Developing a video-based method to compare and adjust examiner effects in fully nested OSCEs, Medical Education, (2018); 
Yeates P., Moult A., Cope N., McCray G., Xilas E., Lovelock T., Vaughan N., Daw D., Fuller R., McKinley R.K., Measuring the effect of examiner variability in a multiple-circuit Objective Structured Clinical Examination (OSCE), Academic Medicine, (2021); 
Yeates P., Moult A., Lefroy J., Walsh-House J., Clews L., McKinley R., Fuller R., Understanding and developing procedures for video-based assessment in medical education, Medical Teacher, 42, 11, pp. 1250-1260, (2020); 
Yeates P., O'Neill P., Mann K., Eva K., Seeing the same thing differently, Advances in Health Sciences Education, 18, 3, pp. 325-341, (2013)#FRF#
