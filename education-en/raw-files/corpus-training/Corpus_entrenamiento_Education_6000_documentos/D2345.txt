#ITI#On Bank Assembly and Block Selection in Multidimensional Forced-Choice Adaptive Assessments#FTI#
#IRE# Multidimensional forced-choice (FC) questionnaires have been consistently found to reduce the effects of socially desirable responding and faking in noncognitive assessments. Although FC has been considered problematic for providing ipsative scores under the classical test theory, item response theory (IRT) models enable the estimation of nonipsative scores from FC responses. However, while some authors indicate that blocks composed of opposite-keyed items are necessary to retrieve normative scores, others suggest that these blocks may be less robust to faking, thus impairing the assessment validity. Accordingly, this article presents a simulation study to investigate whether it is possible to retrieve normative scores using only positively keyed items in pairwise FC computerized adaptive testing (CAT). Specifically, a simulation study addressed the effect of (a) different bank assembly (with a randomly assembled bank, an optimally assembled bank, and blocks assembled on-the-fly considering every possible pair of items), and (b) block selection rules (i.e., T, and Bayesian D and A-rules) over the estimate accuracy and ipsativity and overlap rates. Moreover, different questionnaire lengths (30 and 60) and trait structures (independent or positively correlated) were studied, and a nonadaptive questionnaire was included as baseline in each condition. In general, very good trait estimates were retrieved, despite using only positively keyed items. Although the best trait accuracy and lowest ipsativity were found using the Bayesian A-rule with questionnaires assembled on-the-fly, the T-rule under this method led to the worst results. This points out to the importance of considering both aspects when designing FC CAT.#FRE#
#IPC# adaptive testing; forced-choice format; ipsative data; item selection; multidimensional IRT#FPC#
#IRF# Brown A., Item response models for forced-choice questionnaires: A common framework, Psychometrika, 81, 1, pp. 135-160, (2016); 
Brown A., Maydeu-Olivares A., Item response modeling of forced-choice questionnaires, Educational and Psychological Measurement, 71, 3, pp. 460-502, (2011); 
Brown A., Maydeu-Olivares A., Fitting a Thurstonian IRT model to forced-choice data using Mplus, Behavior Research Methods, 44, 4, pp. 1135-1147, (2012); 
Brown A., Maydeu-Olivares A., Modelling forced-choice response formats, The Wiley handbook of psychometric testing, pp. 523-569, (2018); 
Bunji K., Okada K., Joint modeling of the two-alternative multidimensional forced-choice personality measurement and its response time by a Thurstonian D-diffusion item response model, Behavior Research Methods, 53, 3, pp. 1091-1107, (2020); 
Burkner P.-C., thurstonianIRT: Thurstonian IRT models in R, Journal of Open Source Software, 4, 42, (2019); 
Burkner P.-C., Schulte N., Holling H., On the statistical and practical limitations of Thurstonian IRT models, Educational and Psychological Measurement, 79, 5, pp. 827-854, (2019); 
Cao M., Drasgow F., Does forcing reduce faking? A meta-analytic review of forced-choice personality measures in high-stakes situations, Journal of Applied Psychology, 104, 11, pp. 1347-1368, (2019); 
Global personality inventory—Adaptive technical manual, (2010); 
Chalmers R.P., mirt: A multidimensional item response theory package for the R environment, Journal of Statistical Software, 48, 6, pp. 1-29, (2012); 
Chang H.-H., Understanding computerized adaptive testing: From Robbins-Monro to Lord and beyond, The SAGE handbook of quantitative methods for the social sciences, pp. 117-133, (2004); 
Chen S.-Y., Ankenmann R.D., Spray J.A., The relationship between item exposure and test overlap in computerized adaptive testing, Journal of Educational Measurement, 40, 2, pp. 129-145, (2003); 
Cheung W.-L., Chan W., Reducing uniform response bias with ipsative measurement in multiple-group confirmatory factor analysis, Structural Equation Modeling: A Multidisciplinary Journal, 9, 1, pp. 55-77, (2002); 
Clemans W.V., An analytical and empirical examination of some properties of ipsative measures, (1966); 
Corey D.M., Dunlap W.P., Burke M.J., Averaging correlations: Expected values and bias in combined Pearson rs and Fisher’s z transformations, The Journal of General Psychology, 125, 3, pp. 245-261, (1998); 
Costa P.T., McCrae R.R., Revised NEO Personality Inventory (NEO-PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual, (1992); 
Drasgow F., Stark S., Chernyshenko O.S., Nye C.D., Hulin C.L., White L.A., Development of the tailored adaptive personality assessment system (TAPAS) to support army personnel selection and classification decisions, (2012); 
Frick S., Brown A., Wetzel E., Investigating the normativity of trait estimates from multidimensional forced-choice data, Multivariate Behavioral Research, (2021); 
Hicks L.E., Some properties of ipsative, normative, and forced-choice normative measures, Psychological Bulletin, 74, 3, pp. 167-184, (1970); 
Houston J.S., Borman W.C., Farmer W.F., Bearden R.M., Development of the navy computer adaptive personality scales (NCAPS), (2006); 
Joo S.-H., Lee P., Stark S., Adaptive testing with the GGUM-RANK multidimensional forced choice model: Comparison of pair, triplet, and tetrad scoring, Behavior Research Methods, 52, pp. 761-772, (2020); 
Judge T.A., Rodell J.B., Klinger R.L., Simon L.S., Crawford E.R., Hierarchical representations of the five-factor model of personality in predicting job performance: Integrating three organizing frameworks with two theoretical perspectives, Journal of Applied Psychology, 98, 6, pp. 875-925, (2013); 
Kaplan M., de la Torre J., Barrada J.R., New item selection methods for cognitive diagnosis computerized adaptive testing, Applied Psychological Measurement, 39, 3, pp. 167-188, (2015); 
Kreitchmann R.S., Abad F.J., Sorrel M.A., A genetic algorithm for optimal assembly of pairwise forced-choice questionnaires, Behavior Research Methods, (2021); 
Lee P., Joo S.-H., a new investigation of fake resistance of a multidimensional forced-choice measure: An application of differential item/test functioning, Personnel Assessment and Decisions, 7, 1, (2021); 
Li M., Sun T., Zhang B., AutoFC: An R package for automatic item pairing in forced-choice test construction, Applied Psychological Measurement, 46, 1, pp. 70-72, (2022); 
Lin Y., Reliability estimates for IRT-based forced-choice assessment scores, Organizational Research Methods, (2021); 
Lin Y., Brown A., Influence of context on item parameters in forced-choice personality assessments, Educational and Psychological Measurement, 77, 3, pp. 389-414, (2017); 
Luecht R.M., Multidimensional computerized adaptive testing in a certification or licensure context, Applied Psychological Measurement, 20, 4, pp. 389-404, (1996); 
Martinez A., Salgado J.F., A meta-analysis of the faking resistance of forced-choice personality inventories, Frontiers in Psychology, 12, (2021); 
McCloy R.A., Heggestad E.D., Reeve C.L., A silk purse from the Sow’s ear: Retrieving normative information from multidimensional forced-choice items, Organizational Research Methods, 8, 2, pp. 222-248, (2005); 
McKinley R.L., Reckase M.D., The use of the general Rasch model with multidimensional item response data, (1982); 
Meade A.W., Psychometric problems and issues involved with creating and using ipsative measures for selection, Journal of Occupational and Organizational Psychology, 77, 4, pp. 531-551, (2004); 
Montano D., Reeske A., Franke F., Huffmeier J., Leadership, followers’ mental health and job performance in organizations: A comprehensive meta-analysis from an occupational health perspective, Journal of Organizational Behavior, 38, 3, pp. 327-350, (2017); 
Morillo D., Item response theory models for forced-choice questionnaires, (2018); 
Morillo D., Abad F.J., Kreitchmann R.S., Leenen I., Hontangas P., Ponsoda V., The journey from likert to forced-choice questionnaires: Evidence of the invariance of item parameters, Revista de Psicología Del Trabajo y de Las Organizaciones, 35, 2, pp. 75-83, (2019); 
Morillo D., Leenen I., Abad F.J., Hontangas P., de la Torre J., Ponsoda V., A dominance variant under the multi-unidimensional pairwise-preference framework model formulation and Markov chain Monte Carlo estimation, Applied Psychological Measurement, pp. 500-516, (2016); 
Mulder J., van der Linden W.J., Multidimensional adaptive testing with optimal design criteria for item selection, Psychometrika, 74, 2, pp. 273-296, (2009); 
Ng V., Lee P., Ho M.H.R., Kuykendall L., Stark S., Tay L., The development and validation of a multidimensional forced-choice format character measure: Testing the Thurstonian IRT approach, Journal of Personality Assessment, 103, 2, pp. 224-237, (2021); 
Olejnik S., Algina J., Generalized eta and omega squared statistics: Measures of effect size for some common research designs, Psychological Methods, 8, 4, pp. 434-447, (2003); 
Pavlov G., Shi D., Maydeu-Olivares A., Fairchild A., Item desirability matching in forced-choice test construction, Personality and Individual Differences, 183, (2021); 
Poropat A.E., A meta-analysis of the five-factor model of personality and academic performance, Psychological Bulletin, 135, 2, pp. 322-338, (2009); 
R: A language and environment for statistical computing, (2020); 
Richardson M., Abraham C., Bond R., Psychological correlates of university students’ academic performance: A systematic review and meta-analysis, Psychological Bulletin, 138, 2, pp. 353-387, (2012); 
Sass R., Frick S., Reips U.-D., Wetzel E., Taking the test taker’s perspective: Response process and test motivation in multidimensional forced-choice versus rating scale instruments, Assessment, 27, 3, pp. 572-584, (2020); 
Schulte N., Holling H., Burkner P.-C., Can high-dimensional questionnaires resolve the ipsativity issue of forced-choice response formats?, Educational and Psychological Measurement, 81, 2, pp. 262-289, (2021); 
Segall D.O., Multidimensional adaptive testing, Psychometrika, 61, 2, pp. 331-354, (1996); 
Singmann H., Bolker B., Westfall J., Aust F., afex: Analysis of factorial experiments, (2020); 
Sorrel M.A., Abad F.J., Najera P., Improving accuracy and usage by correctly selecting: The effects of model selection in cognitive diagnosis computerized adaptive testing, Applied Psychological Measurement, 45, 2, pp. 112-129, (2021); 
Stark S., Chernyshenko O.S., Drasgow F., An IRT approach to constructing and scoring pairwise preference items involving stimuli on different dimensions: The multi-unidimensional pairwise-preference model, Applied Psychological Measurement, 29, 3, pp. 184-203, (2005); 
Stark S., Chernyshenko O.S., Drasgow F., Nye C.D., White L.A., Heffner T., Farmer W.L., From ABLE to TAPAS: A new generation of personality tests to support military selection and classification decisions, Military Psychology, 26, 3, pp. 153-164, (2014); 
Thurstone L.L., A law of comparative judgment, Psychological Review, 34, 4, pp. 273-286, (1927); 
van der Linden W.J., Assembling tests for the measurement of multiple traits, Applied Psychological Measurement, 20, 4, pp. 373-388, (1996); 
van der Linden W.J., Multidimensional adaptive testing with a minimum error-variance criterion, Journal of Educational and Behavioral Statistics, 24, 4, pp. 398-412, (1999); 
van der Linden W.J., Linear models for optimal test design, (2006); 
Wang W.-C., Qiu X.-L., Chen C.-W., Ro S., Jin K.-Y., Item response theory models for ipsative tests with multidimensional pairwise comparison items, Applied Psychological Measurement, 41, 8, pp. 600-613, (2017); 
Wetzel E., Frick S., Comparing the validity of trait estimates from the multidimensional forced-choice format and the rating scale format, Psychological Assessment, 32, 3, pp. 239-253, (2020); 
Wetzel E., Frick S., Brown A., Does multidimensional forced-choice prevent faking? Comparing the susceptibility of the multidimensional forced-choice format and the rating scale format to faking, Psychological Assessment, 33, 2, pp. 156-170, (2021)#FRF#
