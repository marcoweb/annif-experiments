#ITI#Are We There Yet? Evaluating the Effectiveness of a Recurrent Neural Network-Based Stopping Algorithm for an Adaptive Assessment#FTI#
#IRE# Many recent studies have looked at the viability of applying recurrent neural networks (RNNs) to educational data. In most cases, this is done by comparing their performance to existing models in the artificial intelligence in education (AIED) and educational data mining (EDM) fields. While there is increasing evidence that, in many situations, RNN models can improve on the performance of these existing methods, in this work we take a different approach. Rather than directly comparing RNNs with other models, we are instead interested in the results when RNNs are combined with one of these existing models. In particular, we attempt to improve the performance of ALEKS (“A ssessment and LE arning in K nowledge S paces”), an adaptive learning and assessment system based on Knowledge Space Theory, through the use of RNN models. Using data from more than 1.4 million ALEKS assessments, we first build an RNN classifier that attempts to predict the final result of each assessment. After verifying the accuracy of these predictions, we develop our stopping algorithm, with the goal of improving the efficiency of the ALEKS assessment by reducing the total number of questions that are asked. Based on this stopping algorithm, we give a comprehensive analysis of the possible effects it would have on students. We show that the combination of an RNN with the ALEKS assessment can reduce the average assessment length by over 26%, while a high degree of accuracy is maintained#FRE#
#IPC# Adaptive assessment; Deep learning; Knowledge Space Theory; Recurrent neural networks#FPC#
#IRF# Baker R.S., Stupid tutoring systems, intelligent humans, International Journal of Artificial Intelligence in Education, 26, 2, pp. 600-614, (2016); 
Baker R.S.J.D., Corbett A.T., Aleven V., More accurate student modeling through contextual estimation of slip and guess probabilities in Bayesian Knowledge Tracing, Intelligent Tutoring Systems, pp. 406-415, (2008); 
Benjamini Y., Hochberg Y., Controlling the false discovery rate: a practical and powerful approach to multiple testing, Journal of the Royal Statistical Society:, Series B (Methodological), 57, 1, pp. 289-300, (1995); 
Benjamini Y., Yekutieli D., The control of the false discovery rate in multiple testing under dependency, The Annals of Statistics, 29, 4, pp. 1165-1188, (2001); 
Botelho A., Baker R., Heffernan N., Improving sensor-free affect detection using deep learning, Artificial Intelligence in Education-18Th International Conference, AIED, 2017, pp. 40-51, (2017); 
Boughorbel S., Jarray F., El-Anbari M., Optimal classifier for imbalanced data using Matthews correlation coefficient metric, PLOS ONE, 12, 6, (2017); 
Brown T.B., Mann B., Ryder N., Subbiah M., Kaplan J., Dhariwal P., Neelakantan A., Shyam P., Sastry G., Askell A., Language Models are Few-Shot Learners, (2020); 
Cen H., Koedinger K., Junker B., Is over practice necessary? - Improving learning efficiency with the cognitive tutor through educational data mining, Proceedings of the 13Th International Conference on Computers in Education, pp. 511-518, (2007); 
Chicco D., Ten quick tips for machine learning in computational biology, BioData Mining, 10, 1, (2017); 
Child R., Gray S., Radford A., Sutskever I., Generating Long Sequences with Sparse Transformers, (2019); 
de Chiusole D., Stefanutti L., Anselmi P., Robusto E., Stat-Knowlab. Assessment and learning of statistics with competence-based knowledge space theory, International Journal of Artificial Intelligence in Education, pp. 1-33, (2020); 
Cho K., van Merrienboer B., Gulcehre C., Bougares F., Schwenk H., Bengio Y., Learning phrase representations using RNN encoder-decoder for statistical machine translation, Corr, (2014); 
Chollet F., Et al., Keras, (2015); 
Chung J., Gulcehre C., Cho K., Bengio Y., Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, (2014); 
Corbett A.T., Anderson J.R., Knowledge tracing: Modeling the acquisition of procedural knowledge, User Modeling and User-Adapted Interaction, 4, 4, pp. 253-278, (1994); 
Cosyn E., Uzun H., Doble C., Matayoshi J., A practical perspective on knowledge space theory: ALEKS and its data, Journal of Mathematical Psychology; 
Desmarais M.C., Baker R.S., A review of recent advances in learner and skill modeling in intelligent learning environments, User Modeling and User-Adapted Interaction, 22, 1-2, pp. 9-38, (2012); 
Devlin J., Chang M.W., Lee K., Toutanova K., BERT: Pre-training Of deep bidirectional transformers for language understanding, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1, pp. 4171-4186, (2019); 
Dickison D., Ritter S., Nixon T., Harris T.K., Towle B., Murray R.C., Hausmann R.G., Predicting the effects of skill model changes on student progress, International Conference on Intelligent Tutoring Systems, pp. 300-302, (2010); 
Dietterich T.G., Approximate statistical tests for comparing supervised classification learning algorithms, Neural Computation, 10, 7, pp. 1895-1923, (1998); 
Ding X., Larson E.C., Why Deep Knowledge Tracing has less depth than anticipated, In Proceedings of the 12Th International Conference on Educational Data Mining, pp. 282-287, (2019); 
Doble C., Matayoshi J., Cosyn E., Uzun H., Karami A., A data-based simulation study of reliability for an adaptive assessment based on knowledge space theory, International Journal of Artificial Intelligence in Education, 29, pp. 258-282, (2019); 
Doignon J.P., Falmagne J.C., Spaces for the assessment of knowledge, International Journal of Man-Machine Studies, 23, pp. 175-196, (1985); 
Edwards A.L., Note on the correction for continuity in testing the significance of the difference between correlated proportions, Psychometrika, 13, 3, pp. 185-187, (1948); 
Knowledge Spaces: Applications in Education, (2013); 
Falmagne J.C., Doignon J.P., Learning spaces, (2011); 
Fancsali S.E., Nixon T., Vuong A., Ritter S., Simulated students, mastery learning, and improved learning curves for real-world cognitive tutors, AIED 2013 Workshops Proceedings, 4, (2013); 
Gal Y., Ghahramani Z., A theoretically grounded application of dropout in recurrent neural networks, In Advances in Neural Information Processing Systems, 29, neurIPS, (2016); 
Gonzalez-Espada W.J., Bullock D.W., Innovative applications of classroom response systems: Investigating students item response times in relation to final course grade, gender, general point average, and high school ACT scores, Electronic Journal for the Integration of Technology in Education, 6, pp. 97-108, (2007); 
Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); 
Gorodkin J., Comparing two k-category assignments by a k-category correlation coefficient, Computational Biology and Chemistry, 28, 5-6, pp. 367-374, (2004); 
Graves A., Mohamed A., Hinton G., Speech recognition with deep recurrent neural networks, Proceedings of ICASSP, 2013, pp. 6645-6649, (2013); 
Hochreiter S., Schmidhuber J., Long short-term memory, Neural Computation, 9, pp. 1735-1780, (1997); 
Hockemeyer C., Held T., Albert D., Rath-A Relational Adaptive Tutoring Hypertext Www-Environment Based on Knowledge Space Theory, (1997); 
Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, In International Conference on Machine Learning, pp. 448-456, (2015); 
Jiang W., Pardos Z., Wei Q., Goal-based course recommendation, Proceedings of the 9Th International Conference on Learning Analytics and Knowledge, pp. 36-45, (2019); 
Jiang Y., Bosch N., Baker R.S., Paquette L., Ocumpaugh J., Andres J.M.A.L., Moore A.L., Biswas G., Expert feature-engineering vs. Deep neural networks: Which is better for sensor-free affect detection?, Artificial Intelligence in Education-19Th International Conference, AIED, 2018, pp. 198-211, (2018); 
Kaser T., Klingler S., Gross M., When to stop? Towards universal instructional policies, In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, pp. 289-298, (2016); 
Kaser T., Schwartz D.L., Exploring neural network models for the classification of students in highly interactive environments, In Proceedings of the 12Th International Conference on Educational Data Mining, pp. 109-118, (2019); 
Khajah M., Lindsey R., Mozer M., How deep is knowledge tracing?, In Proceedings of the 9Th International Conference on Educational Data Mining, pp. 94-101, (2016); 
Klingler S., Kaser T., Busetto A.G., Solenthaler B., Kohn J., von Aster M., Gross M., Stealth assessment in ITS-a study for developmental dyscalculia, International Conference on Intelligent Tutoring Systems, pp. 79-89, (2016); 
Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); 
Le Q., Mikolov T., Distributed representations of sentences and documents, In International Conference on Machine Learning, pp. 1188-1196, (2014); 
LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); 
Lee J.I., Brunskill E., The impact on individualizing student models on necessary practice opportunities, In Proceedings of the 5Th International Conference on Educational Data Mining, pp. 118-125, (2012); 
Lin C., Chi M., A comparison of BKT, RNN and LSTM for learning gain prediction, Artificial Intelligence in Education-18Th International Conference, AIED, 2017, pp. 536-539, (2017); 
Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer L., Stoyanov V., Roberta: A Robustly Optimized BERT Pretraining Approach, (2019); 
Lynch D., Howlin C.P., Real world usage of an adaptive testing algorithm to uncover latent knowledge, In Proceedings of the 7Th International Conference of Education, Research and Innovation, pp. 504-511, (2014); 
Mao Y., Lin C., Chi M., Deep learning vs. Bayesian Knowledge tracing: Student models for interventions, Journal of Educational Data Mining, 10, 2, pp. 28-54, (2018); 
Matayoshi J., Cosyn E., Uzun H., Using recurrent neural networks to build a stopping algorithm for an adaptive assessment, Artificial Intelligence in Education-20Th International Conference, AIED, 2019, pp. 179-184, (2019); 
Matayoshi J., Granziol U., Doble C., Uzun H., Cosyn E., Forgetting curves and testing effect in an adaptive learning and assessment system, Proceedings of the 11Th International Conference on Educational Data Mining, pp. 607-612, (2018); 
Matayoshi J., Uzun H., Cosyn E., Deep (Un)learning: Using neural networks to model retention and forgetting in an adaptive learning system, Artificial Intelligence in Education-20Th International Conference, AIED, 2019, pp. 258-269, (2019); 
Matthews B.W., Comparison of the predicted and observed secondary structure of t4 phage lysozyme, Biochimica et Biophysica Acta (BBA)-Protein Structure, 405, 2, pp. 442-451, (1975); 
About ALEKS, (2021); 
McNemar Q., Note on the sampling error of the difference between correlated proportions or percentages, Psychometrika, 12, 2, pp. 153-157, (1947); 
Mikolov T., Sutskever I., Chen K., Corrado G.S., Dean J., Distributed representations of words and phrases and their compositionality, Advances in Neural Information Processing Systems, pp. 3111-3119, (2013); 
Mojarad S., Essa A., Mojarad S., Baker R.S., Data-driven learner profiling based on clustering student behaviors: Learning consistency, pace and effort, International Conference on Intelligent Tutoring Systems, pp. 130-139, (2018); 
PyTorch: An imperative style, high-performance deep learning library, Advances in Neural Information Processing Systems, 32, pp. 8024-8035, (2019); 
Pavlik P.I., Cen H., Koedinger K.R., Performance factors analysis–a new alternative to knowledge tracing, Artificial Intelligence in Education-14Th International Conference, AIED, (2009); 
Pavlik P.I., Olney A.M., Bankder A., Eglington E., Yarbro J., The mobile fact and concept textbook system (MofaCTS), In Proceedings of the Second Workshop on Intelligent Textbooks, International Conference on Artificial Intelligence in Education, (2020); 
Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay E., Scikit-learn: Machine learning in python, Journal of Machine Learning Research, 12, pp. 2825-2830, (2011); 
Pelc A., Searching games with errors–fifty years of coping with liars, Theoretical Computer Science, 270, 1-2, pp. 71-109, (2002); 
Piech C., Bassen J., Huang J., Ganguli S., Sahami M., Guibas L., Sohl-Dickstein J., Deep knowledge tracing, Advances in Neural Information Processing Systems, pp. 505-513, (2015); 
Powers D.M., Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation, Journal of Machine Learning Technologies, pp. 37-63, (2011); 
Prechelt L., Early stopping – but when?, Neural Networks: Tricks of the Trade, Lecture Notes in Computer Science, 7700, (2012); 
Reddy A., Harper M., Mathematics placement at the University of Illinois, PRIMUS, 23, pp. 683-702, (2013); 
Rollinson J., Brunskill E., From predictive models to instructional policies, In Proceedings of the 8Th International Conference on Educational Data Mining, pp. 179-186, (2015); 
Ruseti S., Dascalu M., Johnson A.M., Balyan R., Kopp K.J., McNamara D.S., Crossley S.A., Trausan-Matu S., Predicting question quality using recurrent neural networks, International Conference on Artificial Intelligence in Education, pp. 491-502, (2018); 
Sak H., Senior A., Beaufays F., Long short-term memory recurrent neural network architectures for large scale acoustic modeling, Fifteenth Annual Conference of the International, (2014); 
Santurkar S., Tsipras D., Ilyas A., Madry A., How does batch normalization help optimization?, Advances in Neural Information Processing Systems, pp. 2483-2493, (2018); 
Silver D., Huang A., Maddison C.J., Guez A., Sifre L., van den Driessche G., Schrittwieser J., Antonoglou I., Panneershelvam V., Lanctot M., Dieleman S., Grewe D., Nham J., Kalchbrenner N., Sutskever I., Lillicrap T., Leach M., Kavukcuoglu K., Graepel T., Hassabis D., Mastering the game of Go with deep neural networks and tree search, Nature, 529, pp. 484-503, (2016); 
Silver D., Schrittwieser J., Simonyan K., Antonoglou I., Huang A., Guez A., Hubert T., Baker L., Lai M., Bolton A., Et al., Mastering the game of Go without human knowledge, Nature, 550, 7676, (2017); 
Srivastava N., Hinton G., Krizhevsky A., Sutskever I., Salakhutdinov R., Dropout: a simple way to prevent neural networks from overfitting, Journal of Machine Learning Research, 15, pp. 1929-1968, (2014); 
Theano: A Python Framework for Fast Computation of Mathematical Expressions, (2016); 
Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Attention is all you need, Advances in Neural Information Processing Systems, pp. 5998-6008, (2017); 
Wang Y., Heffernan N.T., Leveraging first response time into the knowledge tracing model, In Proceedings of the 5Th International Conference on Educational Data Mining, pp. 176-179, (2012); 
Wilson K.H., Karklin Y., Han B., Ekanadham C., Back to the basics: Bayesian extensions of IRT outperform neural networks for proficiency estimation, Proceedings of the 9Th International Conference on Educational Data Mining, pp. 539-544, (2016); 
Wilson K.H., Xiong X., Khajah M., Lindsey R.V., Zhao S., Karklin Y., van Inwegen E.G., Han B., Ekanadham C., Beck J.E., Et al., Estimating student proficiency: Deep learning is not the panacea, In Neural Information Processing Systems, Workshop on Machine Learning for Education, (2016); 
Wu Y., Schuster M., Chen Z., Le Q.V., Norouzi M., Google’s neural machine translation system: Bridging the gap between human and machine translation, (2016); 
Xiong X., Zhao S., Vaninwegen E., Beck J., Going deeper with knowledge tracing, Proceedings of the 9Th International Conference on Educational Data Mining, pp. 545-550, (2016); 
Xu L., Davenport M., Dynamic knowledge embedding and tracing, In Proceedings of the 13Th International Conference on Educational Data Mining, pp. 524-530, (2020); 
Yang Z., Dai Z., Yang Y., Carbonell J., Salakhutdinov R.R., Le Q.V., Xlnet: Generalized autoregressive pretraining for language understanding, Advances in Neural Information Processing Systems, pp. 5754-5764, (2019); 
Yin W., Kann K., Yu M., Schutze H., Comparative Study of CNN and RNN for Natural Language Processing, (2017); 
Yudelson M.V., Koedinger K.R., Gordon G.J., Individualized Bayesian Knowledge Tracing models, Artificial Intelligence in Education-16Th International Conference, AIED 2013, pp. 171-180, (2013)#FRF#
