#ITI#A Cooperative Model of Development and Validation of a Curriculum-Based Scientific Competence Test#FTI#
#IRE#Teachers lack effective curriculum-based instruments to assess their students’ scientific competence that would provide information for modifying their inquiry instruction. The main purpose of this study was to develop and validate a Curriculum-Based Scientific Competence (CBSC) test to assess students’ scientific competence in a 1-semester Grade 9 curriculum. The initial 51-item CBSC test was administered to 318 Grade 10 students from 4 schools. The Rasch partial credit model was used to analyse the quality of the CBSC test. The final 44-item CBSC test demonstrated satisfactory item fit, threshold, and test reliability. Further, a 23-item PISA test was used to measure criterion validity. Results indicated that the CBSC test was valid for assessing students’ scientific competencies and can distinguish the effect of inquiry instruction from that of traditional instruction. A CBSC test would provide teachers the summative assessment of scientific competency for examining the effect of inquiry-based teaching. The model of development and validation of the CBSC test through the cooperation of teachers and researchers for each semester of a science programme is recommended. © National Science and Technology Council, Taiwan 2023. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.#FRE#
#IPC#Assessment; Instrument development; PISA; Rasch analysis; Scientific competence#FPC#
#IRF#Abate T., Michael K., Angell C., Assessment of scientific reasoning: Development and validation of scientific reasoning assessment tool, EURASIA Journal of Mathematics, Science and Technology Education, 16, 12, (2020); 
Alfieri L., Brooks P.J., Aldrich N.J., Tenenbaum H.R., Does discovery-based instruction enhance learning?, Journal of Educational Psychology, 103, 1, pp. 1-18, (2011); 
Anagnostopoulou K., Hatzinikita V., Christidou V., Dimopoulos K., PISA test items and curriculum-based examinations in Greece: Exploring the relationship between global and local assessment discourses, International Journal of Science Education, 35, 4, pp. 636-662, (2013); 
Anderson J.O., Lin H.-S., Treagust D.F., Ross S.P., Yore L.D., Using large-scale assessment datasets for research in science and mathematics education: Programme for International Student Assessment (PISA), International Journal of Science & Mathematics Education, 5, 4, pp. 591-614, (2007); 
Baghaei P., Amrahi N., Validation of a multiple choice English vocabulary test with the Rasch model, Journal of Language Teaching & Research, 2, 5, pp. 1052-1060, (2011); 
Bellocchi A., King D.T., Ritchie S.M., Context-based assessment: Creating opportunities for resonance between classroom fields and societal fields, International Journal of Science Education, 38, 8, pp. 1304-1342, (2016); 
Blanchard M.R., Southerland S.A., Osborne J.W., Sampson V.D., Annetta L.A., Granger E.M., Is inquiry possible in light of accountability? A quantitative comparison of the relative effectiveness of guided inquiry and verification laboratory instruction, Science Education, 94, 4, pp. 577-616, (2010); 
Bond T.G., Fox C.M., Applying the Rasch model: Fundamental measurement in the human sciences, (2007); 
Boone W.J., Scantlebury K., The role of Rasch analysis when conducting science education research utilizing multiple-choice tests, Science Education, 90, 2, pp. 253-269, (2006); 
Brown N.J.S., Wilson M., A model of cognition: The missing cornerstone of assessment, Educational Psychology Review, 23, 2, pp. 221-234, (2011); 
Buffler A., Allie S., Lubben F., The development of first year physics students’ ideas about measurement in terms of point and set paradigms, International Journal of Science Education, 23, 11, pp. 1137-1156, (2001); 
Bybee R.W., McCrae B., Laurie R., PISA 2006: An assessment of scientific literacy, Journal of Research in Science Teaching, 46, 8, pp. 865-883, (2009); 
Chang Y.L., Wu S.C., A case study on developmental changes of eleventh graders’ scientific inquiry competences, Eurasia Journal of Mathematics, Science and Technology Education, 14, 1, pp. 363-382, (2018); 
Cook D.A., Much ado about differences: Why expert-novice comparisons add little to the validity argument, Advances in Health Sciences Education, 20, 3, pp. 829-834, (2015); 
Dixon H., Hawe E., Developing assessment-capable teachers through engagement in assessment for learning (AfL): A New Zealand study, Teacher Learning with Classroom Assessment, pp. 59-76, (2018); 
Elliott J., The curriculum experiment: Meeting the challenge of social change, (1998); 
Espin C.A., Busch T.W., Lembke E.S., Hampton D.D., Seo K., Zukowski B.A., Curriculum-based measurement in science learning: Vocabulary-matching as an indicator of performance and progress, Assessment for Effective Intervention, 38, 4, pp. 203-213, (2013); 
Fensham P.J., Rennie L.J., Towards and authentically assessed science curriculum, Valuing assessment in science education: Pedagogy, curriculum, policy, pp. 69-100, (2013); 
Fischer H.E., Boone W.J., Neumann K., Quantitative research designs and approaches, Handbook of research in science education, pp. 18-37, (2014); 
Fisher W.P., Rating scale instrument quality criteria, Rasch Measurement Transactions, 21, 1, (2007); 
Gormally C., Brickman P., Lutz M., Developing a test of scientific literacy skills (TOSLS): Measuring undergraduates’ evaluation of scientific information and arguments, CBE—Life Sciences Education, 11, 4, pp. 364-377, (2012); 
Hatzinikita V., Dimopoulos K., Christidou V., PISA test items and school textbooks related to science: A textual comparison, Science Education, 92, 4, pp. 664-687, (2008); 
Hintze J.M., Christ T.J., Methe S.A., Curriculum-based assessment, Psychology in the Schools, 43, 1, pp. 45-56, (2006); 
Hsu Y.-S., Fang S.-C., Zhang W.-X., Wu H.-K., Wu P.-H., Hwang F.-K., Identifying effective design features of technology-infused inquiry learning modules: A two-year study of students’ inquiry abilities, Educational Technology & Society, 19, 2, pp. 228-244, (2016); 
Johnson R.B., Christensen L.B., Educational research: Quantitative, qualitative, and mixed approaches, (2008); 
Kane M.T., Validating the interpretations and uses of test scores, Journal of Educational Measurement, 50, 1, pp. 1-73, (2013); 
Kind P.M., Conceptualizing the science curriculum: 40 years of developing assessment frameworks in three large-scale assessments, Science Education, 97, 5, pp. 671-694, (2013); 
Kind P.M., Establishing assessment scales using a novel disciplinary rationale for scientific reasoning, Journal of Research in Science Teaching, 50, 5, pp. 530-560, (2013); 
Kremer K., Fischer H.E., Kauertz A., Mayer J., Sumfleth E., Walpuski M., Assessment of standard-based learning outcomes in science education: Perspectives from the German project ESNAS, Making It Tangible: Learning Outcomes in Science Education, pp. 201-218, (2012); 
Kuhn T.,  Ed.)., (1970); 
Kuo C.Y., Wu H.-K., Jen T.H., Hsu Y.-S., Development and validation of a multimedia-based assessment of scientific inquiry abilities, International Journal of Science Education, 37, 14, pp. 2326-2357, (2015); 
Laugksch R.C., Spargo P.E., Development of a pool of scientific literacy test-items based on selected AAAS literacy goals, Science Education, 80, 2, pp. 121-143, (1996); 
Le Hebel F., Montpied P., Tiberghien A., Fontanieu V., Sources of difficulty in assessment: Example of PISA science items, International Journal of Science Education, 39, 4, pp. 468-487, (2017); 
Le Hebel F., Tiberghien A., Montpied P., Fontanieu V., Teacher prediction of student difficulties while solving a science inquiry task: Example of PISA science items, International Journal of Science Education, 41, 11, pp. 1517-1540, (2019); 
Lin S.-F., Chang J.-S., Wu L.-C., Huang C.-C., Chao L.-H., Sheu C.-D., Tseng P.-Y., Lai Y.-L., Lee C.-Y., Using Rasch model to evaluate physics competence tests and itsimplications for teachers’ development of competency-based items, Journal of Educational Practice and Research, 34, 1, pp. 121-154, (2021); 
Liu O.L., Lee H.S., Linn M.C., Measuring knowledge integration: Validation of four-year assessments, Journal of Research in Science Teaching, 48, 9, pp. 1079-1107, (2011); 
Looney A., Cumming J., van Der Kleij F., Harris K., Reconceptualising the role of teachers as assessors: Teacher assessment identity, Assessment in Education: Principles, Policy & Practice, 25, 5, pp. 442-467, (2017); 
Lorch R.F., Lorch E.P., Calderhead W.J., Dunlap E.E., Hodell E.C., Freer B.D., Learning the control of variables strategy in higher and lower achieving classrooms: Contributions of explicit instruction and experimentation, Journal of Educational Psychology, 102, 1, pp. 90-101, (2010); 
McCoubrie P., Improving the fairness of multiple-choice questions: A literature review, Medical Teacher, 26, 8, pp. 709-712, (2004); 
Messick S., Validity of psychological assessment: Validation of inferences from persons’ responses and performances as scientific inquiry into score meaning, American Psychologist, 50, 9, (1995); 
Millar R., Osborne J., Beyond 2000: Science education for the future, (1998); 
Guidelines for nature and life technology learning discipline in the 9-year curricula of national primary and junior schools, Ministry of Education, Taiwan (, (2008); 
Guidelines for the 12-year basic education curricula: Natural science, Ministry of Education, Taiwan, (2018); 
Minner D.D., Levy A.J., Century J., Inquiry-based science instruction-what is it and does it matter? Results from a research synthesis years 1984 to 2002, Journal of Research in Science Teaching, 47, 4, pp. 474-496, (2010); 
Myers M.J., Burgess A.B., Inquiry-based laboratory course improves students’ ability to design experiments and interpret data, Advances in Physiology Education, 27, 1, pp. 26-33, (2003); 
National science education standards, (1996); 
Onwuegbuzie A.J., Witcher A.E., Collins K.M., Filer J.D., Wiedmaier C.D., Moore C.W., Students’ perceptions of characteristics of effective college teachers: A validity study of a teaching evaluation form using a mixed-methods analysis, American Educational Research Journal, 44, 1, pp. 113-160, (2007); 
Organisation for Economic Co-operation and Development, PISA 2003 Technical Report, (2005); 
OzdemYilmaz Y., Cakiroglu J., Ertepinar H., Erduran S., The pedagogy of argumentation in science education: Science teachers’ instructional practices, International Journal of Science Education, 39, 11, pp. 1443-1464, (2017); 
Organisation for Economic Co-Operation and Development., (2017); 
Reckase M.D., Unifactor latent trait models applied to multifactor tests: Results and implications, Journal of Educational Statistics, 4, 3, pp. 207-230, (1979); 
Roberts D.A., Competing visions of scientific literacy: The influence of a science curriculum policy image, Promoting scientific literacy: Science education research in transaction, pp. 11-27, (2011); 
Roberts D.A., Scientific literacy/science literacy, Handbook of Research on Science Education, pp. 743-794, (2013); 
Routitsky A., Turner R., Item format types and their influence on cross-national comparisons of student performance, Paper Presented at the American Educational Research Association Annual Meeting, (2003); 
Ryder J., Identifying science understanding for functional scientific literacy, Studies in Science Education, 36, 1, pp. 1-44, (2001); 
Sadler T.D., Zeidler D.L., Scientific literacy, PISA, and socioscientific discourse: Assessment for progressive aims of science education, Journal of Research in Science Teaching, 46, 8, pp. 909-921, (2009); 
She H.-C., Huang L.Y., Su Y.P., Taiwan student performance of scientific literacy, Taiwan student performance on PISA 2015, pp. 23-80, (2017); 
Stender A., Schwichow M., Zimmerman C., Hartig H., Making inquiry-based science learning visible: The influence of CVS and cognitive skills on content knowledge learning in guided inquiry, International Journal of Science Education, 40, 15, pp. 1812-1831, (2018); 
Wilson M., Constructing measures an item response modelling approach, (2005); 
Wu M., Adams R., Applying the Rasch model to psycho-social measurement: A practical approach, (2007); 
Wu H.-K., Hsieh C.E., Developing sixth graders’ inquiry skills to construct explanations in inquiry-based learning environments, International Journal of Science Education, 28, 11, pp. 1289-1313, (2006); 
Wu P.-H., Wu H.-K., Hsu Y.-S., Establishing the criterion-related, construct, and content validities of a simulation-based assessment of inquiry abilities, International Journal of Science Education, 36, 10, pp. 1630-1650, (2014); 
Wu M.L., Adams R.J., Wilson M.R., ConQuest [Computer software and manual], Australian Council for Educational Research, (2007)#FRF#
