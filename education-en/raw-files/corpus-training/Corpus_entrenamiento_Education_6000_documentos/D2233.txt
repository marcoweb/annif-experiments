#ITI#A Mixture IRTree Model for Performance Decline and Nonignorable Missing Data#FTI#
#IRE# In educational assessments and achievement tests, test developers and administrators commonly assume that test-takers attempt all test items with full effort and leave no blank responses with unplanned missing values. However, aberrant response behavior—such as performance decline, dropping out beyond a certain point, and skipping certain items over the course of the test—is inevitable, especially for low-stakes assessments and speeded tests due to low motivation and time limits, respectively. In this study, test-takers are classified as normal or aberrant using a mixture item response theory (IRT) modeling approach, and aberrant response behavior is described and modeled using item response trees (IRTrees). Simulations are conducted to evaluate the efficiency and quality of the new class of mixture IRTree model using WinBUGS with Bayesian estimation. The results show that the parameter recovery is satisfactory for the proposed mixture IRTree model and that treating missing values as ignorable or incorrect and ignoring possible performance decline results in biased estimation. Finally, the applicability of the new model is illustrated by means of an empirical example based on the Program for International Student Assessment.#FRE#
#IPC# IRTree; item response theory (IRT); missing not at random; mixture models; performance decline#FPC#
#IRF# Ashcraft M.H., Krause J.A., Working memory, math performance, and math anxiety, Psychonomic Bulletin & Review, 14, 2, pp. 243-248, (2007); 
Bliss L.B., A test of Lord’s assumption regarding examinee guessing behavior on multiple choice tests using elementary school children, Journal of Educational Measurement, 17, 2, pp. 147-153, (1980); 
Boekaerts M., Self-regulated learning: A new concept embraced by researchers, policy makers, educators, teachers, and students, Learning and Instruction, 7, 2, pp. 161-186, (1997); 
Bolt D.M., Cohen A.S., Wollack J.A., Item parameter estimation under conditions of test speededness: Applications of a mixture Rasch model with ordinal constraints, Journal of Educational Measurement, 39, 4, pp. 331-348, (2002); 
Brooks S.P., Gelman A., General methods for monitoring convergence of iterative simulations, Journal of Computational and Graphical Statistics, 7, 4, pp. 434-455, (1998); 
Cao J., Stokes S.L., Bayesian IRT guessing models for partial guessing behaviors, Psychometrika, 73, pp. 209-230, (2008); 
Cassady J.C., Johnson R.E., Cognitive test anxiety and academic performance, Contemporary Educational Psychology, 27, 2, pp. 270-295, (2002); 
Cho S.-J., Cohen A.S., Multilevel mixture IRT model with an application to DIF, Journal of Educational and Behavioral Statistics, 35, 3, pp. 336-370, (2010); 
Cohen A.S., Bolt D.M., A mixture model analysis of differential item functioning, Journal of Educational Measurement, 42, 2, pp. 133-148, (2005); 
Cole J.S., Bergin D.A., Whittaker T.A., Predicting student achievement for low stakes tests with effort and task value, Contemporary Educational Psychology, 33, 4, pp. 609-624, (2008); 
Crocker L., Algina J., Introduction to classical and modern test theory, (1986); 
Cross L., Frary R., An empirical test of Lord’s theoretical results regarding formula scoring of multiple choice tests, Journal of Educational Measurement, 14, 4, pp. 313-322, (1977); 
Debeer D., Janssen R., Modeling item-position effects within an IRT framework, Journal of Educational Measurement, 50, 2, pp. 164-185, (2013); 
Debeer D., Janssen R., De Boeck P., Modeling skipped and not-reached items using IRTrees, Journal of Educational Measurement, 54, 3, pp. 333-363, (2017); 
De Boeck P., Partchev I., IRTrees: Tree-based item response models of the GLMM family, Journal of Statistical Software, 48, 1, pp. 1-28, (2012); 
De Boeck P., Wilson M., Explanatory item response models: A generalized linear and nonlinear approach, (2004); 
Douglas J., Kim H.R., Habing B., Gao F., Investigating local dependence with conditional covariance functions, Journal of Educational & Behavioral Statistics, 23, 2, pp. 129-151, (1998); 
Dutke S., Stober J., Test anxiety, working memory, and cognitive performance: Supportive effects of sequential demands, Cognition and Emotion, 15, 3, pp. 381-389, (2001); 
Evans F.R., Reilly R.R., A study of speededness as a source of test bias, Journal of Educational Measurement, 9, 2, pp. 123-131, (1972); 
Eysenck M.W., Calvo M.G., Anxiety and performance: The processing efficiency theory, Cognition and Emotion, 6, 6, pp. 409-434, (1992); 
Glas C.A.W., Pimentel J., Modeling nonignorable missing data in speeded tests, Educational and Psychological Measurement, 68, 6, pp. 907-922, (2008); 
Glas C.A.W., Pimentel J., Lamers S.M.A., Nonignorable data in IRT models: Polytomous responses and response propensity models with covariates, Psychological Test and Assessment Modeling, 57, 4, pp. 523-541, (2015); 
Goegebeur Y., De Boeck P., Wollack J.A., Cohen A.S., A speeded item response model with gradual process change, Psychometrika, 73, pp. 65-87, (2008); 
Grigg W., Donahue P., Dion G., The nation’s report card: 12th-grade reading and mathematics 2005, (2007); 
Holman R., Glas C.A.W., Modelling nonignorable missing data mechanism with item response theory models, British Journal of Mathematical and Statistical Psychology, 58, 1, pp. 1-18, (2005); 
Hong E., Peng Y., Rowell L.L., Homework self-regulation: Grade, gender, and achievement level differences, Learning and Individual Differences, 19, 2, pp. 269-276, (2009); 
Huang H.-Y., Effects of the common scale setting in the assessment of differential item functioning, Psychological Reports, 114, 1, pp. 104-125, (2014); 
Huang H.-Y., Mixture random-effect IRT models for controlling extreme response style on rating scales, Frontiers in Psychology, 7, (2016); 
Huang H.-Y., Mixture IRT model with a higher-order structure for latent traits, Educational and Psychological Measurement, 77, 2, pp. 275-304, (2017); 
Jeon M., De Boeck P., A generalized item response tree model for psychological assessments, Behavior Research Methods, 48, 3, pp. 1070-1085, (2016); 
Jin K.-Y., Wang W.-C., Item response theory models for performance decline during testing, Journal of Educational Measurement, 51, 2, pp. 178-200, (2014); 
Khine M.S., Areepattamannil S., Non-cognitive skills and factors in educational attainment, (2016); 
Kohler C., Pohl S., Carstensen C.H., Dealing with item nonresponse in large-scale cognitive assessments: The impact of missing data methods on estimated explanatory relationships, Journal of Educational Measurement, 54, 4, pp. 397-419, (2017); 
Leighton J.P., Gierl M.J., Why cognitive diagnostic assessment?, Cognitive diagnostic assessment for education: Theory and applications, pp. 1-18, (2007); 
Little R.J.A., Rubin D.B., Statistical analysis with missing data, (1987); 
Lord F.M., Formula scoring and number-right scoring, Journal of Educational Measurement, 12, 1, pp. 7-11, (1975); 
Lord F.M., Application of item response theory to practical testing problems, (1980); 
Lu Y., Sireci S.G., Validity issues in test speededness, Educational Measurement: Issues and Practice, 26, 4, pp. 29-37, (2007); 
Meijer R.R., Person-fit research: An introduction, Applied Measurement in Education, 9, 1, pp. 3-8, (1996); 
Messick S., Validity, Educational measurement, pp. 1-103, (1989); 
Mroch A.A., Bolt D.M., An IRT-based response likelihood approach for addressing test speededness, (2006); 
Newell A., Simon H.A., Human problem solving, (1972); 
PISA 2006 technical report, (2009); 
PISA 2015 technical report, (2017); 
Okumura T., Empirical differences in omission tendency and reading ability in PISA: An application of tree-based item response models, Educational and Psychological Measurement, 74, 4, pp. 611-626, (2014); 
Oshima T.C., The effect of speededness on parameter estimation in item response theory, Journal of Educational Measurement, 31, 3, pp. 200-219, (1994); 
Peng Y., Hong E., Mason E., Motivational and cognitive test-taking strategies and their influence on test performance in mathematics, Educational Research and Evaluation: An International Journal on Theory and Practice, 20, 5, pp. 366-385, (2014); 
Pohl S., Grafe L., Rose N., Dealing with omitted and not reached items in competence tests: Evaluating approaches accounting for missing responses in IRT models, Educational and Psychological Measurement, 74, 3, pp. 423-452, (2014); 
Pohl S., Haberkorn K., Hardt K., Wiegand E., NEPS Technical report for reading: Scaling results of starting cohort 3 in fifth grade, (2012); 
Qian J., An investigation of position effects in large-scale writing assessments, Applied Psychological Measurement, 38, 7, pp. 518-534, (2014); 
Rose N., von Davier M., Nagengast B., Modeling omitted and not-reached items in IRT models, Psychometrika, 82, pp. 795-819, (2017); 
Rose N., von Davier M., Xu X., Modeling nonignorable missing data with item response theory (IRT), (2010); 
Rowley G.L., Traub R.E., Formula scoring, number-right scoring, and test-taking. strategy, Journal of Educational Measurement, 14, 1, pp. 15-22, (1977); 
Rubin D.B., Inference and missing data, Biometrika, 63, 3, pp. 581-592, (1976); 
Rupp A.A., Templin J.L., Henson R.A., Diagnostic measurement: Theory, methods, and applications, (2010); 
Sarason I.G., Anxiety, self-preoccupation and attention, Anxiety Research, 1, 1, pp. 3-8, (1988); 
Snow R.E., Lohman D.F., Implications of cognitive psychology for educational measurement, Educational measurement, pp. 263-331, (1989); 
Spiegelhalter D.J., Thomas A., Best N.G., Lunn D., WinBUGS, (2003); 
Sternberg R.J., Component processes in analogical reasoning, Psychological Review, 84, 4, pp. 353-378, (1977); 
Sternberg R.J., Lautrey J., Lubart T.I., Models of intelligence: International perspectives, (2003); 
Suh Y., Cho S.-J., Wollack J.A., A comparison of item calibration procedures in the presence of test speededness, Journal of Educational Measurement, 49, 3, pp. 285-311, (2012); 
Tobias S., The impact of test anxiety on cognition in school learning, Advances in test anxiety research, 7, pp. 18-31, (1992); 
Tobias S., Everson H.T., Studying the relationship between affective and metacognitive variables, Anxiety, Stress, and Coping, 10, 1, pp. 59-81, (1997); 
van Barneveld C., The effect of examinee motivation on test construction within an IRT framework, Applied Psychological Measurement, 31, 1, pp. 31-46, (2007); 
van der Linden W.J., Klein Entink R.H., Fox J.-P., IRT parameter estimation with response times as collateral information, Applied Psychological Measurement, 34, 5, pp. 327-347, (2010); 
Weirich S., Hecht M., Penk C., Roppelt A., Bohme K., Item position effects are moderated by changes in test-taking effort, Applied Psychological Measurement, 41, 5, pp. 115-129, (2017); 
Wigfield A., Eccles J.S., Expectancy–value theory of achievement motivation, Contemporary Educational Psychology, 25, 1, pp. 68-81, (2000); 
Wise L., A persistent model of motivation and test performance, (1996); 
Wise S.L., Strategies for managing the problem of unmotivated examinees in low-stakes testing programs, Journal of General Education, 58, 3, pp. 152-166, (2009); 
Wise S.L., DeMars C.E., Low examinee effort in low-stakes assessment: Problems and potential solutions, Educational Assessment, 10, 3, pp. 1-17, (2005); 
Yamamoto K., Everson H., Modeling the effects of test length and test time on parameter estimation using the HYBRID model, Applications of latent trait and latent class models in the social sciences, pp. 89-98, (1997)#FRF#
