#ITI#The Response Vector for Mastery Method of Standard Setting#FTI#
#IRE# Proposed is a new method of standard setting referred to as response vector for mastery (RVM) method. Under the RVM method, the task of panelists that participate in the standard setting process does not involve conceptualization of a borderline examinee and probability judgments as it is the case with the Angoff and bookmark methods. Also, the RVM-based computation of a cut-score is not based on a single item (e.g., marked in an ordered item booklet) but, instead, on a response vector (1/0 scores) on items and their parameters calibrated in item response theory or under the recently developed D-scoring method. Illustrations with hypothetical and real-data scenarios of standard setting are provided and methodological aspects of the RVM method are discussed.#FRE#
#IPC# assessment; cut-score; standard setting#FPC#
#IRF# Angoff W.H., Scales, norms, and equivalent scores, Educational measurement, pp. 508-600, (1971); 
Atanasov D.V., Dimitrov D.M., DELTA: A computer program for D-scoring and equating of test data (v.2.0), (2019); 
Baldwin P., Some problems with the analytical argument in support of RP67 in the context of the bookmark standard setting method, Applied Psychological Measurement, 43, 6, pp. 481-492, (2018); 
Baldwin P., Margolis M., Clauser B.E., Mee J., Winward M., The choice of response probability in bookmark standard setting: An experimental study, Educational Measurement, 39, 1, pp. 37-44, (2019); 
Beretvas S.N., Comparison of bookmark difficulty locations under different item response models, Applied Psychological Measurement, 28, 1, pp. 25-47, (2004); 
Berk R.A., A consumer’s guide to setting performance standards on criterion-referenced tests, Review of Educational Research, 56, 1, pp. 137-172, (1986); 
Chang L., Judgmental item analysis of the Nedelsky and Angoff standard-setting methods, Applied Measurement in Education, 12, 2, pp. 151-165, (1999); 
Cizek G.J., Bunch M.B., Standard stetting: A guide to establishing and evaluating performance standards on test, (2007); 
Cizek G.J., Bunch M.B., Koons H., Setting performance standards: Contemporary methods, Educational Measurement, 23, 4, pp. 31-50, (2004); 
Clauser B.E., Margolis M.J., Case S.M., Testing for licensure and certification in the professions, Educational Measurement, pp. 701-731, (2006); 
Davis-Becker S.L., Buckendahl C., Gerrow J., Evaluating the bookmark standard setting method: The impact of random item ordering, International Journal of Testing, 11, 1, pp. 24-37, (2011); 
Dimitrov D.M., An approach to scoring and equating tests with binary items: Piloting with large-scale assessments, Educational and Psychological Measurement, 76, 7, pp. 954-975, (2016); 
Dimitrov D.M., The delta scoring method of tests with binary items: A note on true score estimation and equating, Educational and Psychological Measurement, 78, 5, pp. 805-825, (2018); 
Dimitrov D.M., Modeling of item response functions under the D-scoring method, Educational and Psychological Measurement, 80, 1, pp. 126-144, (2020); 
Dimitrov D.M., Alsadaawi A., A standard setting method using the D- scoring method: Procedures and application to assessment for teacher certification, (2019); 
Dimitrov D.M., Al-Shamrani A., A new approach to setting cutting scores for mastery in language testing, (2019); 
Dimitrov D.M., Atanasov D.V., Latent D-scoring modeling: Estimation of item and person parameters, Educational and Psychological Measurement, 81, 2, pp. 388-404, (2021); 
Dimitrov D.M., Ghamdi H., Alahmadi M., Setting cutting scores for mastery on Mawhibah’s multiple cognitive ability tests, (2020); 
Efron B., Bootstrap methods: Another look at the jackknife, Annals of Statistics, 7, pp. 1-26, (1979); 
Egan K.L., Ferrara S., Schneider M.C., Barton K.E., Writing performance level descriptors and setting performance standards for assessments of modified achievement standards: The role of innovation and importance of following conventional practice, Peabody Journal of Education, 84, 4, pp. 552-577, (2009); 
Ferrara S., Lewis D.M., The item-descriptor (ID) matching method, Setting performance standards: Foundations, methods, and innovations, pp. 255-282, (2012); 
Ferrara S., Perie M., Johnson E., Matching the judgmental task with standard setting panelist expertise: The item-descriptor (ID) matching procedure, Journal of Applied Testing Technology, 9, 1, pp. 1-22, (2008); 
Ferrara S., Swaffield S., Mueller L., Conceptualizing and setting performance standards for alternate assessments, Alternate assessments based on alternate achievement standards: Policy, practice, and potential, pp. 93-111, (2009); 
Grabovsky I., Wainer H., The cut-score operating function: A new tool to aid in standard setting, Journal of Educational and Behavioral Statistics, 42, 3, pp. 251-263, (2017); 
Hambleton R.K., Setting performance standards on educational assessments and criteria for evaluating the process, Setting performance standards, pp. 89-115, (2001); 
Hambleton R.K., Pitoniak M.J., Setting performance standards, Educational measurement, (2006); 
Hambleton R.K., Plake B.S., Using an extended Angoff procedure to set standards on complex performance assessments, Applied Measurement in Education, 8, 1, pp. 41-55, (1995); 
Hambleton R.K., Swaminathan H., Rogers H.J., Fundamentals of item response theory, (1991); 
Hauser R.M., Edley C.F., Koenig J.A., Elliott S.W., Measuring literacy: Performance levels for adults, (2005); 
Huynh H., On score locations of binary and partial credit items and their applications to item mapping and criterion-referenced interpretation, Journal of Educational and Behavioral Statistics, 23, 1, pp. 35-56, (1998); 
Huynh H., A clarification on the response probability criterion RP67 for standard settings based on bookmark and item mapping, Educational Measurement: Issues and Practice, 25, 2, pp. 19-20, (2006); 
Kane M.T., Validating the performance standards associated with passing scores, Review of Educational Research, 64, 3, pp. 425-461, (1994); 
Kane M.T., So much remains the same: Conception and status of validation in setting standards, Setting performance standards, pp. 53-88, (2001); 
Karantonis A., Sireci S.G., The bookmark standard-setting method: A literature review, Educational Measurement: Issues and Practice, 25, 1, pp. 4-12, (2006); 
Lewis D.M., Green R., The validity of PLDs, (1997); 
Lewis D.M., Mitzel H.C., Green D.R., Patz R.J., The bookmark standard setting procedure, (1999); 
Lewis D.M., Mitzel H.C., Mercado R.L., Schulz E.M., The bookmark standard setting procedure, Setting performance standards: Foundations, methods, and innovations, pp. 225-254, (2012); 
Lin J., The bookmark procedure for setting cut-scores and finalizing performance standards: Strengths and weaknesses, Alberta Journal of Educational Research, 52, 1, pp. 36-52, (2006); 
Mills C.N., Jaeger R.M., Creating descriptions of desired student achievement when setting performance standards, Handbook for the development of performance standards, pp. 73-85, (1998); 
Mitzel H.C., Lewis D.M., Patz R.J., Green D.R., Setting performance standards: Concepts, methods and perspectives, The bookmark procedure: Psychological perspectives, pp. 249-281, (2001); 
Mourgues C.V., Tan M., Hein S., Al-Harbi K., Aljughaiman A., Grigorenko E.L., The relationship between analytical and creative cognitive skills from middle childhood to adolescence: Testing for the threshold theory in the Kingdom of Saudi Arabia, Learning and Individual Differences, 52, pp. 137-147, (2016); 
Perie M., A guide to understanding and developing PLDs, Educational Measurement, 27, 4, pp. 15-29, (2008); 
Phillips G.W., The benchmark method of standard setting, Setting performance standards: Foundations, methods, and innovations, pp. 323-346, (2012); 
Plake B.S., Cizek G.J., Variations on a theme: The modified Angoff, extended Angoff, and Yes/No standard setting methods, Setting performance standards: Foundations, methods, and innovations, pp. 181-200, (2012); 
Ricker K.L., Setting cut-scores: A critical review of the Angoff and modified Angoff methods, Alberta Journal of Educational Research, 52, 1, pp. 53-64, (2006); 
Robitzsch A., On the equivalence of the latent D-scoring model and the two-parameter logistic item response model, (2021); 
Schulz E.M., Mitzel H., The mapmark standard setting method, (2005); 
Schultz E.M., Mitzel H.C., A mapmark method of standard setting as implemented for the National Assessment Governing Board, Journal of Applied Measurement, 12, 2, pp. 165-193, (2011); 
Sheng Y., Markov chain Monte Carlo estimation of normal ogive IRT models in MATLAB, Journal of Statistical Software, 25, 8, pp. 1-15, (2008); 
Skaggs G., Tessema A., Item disordinality with the bookmark standard setting procedure, (2001); 
Skorupski W.P., Hambleton R.K., What are panelists thinking when they participate in standard-setting studies?, Applied Measurement in Education, 18, 3, pp. 233-256, (2005); 
van der Linden W.J., A latent trait method for determining intrajudge inconsistency in the Angoff and Nedelsky techniques of standard-setting, Journal of Educational Measurement, 19, pp. 295-308, (1982); 
Wang N., Use of the Rasch IRT model in standard setting: An item mapping method, Journal of Educational Measurement, 40, pp. 231-252, (2003); 
Williams N.J., Schulz E.M., An investigation of response probability (RP) values used in standard setting, (2005); 
Wyse A.E., The similarity of bookmark cut scores with different response probability values, Educational and Psychological Measurement, 71, 6, pp. 963-985, (2011); 
Wyse A.E., Bunch M., Deville C., Viger S.G., Body of work standard-setting method with construct maps, Educational and Psychological Measurement, 74, 2, pp. 236-262, (2014); 
Zieky M.J., So much has changed: How the setting of cutscores has evolved since the 1980s, Setting performance standards: Concepts, methods, and perspectives, pp. 19-51, (2001); 
Zwick R., Senturk D., Wang J., Loomis S.C., An investigation of alternative methods for item mapping in the National Assessment of Educational Progress, Educational Measurement: Issues and Practice, 20, 2, pp. 15-25, (2001)#FRF#
