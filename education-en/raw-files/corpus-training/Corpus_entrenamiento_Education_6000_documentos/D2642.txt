#ITI#Standing on the shoulders of giants: Online formative assessments as the foundation for predictive learning analytics models#FTI#
#IRE# As universities around the world have begun to use learning management systems (LMSs), more learning data have become available to gain deeper insights into students' learning processes and make data-driven decisions to improve student learning. With the availability of rich data extracted from the LMS, researchers have turned much of their attention to learning analytics (LA) applications using educational data mining techniques. Numerous LA models have been proposed to predict student achievement in university courses. To design predictive LA models, researchers often follow a data-driven approach that prioritizes prediction accuracy while sacrificing theoretical links to learning theory and its pedagogical implications. In this study, we argue that instead of complex variables (e.g., event logs, clickstream data, timestamps of learning activities), data extracted from online formative assessments should be the starting point for building predictive LA models. Using the LMS data from multiple offerings of an asynchronous undergraduate course, we analysed the utility of online formative assessments in predicting students' final course performance. Our findings showed that the features extracted from online formative assessments (e.g., completion, timestamps and scores) served as strong and significant predictors of students' final course performance. Scores from online formative assessments were consistently the strongest predictor of student performance across the three sections of the course. The number of clicks in the LMS and the time difference between first access and due dates of formative assessments were also significant predictors. Overall, our findings emphasize the need for online formative assessments to build predictive LA models informed by theory and learning design. Practitioner notes What is already known about this topic Higher education institutions often use learning analytics for the early identification of low-performing students or students at risk of dropping out. Most predictive models in learning analytics rely on immutable student characteristics (e.g., gender, race and socioeconomic status) and complex variables extracted from log data within a learning management system. Prioritizing prediction accuracy without theory orientation often yields “black-box” models that fail to inform educators on what remedies need to be taken to improve student learning. What this paper adds Predictive models in learning analytics should consider learning theory, pedagogy and learning design to identify key predictors of student learning. Online formative assessments can be a starting point for building predictive models that are not only accurate but also provide educators with actionable insights on how student learning can be improved. Time-related and score-related features extracted from online formative assessments are particularly useful for predicting students' course performance. Implications for practice and/or policy This study provides strong evidence for using online formative assessments as the foundation for predictive models in learning analytics. Student data from online formative assessments can help educators provide students with feedback while informing future formative assessment cycles. Higher education institutions should avoid the hype around complex data from learning management systems and instead rely on effective learning tools such as online formative assessments to revolutionize the use of learning analytics#FRE#
#IPC# formative assessment; learning analytics; learning management system; log data; predictive modelling#FPC#
#IRF# Aljohani N.R., Davis H.C., Learning analytics and formative assessment to provide immediate detailed feedback using a student centered mobile dashboard, 2013 seventh international conference on next generation mobile apps, services and technologies, pp. 262-267, (2013); 
Andrade H.L., Students as the definitive source of formative assessment, Handbook of formative assessment, pp. 90-105, (2010); 
Angus S.D., Watson J., Does regular online testing enhance student learning in the numerical sciences? Robust evidence from a large data set, British Journal of Educational Technology, 40, 2, pp. 255-272, (2009); 
Aulck L., West J., Attrition and performance of community college transfers, PloS ONE, 12, 4, (2017); 
Baig M., Gazzaz Z.J., Farouq M., Blended learning: The impact of blackboard formative assessment on the final marks and students' perception of its effectiveness, Pakistan Journal of Medical Sciences, 36, 3, pp. 327-332, (2020); 
Barana A., Conte A., Fissore C., Marchisio M., Rabellino S., Learning analytics to improve formative assessment strategies, Journal of E-Learning and Knowledge Society, 15, 3, pp. 75-88, (2019); 
Black P., Wiliam D., Assessment and classroom learning, Assessment in Education: Principles, Policy & Practice, 5, 1, pp. 7-74, (1998); 
Blikstein P., Worsley M., Piech C., Sahami M., Cooper S., Koller D., Programming pluralism: Using learning analytics to detect patterns in the learning of computer programming, Journal of the Learning Sciences, 23, 4, pp. 561-599, (2014); 
Blumenstein M., Synergies of learning analytics and learning design: A systematic review of student outcomes, Journal of Learning Analytics, 7, 3, pp. 13-32, (2020); 
Bukralia R., Deokar A., Sarnikar S., Using academic analytics to predict dropout risk in e-learning courses, Reshaping society through analytics, collaboration and decision support, pp. 67-93, (2014); 
Bulut O., Cutumisu M., Aquilina A.M., Singh D., Effects of digital score reporting and feedback on students' learning in higher education, Frontiers in Education, 4, 65, pp. 1-16, (2019); 
Bulut O., Shin J., Cormier D.C., Learning analytics and computerized formative assessments: An application of Dijkstra's shortest path algorithm for personalized test scheduling, Mathematics, 10, 13, (2022); 
Chai K.E.K., Gibson D.C., Predicting the risk of attrition for undergraduate students with time based modelling, Proceedings of cognition and exploratory learning in the digital age, pp. 109-116, (2015); 
Chen F., Cui Y., Utilizing student time series behaviour in learning management systems for early prediction of course performance, Journal of Learning Analytics, 7, 2, pp. 1-17, (2020); 
Chiera B.A., Korolkiewicz M.W., Schultz L.J., Learning from learning analytics: How much do we know about patterns of student engagement?, Big data in education: Pedagogy and research, pp. 163-197, (2021); 
Cho M.H., The effects of design strategies for promoting students' self-regulated learning skills on students' self-regulation and achievements in online learning environments, (2004); 
Cohen D., Sasson I., Online quizzes in a virtual learning environment as a tool for formative assessment, JOTSE, 6, 3, pp. 188-208, (2016); 
Conijn R., Van den Beemt A., Cuijpers P., Predicting student performance in a blended MOOC, Journal of Computer Assisted Learning, 34, 5, pp. 615-628, (2018); 
Costa D.S., Mullan B.A., Kothe E.J., Butow P., A web-based formative assessment tool for masters students: A pilot study, Computers & Education, 54, 4, pp. 1248-1253, (2010); 
Dawson S., Gasevic D., Siemens G., Joksimovic S., Current state and future trends: A citation network analysis of the learning analytics field, Proceedings of the fourth international conference on learning analytics and knowledge, pp. 231-240, (2014); 
Dessi D., Fenu G., Marras M., Recupero D.R., Bridging learning analytics and cognitive computing for big data classification in micro-learning video collections, Computers in Human Behavior, 92, pp. 468-477, (2019); 
Dietrichson A., Beyond clickometry: Analytics for constructivist pedagogies, International Journal on E-Learning, 12, 4, pp. 333-351, (2013); 
Eccles J.S., Adler T.F., Futterman R., Goff S.B., Kaczala C.M., Meece J.L., Midgley C., Expectancies, values, and academic behaviors, Achievement and achievement motivation, pp. 75-146, (1983); 
Engestrom Y., Activity theory as a framework for analyzing and redesigning work, Ergonomics, 43, 7, pp. 960-974, (2000); 
Ferguson R., Clow D., Macfadyen L., Essa A., Dawson S., Alexander S., Setting learning analytics in context: Overcoming the barriers to large-scale adoption, Proceedings of the international conference on learning analytics and knowledge, pp. 251-253, (2014); 
Foster E., Siddle R., The effectiveness of learning analytics for identifying at-risk students in higher education, Assessment & Evaluation in Higher Education, 45, 6, pp. 842-854, (2020); 
Gasevic D., Dawson S., Rogers T., Gasevic D., Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success, The Internet and Higher Education, 28, pp. 68-84, (2016); 
Gasevic D., Dawson S., Siemens G., Let's not forget: Learning analytics are about learning, TechTrends, 59, 1, pp. 64-71, (2015); 
Gasevic D., Kovanovic V., Joksimovic S., Piecing the learning analytics puzzle: A consolidated model of a field of research and practice, Learning: Research and Practice, 3, 1, pp. 63-78, (2017); 
Giannakas F., Troussas C., Voyiatzis I., Sgouropoulou C., A deep learning classification framework for early prediction of team-based academic performance, Applied Soft Computing, 106, (2021); 
Gikandi J.W., Morrow D., Davis N.E., Online formative assessment in higher education: A review of the literature, Computers & Education, 57, 4, pp. 2333-2351, (2011); 
Glick D., Cohen A., Festinger E., Xu D., Li Q., Warschauer M., Predicting success, preventing failure, Utilizing learning analytics to support study success, pp. 249-273, (2019); 
Goggins S., Xing W.L., Building models explaining student participation behavior in asynchronous online discussion, Computers & Education, 94, pp. 241-251, (2016); 
Goodyear P., Teaching as design, HERDSA Review of Higher Education, 2, 2, pp. 27-50, (2015); 
Greller W., Drachsler H., Translating learning into numbers: A generic framework for learning analytics, Journal of Educational Technology & Society, 15, 3, pp. 42-57, (2012); 
Grolemund G., Wickham H., Dates and times made easy with lubridate, Journal of Statistical Software, 40, 3, pp. 1-25, (2011); 
Harris K.R., Graham S., Programmatic intervention research: Illustrations from the evolution of self-regulated strategy development, Learning Disability Quarterly, 22, 4, pp. 251-262, (1999); 
Hebbali A., olsrr: Tools for building OLS regression models, (2020); 
Hooshyar D., Pedaste M., Yang Y., Mining educational data to predict students' performance through procrastination behavior, Entropy, 22, 1, (2019); 
Huberth M., Chen P., Tritz J., McKay T.A., Computer-tailored student support in introductory physics, PLOS ONE, 10, 9, (2015); 
Ifenthaler D., Gibson D., Dobozy E., Informing learning design through analytics: Applying network graph analysis, Australasian Journal of Educational Technology, 34, 2, pp. 117-132, (2018); 
Jayaprakash S.M., Moody E.W., Lauria E.J., Regan J.R., Baron J.D., Early alert of academically at-risk students: An open source analytics initiative, Journal of Learning Analytics, 1, 1, pp. 6-47, (2014); 
Jokhan A., Sharma B., Singh S., Early warning system as a predictor for student performance in higher education blended courses, Studies in Higher Education, 44, 11, pp. 1900-1911, (2019); 
Khalil M., Ebner M., Clustering patterns of engagement in massive open online courses (MOOCs): The use of learning analytics to reveal student categories, Journal of Computing in Higher Education, 29, 1, pp. 114-132, (2017); 
Kibble J.D., Voluntary participation in online formative quizzes is a sensitive predictor of student success, Advances in Physiology Education, 35, 1, pp. 95-96, (2011); 
Kizilcec R.F., Piech C., Schneider E., Deconstructing disengagement: Analyzing learner subpopulations in massive open online courses, Proceedings of the third international conference on learning analytics and knowledge, pp. 170-179, (2013); 
Knapp T.R., Sawilowsky S.S., Constructive criticisms of methodological and editorial practices, The Journal of Experimental Education, 70, 1, pp. 65-79, (2001); 
Knight S., Shum S.B., Theory and learning analytics, Handbook of learning analytics, pp. 17-22, (2018); 
Kovanovic V., Gasevic D., Dawson S., Joksimovic S., Baker R., Does time-on-task estimation matter? Implications on validity of learning analytics findings, Journal of Learning Analytics, 2, 3, pp. 81-110, (2015); 
Kuhn M., caret: Classification and regression training, (2022); 
Lang C., Siemens G., Wise A., Gasevic D., Handbook of learning analytics (First), (2017); 
Larusson J.A., White B., Learning analytics from research to practice, (2014); 
Lawton D., Vye N., Bransford J., Sanders E., Richey M., French D., Stephens R., Online learning based on essential concepts and formative assessment, Journal of Engineering Education, 101, 2, pp. 244-287, (2012); 
Lewis M., Stepwise versus hierarchical regression: Pros and cons, (2007); 
Lockyer L., Heathcote E., Dawson S., Informing pedagogical action: Aligning learning analytics with learning design, American Behavioral Scientist, 57, 10, pp. 1439-1459, (2013); 
Macfadyen L., Dawson S., Mining LMS data to develop an “early warning system” for educators: A proof of concept, Computers & Education, 54, 2, pp. 588-599, (2010); 
Mangaroska K., Giannakos M., Learning analytics for learning design: A systematic literature review of analytics-driven design to enhance learning, IEEE Transactions on Learning Technologies, 12, 4, pp. 516-534, (2019); 
McLaughlin T., Yan Z., Diverse delivery methods and strong psychological benefits: A review of online formative assessment, Journal of Computer Assisted Learning, 33, 6, pp. 562-574, (2017); 
Menton W.H., Generalizability of statistical prediction from psychological assessment data: An investigation with the MMPI-2-RF, Psychological Assessment, 32, 5, pp. 473-492, (2020); 
Mor Y., Craft B., Learning design: Reflections on a snapshot of the current landscape, Research in Learning Technology, 20, pp. 85-94, (2012); 
Nguyen Q., Huptych M., Rienties B., Linking students' timing of engagement to learning design and academic performance, Proceedings of the 8th international conference on learning analytics and knowledge, pp. 141-150, (2018); 
Nicol D., Assessment for learner self-regulation: Enhancing achievement in the first year using learning technologies, Assessment & Evaluation in Higher Education, 34, 3, pp. 335-352, (2009); 
Nicol D., Macfarlane-Dick D., Formative assessment and self-regulated learning: A model and seven principles of good feedback practice, Studies in Higher Education, 31, pp. 199-218, (2006); 
Nistor N., Hernandez-Garciac A., What types of data are used in learning analytics? An overview of six cases, Computers in Human Behavior, 89, pp. 335-338, (2018); 
Norris D.M., Seven things you should know about first generation learning analytics, (2011); 
R: A language and environment for statistical computing, (2021); 
Reimann P., Connecting learning analytics with learning research: The role of design-based research, Learning: Research and Practice, 2, 2, pp. 130-142, (2016); 
Rienties B., Nguyen Q., Holmes W., Reedy K., A review of ten years of implementation and research in aligning learning design with learning analytics at the Open University UK, Interaction Design and Architecture(s), 33, pp. 134-154, (2017); 
Rogers T., Gasevic D., Dawson S., Learning analytics and the imperative for theory driven research, The SAGE handbook of e-learning research, pp. 232-250, (2016); 
Romero C., Lopez M.I., Luna J.M., Ventura S., Predicting students' final performance from participation in online discussion forums, Computers and Education, 68, pp. 458-472, (2013); 
Romero-Zaldivar V.A., Pardo A., Burgos D., Kloos C.D., Monitoring student progress using virtual appliances: A case study, Computers & Education, 58, 4, pp. 1058-1067, (2012); 
Schumacher C., Ifenthaler D., The importance of students' motivational dispositions for designing learning analytics, Journal of Computing in Higher Education, 30, 3, pp. 599-619, (2018); 
Siemens G., Gasevic D., Special issue on learning and knowledge analytics, Educational Technology & Society, 15, 3, pp. 1-163, (2012); 
Smith G., Step away from stepwise, Journal of Big Data, 5, 1, pp. 1-12, (2018); 
Sonderlund L.A., Hughes E., Smith J., The efficacy of learning analytics interventions in higher education: A systematic review, British Journal of Educational Technology, 50, 5, pp. 2594-2618, (2019); 
Stiggins R.J., Assessment crisis: The absence of assessment for learning, Phi Delta Kappan, 83, 10, pp. 758-765, (2002); 
Sudakova N.E., Savina T.N., Masalimova A.R., Mikhaylovsky M.N., Karandeeva L.G., Zhdanov S.P., Online formative assessment in higher education: Bibliometric analysis, Education Sciences, 12, 3, (2022); 
Tempelaar D., Supporting the less-adaptive student: The role of learning analytics, formative assessment and blended learning, Assessment & Evaluation in Higher Education, 45, 4, pp. 579-593, (2020); 
Tempelaar D., Rienties B., Giesbers B., In search for the most informative data for feedback generation: Learning analytics in a data-rich context, Computers in Human Behavior, 47, pp. 157-167, (2015); 
Tempelaar D., Rienties B., Mittelmeier J., Nguyen Q., Student profiling in a dispositional learning analytics application using formative assessment, Computers in Human Behavior, 78, pp. 408-420, (2018); 
Tempelaar D.T., Heck A., Cuypers H., van der Kooij H., van de Vrie E., Formative assessment and learning analytics, Proceedings of the third international conference on learning analytics and knowledge - LAK '13, pp. 205-209, (2013); 
Thayer J.D., Stepwise regression as an exploratory data analysis procedure, (2002); 
Tsai Y.S., Mello R.F., Jovanovic J., Gasevic D., Student appreciation of data-driven feedback: A pilot study on OnTask, LAK21: 11th international learning analytics and knowledge conference, pp. 511-517, (2021); 
Veerasamy A.K., Laakso M.J., D'Souza D., Formative assessment tasks as indicators of student engagement for predicting at-risk students in programming courses, Informatics in Education, 21, 2, (2022); 
Wang D., Han H., Applying learning analytics dashboards based on process-oriented feedback to improve students' learning effectiveness, Journal of Computer Assisted Learning, 37, 2, pp. 487-499, (2021); 
Wigfield A., Eccles J.S., Expectancy–value theory of achievement motivation, Contemporary Educational Psychology, 25, 1, pp. 68-81, (2000); 
Wiliam D., What is assessment for learning?, Studies in Educational Evaluation, 37, 1, pp. 3-14, (2011); 
Wise A.F., Schaffer D.W., Why theory matters more than ever in the age of big data, Journal of Learning Analytics, 2, 2, pp. 5-13, (2015); 
Wylie C., Lyon C., Using the formative assessment rubrics, reflection and observation tools to support professional reflection on practice (revised), (2016); 
Yu S., Liu C., Improving student feedback literacy in academic writing: An evidence-based framework, Assessing Writing, 48, pp. 1-11, (2021); 
Yukselturk E., Ozekes S., Turel Y.K., Predicting dropout student: An application of data mining methods in an online education program, European Journal of Open, Distance and e-Learning, 17, 1, pp. 118-133, (2014); 
Zacharis N.Z., A multivariate approach to predicting student outcomes in web-enabled blended learning courses, Internet and Higher Education, 27, pp. 44-53, (2015); 
Zhang Z., Variable selection with stepwise and best subset approaches, Annals of Translational Medicine, 4, 7, (2016); 
Zimmerman B.J., Becoming a self-regulated learner: An overview, Theory Into Practice, 41, 2, pp. 64-70, (2002)#FRF#
