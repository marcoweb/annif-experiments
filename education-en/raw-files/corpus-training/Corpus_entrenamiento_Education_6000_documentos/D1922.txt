#ITI#A Smart Authoring System for Designing, Configuring, and Deploying Adaptive Assessments at Scale#FTI#
#IRE# The appeal of a shorter testing time makes a computer adaptive testing approach highly desirable for use in multiple assessment and learning contexts. However, for those who have been tasked with designing, configuring, and deploying adaptive tests for operational use at scale, preparing an adaptive test is anything but simple. The process often involves a complex interplay among psychometricians, content experts, and technologists who operate with different vocabularies and subject matter expertise. This paper presents the authors’ experience of developing smart platforms for designing, configuring, and deploying adaptive assessments. We present six design principles for the development of such systems. Through an example of an authoring system now used in production, we discuss smart feedback loops built into the system and how they support efficient iteration. We conclude that it is not possible to overstate the importance of process transition support when launching smart platforms, and that a thoughtful and integrated user experience that allows each of the people in the process to work in a context most accessible to them given their background and experience yields substantial business value#FRE#
#IPC# Assessment; Automated test assembly; Computerized adaptive testing; Shadow test; Smart authoring#FPC#
#IRF# Echo-Adapt, (2020); 
Echo-Adapt: User Guide 1.58, (2020); 
Bock R.D., Mislevy R.J., Adaptive EAP estimation of ability in a microcomputer environment, Applied Psychological Measurement, 6, 4, pp. 431-444, (1982); 
Chang H.H., van der Linden W.J., Optimal stratification of item pools in α-stratified computerized adaptive testing, Applied Psychological Measurement, 27, 4, pp. 262-274, (2003); 
Dermeval D., Paiva R., Bittencourt I., Vassileva J., Borges D., Authoring tools for designing intelligent tutoring systems: A systematic review of the literature. Int J Artif Intell Educ, 336–384, (2018); 
Hiatt J.M., ADKAR: A model for change in business, government, and our community, Prosci Learning Center Publications, (2006); 
Global I.M.S., IMS Question & Test Interoperability (QTI) Specification, (2015); 
Global I.M.S., About the IMS Global Learning Consortiumap, (2020); 
Jiang B., RSCAT: Shadow-Test Approach to Computerized Adaptive Testing, (2020); 
Kurdi G., Leo J., Parsia B., Sattler U., Al-Emari A., A systematic review of automatic question generation for educational purposes, Int J Artif Intell Educ, pp. 121-204, (2020); 
Luo X., Automated test assembly with mixed-integer programming: The effects of modeling approaches and solvers, Journal of Educational Measurement, (2020); 
Pandarova I., Schmidt T., Hartig J., Boubekki A., Jones R., Brefeld U., Predicting the difficulty of exercise items for dynamic difficulty adaptation in adaptive language tutoring, Int J Artif Intell Educ, pp. 342-367, (2019); 
Smith J.C., Taskin Z.C., A Tutorial Guide to Mixed-Integer Programming Models and Solution Techniques, (2007); 
Sottilare R., Baker R., Graesser A., Lester J., Special issue on the generalized intelligent framework for tutoring (gift): Creating a stable and flexible platform for innovations in aied research, Int J Artif Intell Educ, pp. 139-151, (2018); 
Sympson J.B., Hetter R.D., Controlling item-exposure rates in computerized adaptive testing, Proceedings of the 27Th Annual Meeting of the Military Testing Association, (1985); 
van der Linden W.J., Linear models for optimal test design, (2005); 
van der Linden W.J., Elements of adaptive testing, pp. 31-55, (2009); 
van der Linden W.J., Veldkamp B.P., Conditional item-exposure control in adaptive testing using item-ineligibility probabilities, Journal of Educational and Behavioral Statistics, 32, 4, pp. 398-418, (2007)#FRF#
